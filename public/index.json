[{"categories":["Go"],"contents":"拿上一篇用的例子改一下, 看看 Gin 的RouterGroup 与 middleware 是怎么工作的.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { r := gin.New() r.GET(\u0026#34;/\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;gin home page\u0026#34;) }) admin := r.Group(\u0026#34;/admin\u0026#34;) admin.Use(gin.Logger()) admin.GET(\u0026#34;/hello\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;admin\u0026#34;) }) r.Run() }   New 函数返回的是一个没有附加任何 middleware 的 gin.Engine 指针. 其中这个 Engine 的嵌套类型 RouterGroup 被赋值为:\n1 2 3 4 5  RouterGroup{ Handlers: nil, basePath: \u0026#34;/\u0026#34;, root: true, }   Handlers 是一组 gin.HandlerFunc,\n1 2 3 4 5  // HandlerFunc defines the handler used by gin middleware as return value. type HandlerFunc func(*Context) // HandlersChain defines a HandlerFunc array. type HandlersChain []HandlerFunc   basePath 就是这一组路由的路径前缀, 类似 Rails 路由中的 scope 或者基于 scope 的 namespace 加上的路径前缀.\n跟 r.GET, r.POST 一样, 其实 r.Group 也是 gin.Engine 里面的 RouterGroup 嵌套类型的方法. 作用就是构建一个有共通前缀路径的 RouterGroup, 并且这组路由在该前缀路径下使用着特定的一些 middleware.\n1 2 3 4 5 6 7  func (group *RouterGroup) Group(relativePath string, handlers ...HandlerFunc) *RouterGroup { return \u0026amp;RouterGroup{ Handlers: group.combineHandlers(handlers), basePath: group.calculateAbsolutePath(relativePath), engine: group.engine, } }   middleware 可以在使用 Group 方法时候作为其第二个参数传入进来, 也可以单独使用 Use 方法.\n1 2  admin := r.Group(\u0026#34;/admin\u0026#34;, gin.Logger()) admin.Use(gin.Logger())   Use 是直接 append 到 RouterGroup 的 Handlers 最后的, 而 Group 会稍复杂一些调用 group.combineHandlers(handlers),\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  func (group *RouterGroup) combineHandlers(handlers HandlersChain) HandlersChain { finalSize := len(group.Handlers) + len(handlers) if finalSize \u0026gt;= int(abortIndex) { panic(\u0026#34;too many handlers\u0026#34;) } mergedHandlers := make(HandlersChain, finalSize) copy(mergedHandlers, group.Handlers) copy(mergedHandlers[len(group.Handlers):], handlers) return mergedHandlers } // ... func (group *RouterGroup) Use(middleware ...HandlerFunc) IRoutes { group.Handlers = append(group.Handlers, middleware...) return group.returnObj() }   不过, 除了限制 handlers 的总数以外, 没看懂这个 copy 来 copy 去, 跟直接 append 有啥区别 = 。 =\n所以, 说了半天, 其实在 Gin 里边 middleware 就是 HandlerFunc.\n比如 admin.Use(gin.Logger()), Logger 函数的返回值就是 HandlerFunc, 即签名为 func(*Context) 的函数类型\n1 2 3 4 5  // Logger instances a Logger middleware that will write the logs to gin.DefaultWriter. // By default gin.DefaultWriter = os.Stdout. func Logger() HandlerFunc { return LoggerWithConfig(LoggerConfig{}) }   同样的, 当我们要自定义一些 middleware 的时候, 不管处理的逻辑怎么着, 最后都是返回一个有且仅有一个 *gin.Context 参数, 且没有返回值的函数类型.\n这里先看下如果不是使用 Gin 框架, 只是 net/http 的话, 实现类似 middleware 的功能.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { http.Handle(\u0026#34;/\u0026#34;, customMiddleware(\u0026amp;MyHandler{})) http.Handle(\u0026#34;/ping\u0026#34;, customMiddleware(http.HandlerFunc(hello))) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } func hello(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;log hello\u0026#34;) w.Write([]byte(\u0026#34;pong\u0026#34;)) } type MyHandler struct{} func (m *MyHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;log in MyHandler\u0026#34;) w.Write([]byte(\u0026#34;MyHandler ServeHTTP\u0026#34;)) } func customMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(time.Now()) next.ServeHTTP(w, r) log.Println(\u0026#34;after: next.ServeHTTP(w, r)\u0026#34;) }) }   这个输入参数 http.Handler, 返回值也是 http.Handler 的函数就完成了一个简(mei)单(yong)的 middleware 编写了. 其中输入的参数 http.Handler 则是下一层要执行的操作.\n然后, Gin 主页的 README 就有一个自定义 middleware 例子直接拿来了:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; ) func Logger() gin.HandlerFunc { return func(c *gin.Context) { t := time.Now() // Set example variable  c.Set(\u0026#34;example\u0026#34;, \u0026#34;12345\u0026#34;) // before request  // after request  latency := time.Since(t) log.Print(latency) // access the status we are sending  status := c.Writer.Status() log.Println(status) } } func main() { r := gin.New() r.Use(Logger()) r.GET(\u0026#34;/test\u0026#34;, func(c *gin.Context) { example := c.MustGet(\u0026#34;example\u0026#34;).(string) // it would print: \u0026#34;12345\u0026#34;  log.Println(example) }) // Listen and serve on 0.0.0.0:8080  r.Run(\u0026#34;:8080\u0026#34;) }   对比下来, 不管是 net/http 的实现还是 Gin 的实现其实都是\n 不断地进行函数压栈再出栈\n 但因为有了 gin.Context 的封装和串联, 所以, 写起来要好看一些.\n所以, 最后调用的时候, 关键点在 func (c *Context) Next(), Context 的 index 默认为 -1,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  func (engine *Engine) handleHTTPRequest(c *Context) { httpMethod := c.Request.Method path := c.Request.URL.Path // ...  // Find root of the tree for the given HTTP method  t := engine.trees for i, tl := 0, len(t); i \u0026lt; tl; i++ { // ...  if handlers != nil { c.handlers = handlers c.Params = params c.Next() c.writermem.WriteHeaderNow() return } // ...  } // ... } func (c *Context) Next() { c.index++ for s := int8(len(c.handlers)); c.index \u0026lt; s; c.index++ { c.handlers[c.index](c) } }   Related:\n 从 net/http 入门到 Gin 源码梳理 https://chai2010.gitbooks.io/advanced-go-programming-book/content/ https://stackoverflow.com/questions/49668070/how-does-servehttp-work  ","permalink":"https://xguox.me/gin-routergroup-and-middleware-walkthrough.html/","tags":["Go"],"title":"Gin RouterGroup 与 middleware 相关源码"},{"categories":["Go"],"contents":" 通过 Go 的标准库 net/http 可以轻松几行运行起一个简单的 web 服务, 比如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  package main import \u0026#34;net/http\u0026#34; func main() { http.Handle(\u0026#34;/\u0026#34;, \u0026amp;MyHandler{}) http.Handle(\u0026#34;/hello\u0026#34;, http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;world\u0026#34;)) })) http.HandleFunc(\u0026#34;/ping\u0026#34;, hello) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } func hello(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;pong\u0026#34;)) } type MyHandler struct{} func (m *MyHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;MyHandler ServeHTTP\u0026#34;)) }   上面这段代码分别用了不太一样的方式注册了三个简单的路由路径及相应的请求处理. \u0008说不太一样其实还是有共通的地方的. 其实就是两个 net/http 包里边的函数\n1 2 3  func Handle(pattern string, handler Handler) func HandleFunc(pattern string, handler func(ResponseWriter, *Request))   这两个函数都接受两个参数, 第一个都是请求的路径 pattern, Handle 的第二个参数是实现了 Handler 接口的类型, HandleFunc 的第二个参数则是一个函数类型的参数, 只要传入的函数签名是 func(http.ResponseWriter, *http.Request) 就可以了. 所以, 其实第三个路由方式还可以写成下面这样:\n1 2 3  m := MyHandler{} http.HandleFunc(\u0026#34;/ping\u0026#34;, m.ServeHTTP) // ServeHTTP 的签名能匹配得上   即使第二个参数不一样, 但实际要做的事情还是一样的, 最后都是丢给 DefaultServeMux 完成请求的处理.\n1 2 3 4 5 6  func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\u0026#34;http: nil handler\u0026#34;) } mux.Handle(pattern, HandlerFunc(handler)) }   比较特别一些的是第二个路由,\n1 2 3  http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;world\u0026#34;)) })   这里把一个匿名函数转换成为了 http.HandlerFunc 类型, 而 http.HandlerFunc 类型实现了 http.Handler, 因此, 也是一个正确的使用方式.\nhttp.HandlerFunc 是一个等价于 func(ResponseWriter, *Request) 的类型. (每次看到这类写法都要细细想一下是类型定义还是类型别名, 以及有啥不一样, 然后\u0026hellip;又慢慢的忘了)\n1 2 3 4 5 6 7 8 9 10  // The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler that calls f. type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) }   Gin Gin 是基于 net/http 的 web framework(确切一丢说是基于 httprouter). 相较于 net/http 默认的路由功能, httprouter 提供了更好的拓展性(基于 trie 的路由结构). httprouter 的 github 主页上强调了几次 scale, P.S. 这里有一篇站队 net/http 的路由功能的文章).\n类似上面的三个请求服务, 下面是用 Gin 所写的版本(没有使用任何 middleware)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { r := gin.New() r.GET(\u0026#34;/\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;gin home page\u0026#34;) }) r.GET(\u0026#34;/ping\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;pong\u0026#34;) }) r.GET(\u0026#34;/hello\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;world\u0026#34;) }) r.Run() }   丢完示例以后, 跟着示例代码大概的走一遍 Gin 的全流程. 之后在深入看 RouterGroup 和 middleware 相关的探究.\n示例代码用的是 gin.New() 初始化得到一个 *gin.Engine, 这个 Engine 是不带任何 middleware 的, 有时候还会使用 gin.Default() 来初始化, 其实就是空的 Engine 加上了 Logger 和 Recovery 这俩 middleware 了.\n1 2 3 4 5 6  func Default() *Engine { debugPrintWARNINGDefault() engine := New() engine.Use(Logger(), Recovery()) return engine }   gin.Engine 的结构体定义:\n1 2 3 4 5  type Engine struct { RouterGroup // ...好吧, 省略一大票  trees methodTrees // 路由 radix trie 数组 }   RouterGroup 是其中的一个嵌套类型, 所以, 前面的示例中:\n1 2 3  r.GET(\u0026#34;/\u0026#34;, func(context *gin.Context) { context.String(http.StatusOK, \u0026#34;gin home page\u0026#34;) })   类似的 r.GET, r.POST, r.DELETE 其实调用的是 gin.Engine 中的 RouterGroup 对应的方法.\n1 2 3 4 5 6 7 8 9 10 11  // GET is a shortcut for router.Handle(\u0026#34;GET\u0026#34;, path, handle). func (group *RouterGroup) GET(relativePath string, handlers ...HandlerFunc) IRoutes { return group.handle(\u0026#34;GET\u0026#34;, relativePath, handlers) } func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { absolutePath := group.calculateAbsolutePath(relativePath) handlers = group.combineHandlers(handlers) group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj() }   大概就是注册路由相关的事情, 整理好路径, 合并 handlers, 最后加入到相应的路由表(radix tree).\n然后就是, r.Run(),\n1 2 3 4 5 6 7 8  func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(\u0026#34;Listening and serving HTTP on %s\\n\u0026#34;, address) err = http.ListenAndServe(address, engine) return }   其实就还是 net/http 的那套 func ListenAndServe(addr string, handler Handler), 只不过传入的是 gin.Engine, 替代了 http.DefaultServeMux (当ListenAndServe 第二个参数为 nil 的时候).\n1 2 3 4 5 6 7 8 9 10 11  // ServeHTTP conforms to the http.Handler interface. func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) }   之后就是 net/http 包的 server.go 那一套了. TCP Scoket 的监听, 请求与响应.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  // net/http/server.go  func ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } func (srv *Server) ListenAndServe() error { // ...  ln, err := net.Listen(\u0026#34;tcp\u0026#34;, addr) // 监听 TCP(HTTP) 的连接  if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } func (srv *Server) Serve(l net.Listener) error { // ...  defer l.Close() // ...  for { rw, e := l.Accept() // 阻塞等待客户端连接  // ...  c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return  go c.serve(ctx) // 起一个 goroutine 处理接受的请求  } } func (c *conn) serve(ctx context.Context) { // ...  for { w, err := c.readRequest(ctx) // ...  serverHandler{c.server}.ServeHTTP(w, w.req) // ...  } } // serverHandler delegates to either the server\u0026#39;s Handler or // DefaultServeMux and also handles \u0026#34;OPTIONS *\u0026#34; requests. type serverHandler struct { srv *Server } func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler // gin.Engine  if handler == nil { handler = DefaultServeMux } if req.RequestURI == \u0026#34;*\u0026#34; \u0026amp;\u0026amp; req.Method == \u0026#34;OPTIONS\u0026#34; { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) }   最后这个 handler 就是一路带进来的 gin.Engine, 回到前面贴过的 func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request)\n1 2 3 4 5 6 7 8 9 10  // ServeHTTP conforms to the http.Handler interface. func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) // 通过请求 method 找到 engine.trees 中对应的树 -\u0026gt; 在树中查找对应的路由, 执行相关的 handlers  engine.pool.Put(c) }   接下来就是遍布 Gin 项目的 gin.Context 登场. 源码注释也说了, Context 是 Gin 最重要的一部分. 它可以让我们在 middleware 之间传递变量, 管理整个请求/响应流程, 比如验证请求的 JSON 以及返回一个 JSON 响应等等.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  type Context struct { writermem responseWriter Request *http.Request Writer ResponseWriter Params Params handlers HandlersChain index int8 engine *Engine Keys map[string]interface{} Errors errorMsgs Accepted []string } // ...  func (c *Context) Next() { c.index++ for s := int8(len(c.handlers)); c.index \u0026lt; s; c.index++ { c.handlers[c.index](c) // 执行 handler  } }   可以看出其实主要是对 func(w http.ResponseWriter, r *http.Request) 的一个封装, 其中用于响应的 ResponseWriter 基于 http.ResponseWriter 做了些拓展.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  type responseWriterBase interface { http.ResponseWriter http.Hijacker http.Flusher http.CloseNotifier // ... } type responseWriter struct { http.ResponseWriter size int status int }   gin.Context 中的一大票返回 Format 方法\n1 2 3  func (c *Context) String(code int, format string, values ...interface{}) { c.Render(code, render.String{Format: format, Data: values}) }   Related:\n Gin RouterGroup 与 middleware 相关源码 https://chai2010.gitbooks.io/advanced-go-programming-book/content/ https://stackoverflow.com/questions/49668070/how-does-servehttp-work  ","permalink":"https://xguox.me/gin-source-code.html/","tags":["Go"],"title":"从 net/http 入门到 Gin 源码梳理"},{"categories":["Go","Tools"],"contents":"曾几何时也不喜欢 IDE, 启动超慢, 风扇呼呼的转. 写 Ruby 以后更甚, 因为 Ruby / Rails 的很多魔法在 IDE 里边会水土不服.\n虽然据说 Rubymine 挺好的, 不过尝试的欲望还是不高.\n之前写 Go 基本都用的是 Visual Studio Code, 很多 IDE 的功能基本都能完成. 启动虽然可能不及 Sublime, Vim(其实装上一堆插件也启动的挺慢的), 不过开箱即用(装个官方的 Go 插件)也不折腾了.\n只是 go 1.11 以后对 go modules 支持似乎不太好(还要额外自己手工 go get 一些包). 于是开始尝试了 Goland, 写的时候虽然有很多魔法快捷方式支持, 但是可能还是更熟悉编辑器的那套 = 。 =\n但是, 阅读代码可谓甩编辑器几条街. Go To Declaration 本来就是 IDE 的强项了, 虽然 Visual Studio Code 也能做到, 但是, 速度和精准度就只能用差强人意形容. 用了 Goland 定义的跳转直接起飞了.\nGoland 还有个至少目前没看到在其他一些编辑器上被实现的强大功能, 就是可以一键查看到接口被哪些类型实现以及实现了哪些接口.\n","permalink":"https://xguox.me/goland-is-awesome-for-reading.html/","tags":["Go","Tools"],"title":"用 Goland 阅读代码是真 6"},{"categories":["Go"],"contents":"上一篇 在用 Go 实现 Trie 结构时候踩了个坑,\n1 2 3 4  type Node struct { Char rune Children []*Node }   最开始这里 Children 字段用的是 []Node 而不是 []*Node, 结果数据只塞进了一层以后, 每一结点的这个 Children 都为空的 []Node.\nDebug 了好一会一步步的执行下来才发现问题所在, \u0008\u0008数组在元素赋值的时候是传副本的.\n改一下 insert 方法打印一下内存地址就知道了.\n1 2 3 4 5 6 7 8 9 10 11 12  func (n *Node) insert(r rune) *Node { child := n.get(r) if child == nil { child = NewNode(r) n.Children = append(n.Children, *child) fmt.Printf(\u0026#34;new child addr: %p \\n\u0026#34;, \u0026amp;child) last := n.Children[len(n.Children)-1] fmt.Printf(\u0026#34;but last addr: %p \\n\u0026#34;, \u0026amp;last) } return child }   输出结果类似:\n1 2 3 4 5 6 7  ... new child addr: 0xc000287f18 but last addr: 0xc0002a3640 new child addr: 0xc000287f20 but last addr: 0xc0002a36c0 new child addr: 0xc000287f28 but last addr: 0xc0002a3740   append 进去和取出来的是不一样的地址 = 。 = 设置进去的只是副本, 类似方法调用的参数一样. (map, channel 也是)\n","permalink":"https://xguox.me/golang-pointer-array-slice-map-copy.html/","tags":["Go"],"title":"Go 的指针与数组"},{"categories":["Go","Ruby"],"contents":" 上一篇看到 Trie 的数据结构, 想着用 Ruby 和 Go 大概实现一下对比看看, \u0008顺便看看一下 Benchmark.\n(挺没意义的一个事 = 。 =)\n普通的 trie 是一个字符一个结点, 压缩 trie 的结点可能是一个字符串, 空间更省一些吧.\n20k.txt 是两万个英文单词, github 地址 https://github.com/first20hours/google-10000-english/blob/master/20k.txt.\nTrie in Ruby 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106  # ruby 2.4 require \u0026#39;benchmark\u0026#39; class Node attr_reader :char, :children def initialize(c) @char = c @children = [] end def build(c) child = find(c) if child.nil? child = Node.new(c) @children \u0026lt;\u0026lt; child end return child end def find(c) @children.each do |child| return child if child.char == c end nil end end class Trie attr_reader :root def initialize @root = Node.new(nil) end def build(word) node = @root word.chars.each do |char| child = node.build(char) node = child end end def has?(word) node = @root word.chars.each do |char| found = node.find(char) return false if found.nil? node = found end return true end end class Trie2 \u0026lt; Hash def initialize super end def build(string) string.chars.inject(self) do |h, char| h[char] ||= { } end end def has?(string) tr = self string.chars.each do |char| return false if tr[char].nil? tr = tr[char] end return true end end t = Trie.new File.readlines(\u0026#39;./20k.txt\u0026#39;).each do |line| t.build(line.gsub(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;)) end puts Benchmark.measure { 200_000.times do t.has?(\u0026#39;42082\u0026#39;) t.has?(\u0026#39;oops\u0026#39;) t.has?(\u0026#39;Supercalifragilisticexpialidocious\u0026#39;) end } t2 = Trie2.new File.readlines(\u0026#39;./20k.txt\u0026#39;).each do |line| t2.build(line.gsub(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;)) end puts Benchmark.measure { 200_000.times do t2.has?(\u0026#39;42082\u0026#39;) t2.has?(\u0026#39;oops\u0026#39;) t2.has?(\u0026#39;Supercalifragilisticexpialidocious\u0026#39;) end } # Benchmark # 2.950000 0.010000 2.960000 ( 2.978725) # 1.320000 0.010000 1.330000 ( 1.325868)   Trie in Golang 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99  // go 1.11 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; ) type Node struct { Char rune Children []*Node } func NewNode(r rune) *Node { return \u0026amp;Node{Char: r} } func (n *Node) insert(r rune) *Node { child := n.get(r) if child == nil { child = NewNode(r) n.Children = append(n.Children, child) } return child } func (n *Node) get(r rune) *Node { for _, child := range n.Children { if child.Char == r { return child } } return nil } type Trie struct { Root *Node } func NewTrie() *Trie { var r rune trie := Trie{Root: NewNode(r)} return \u0026amp;trie } func (tr *Trie) Build(word string) { node := tr.Root runeArr := []rune(word) for _, char := range runeArr { child := node.insert(char) node = child } } func (tr *Trie) Has(word string) bool { node := tr.Root runeArr := []rune(word) for _, char := range runeArr { found := node.get(char) if found == nil { return false } node = found } return true } func BenchmarkTrieFind(b *testing.B) { var trie1 = NewTrie() file, err := os.Open(\u0026#34;./20k.txt\u0026#34;) if err != nil { log.Fatal(err) } defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() { trie1.Build(scanner.Text()) } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { trie1.Has(\u0026#34;42082\u0026#34;) trie1.Has(\u0026#34;oops\u0026#34;) trie1.Has(\u0026#34;Supercalifragilisticexpialidocious\u0026#34;) } } // goos: darwin // goarch: amd64 // BenchmarkTrieFind-8 5000000\t255 ns/op\t144 B/op\t1 allocs/op // PASS // ok _/Users/xguox/Desktop\t1.591s // Success: Benchmarks passed.   Ruby 的 Benchmark 是 200_000 次的总和, 单位是 s, 换成 go 实现下面的 ns 大概 6629 ns/op. \u0008\n","permalink":"https://xguox.me/trie-implementing-ruby-vs-golang.html/","tags":["Go","Ruby"],"title":"Trie 的实现, Ruby vs Go"},{"categories":["Go"],"contents":"Gin 或者说 httprouter 的路由使用的数据结构是动态压缩的 trie.\n每个 HTTP 方法维护着一个 trie. 不像 Rails 遇到路由冲突的时候, 定义在前的会覆盖后面的, Gin 在 build 的时候就会 panic.\n几种冲突的场景:\n 在插入 wildcard 节点时, 父节点的 children 数组非空且 wildChild 被设置为 false. 例如: GET /user/getAll 和 GET /user/:id/getAddr, 或者 GET /user/*aaa 和 GET /user/:id. 在插入 wildcard 节点时, 父节点的 children 数组非空且 wildChild 被设置为 true , 但该父节点的 wildcard 子节点要插入的 wildcard 名字不一样. 例如: GET /user/:id/info 和 GET /user/:name/info. 在插入 catchAll 节点时, 父节点的 children 非空. 例如: GET /src/abc 和 GET /src/*filename, 或者 GET /src/:id 和 GET /src/*filename. 在插入 static 节点时, 父节点的 wildChild 字段被设置为 true. 在插入 static 节点时, 父节点的 children 非空, 且子节点 nType 为 catchAll.  Related:\n https://github.com/chai2010/advanced-go-programming-book  ","permalink":"https://xguox.me/gin-router-conflicts.html/","tags":["Go"],"title":"Gin 路由冲突"},{"categories":["Go"],"contents":"最开始学的时候看的一本书, 当时只是通读看语法的阶段, 其中有几个像下面这样关于 goroutine 调度的图并没有什么概念, 那个章节是关于 Go 的并发的. 现在回头理一理这一部分的知识.\ngoroutine 是构建在操作系统的线程调度之上的, 图中的主要部分分成三类, 也就是大家每每提及 goroutine 时候会牵扯到的 MPG 调度模式.\n M: 系统级线程 P: 逻辑处理器 G: goroutine  M 是操作系统线程在 Go runtime 的体现. 操作系统不管 P, G, 调度什么的, 只对接 M. 然后, 就像操作系统需要把线程放到一个 CPU 核心上运行一样, M 也需要绑定到一个 P 上才会运行 G. \u0008按需创建, 默认最大限制为10000个.\nP for processor, 执行 G 的所必须的资源. P 的数量可以通过调用 runtime 的 func GOMAXPROCS(n int) int 来设置, 默认为机器的逻辑 CPU 核数(比如四核可能用超线程虚拟成八核的跑), 可以通过 runtime 的 func NumCPU() int 获取得到值. 最多也只能是逻辑 CPU 核数个线程同时在操作系统跑, 所以 P 的数量决定了最多有多少个 G 可以同时运行. 一般也没必要去修改.\n一般情况下调度器会让 M 完整执行 G 的代码, 但是, 遇到下面几种场景时候会切换到其他的 G\n G 里边有新的 G 产生, 也就是调用了 go 函数 channel 操作 系统调用, 如文件读写 网络 I/O 一个完整的 GC 周期以后?  图片来源 - http://morsmachine.dk/go-scheduler\n关于 MPG 这部分相关的 go 源代码基本上就在 runtime 包的 runtime2.go 和 proc.go可以看到. 下面只是截取几个主要的结构体(其实还没完整看明白这些结构体字段的意思 = 。 =):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  type g struct { stack stack // offset known to runtime/cgo  stackguard0 uintptr // offset known to liblink  stackguard1 uintptr // offset known to liblink  _panic *_panic // innermost panic - offset known to liblink  _defer *_defer // innermost defer  m *m // current m; offset known to arm liblink  sched gobuf ... } type m struct { g0 *g // goroutine with scheduling stack  ... curg *g // current running goroutine  p puintptr // attached p for executing go code (nil if not executing go code)  ... spinning bool // m is out of work and is actively looking for work  blocked bool // m is blocked on a note  inwb bool // m is executing a write barrier  ... } type p struct { lock mutex id int32 status uint32 // one of pidle/prunning/...  link puintptr schedtick uint32 // incremented on every scheduler call  syscalltick uint32 // incremented on every system call  sysmontick sysmontick // last tick observed by sysmon  m muintptr // back-link to associated m (nil if idle)  ... // Queue of runnable goroutines. Accessed without lock.  runqhead uint32 runqtail uint32 runq [256]guintptr } type schedt struct { // Global runnable queue.  runqhead guintptr runqtail guintptr runqsize int32 }   调度器 schedt 结构体维护着一个全局的 goroutines 队列, 然后 每个 P 都管理着一组本地 goroutines 队列 runq, 每当有 go\u0008 函数调用产生(func newproc(siz int32, fn *funcval))一个 goroutine 就会被加入到全局/本地队列中, 具体规则没理解清楚 (Put it on the queue of g\u0026rsquo;s waiting to run). 查了其他文档貌似说是优先放入当前 P 的本地队列中.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // Create a new g running fn with siz bytes of arguments. // Put it on the queue of g\u0026#39;s waiting to run. // The compiler turns a go statement into a call to this. // Cannot split the stack because it assumes that the arguments // are available sequentially after \u0026amp;fn; they would not be // copied if a stack split occurred. //go:nosplit func newproc(siz int32, fn *funcval) { argp := add(unsafe.Pointer(\u0026amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() { newproc1(fn, (*uint8)(argp), siz, gp, pc) }) }   当 M 执行完了当前 P 的 本地队列中所有的 G 以后, 这个 P 会先尝试从全局队列中拿一些 G 来执行, 当全局队列为空的时候, 就会随机挑选其他的 P, 从这个随机 P 的本地队列里中拿走一半的 G 到自己的队列中执行. ( proc.go 的 func runqsteal(_p_, p2 *p, stealRunNextG bool) *g )\n当某个 G 遇到前面提到的几种阻塞或者可能需要切换上下文的场景时, 调度器会让 P 会与 M(0) 分离, 并创建一个新的 M(1) 或者从缓存起来的其他 M(1) 启动来运行这个 P 上其余的 G, 而 M(0)继续运行着这个阻塞的 G. 当阻塞 G 执行完成以后会被放回 runq 队列, 看其他文章有说放回全局队列, 也有说放回原本的队列 = 。 = 反正就是放回队列, 然后 M(0) 可以洗洗睡了.\nRelated:\n Go: Design Patterns for Real-World Projects Go in Action https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html https://tonybai.com/2017/06/23/an-intro-about-goroutine-scheduler/ https://golang.org/s/go11sched  ","permalink":"https://xguox.me/golang-scheduler.html/","tags":["Go"],"title":"理解 Goroutine 的调度"},{"categories":["多肉与植物"],"contents":"","permalink":"https://xguox.me/ruby-on-earth.html/","tags":["多肉与植物"],"title":"Ruby on earth"},{"categories":["Go"],"contents":"尝试用 docker 部署个简单的 go 应用, 然后发现 build 不起来. 因为一些神秘力量, 会出现类似下面的一些错误信息.\nunrecognized import path \u0026ldquo;cloud.google.com/go\u0026rdquo;\nunrecognized import path \u0026ldquo;golang.org/x/sync\u0026rdquo;\ni/o timeout\n想着总不能在服务器上用神秘的方式去解决这股力量吧 = 。 =\n幸好 go 1.11 的 modules 功能可以启用 vendor 模式\ngo mod vendor\n这样, 本地拉好相关依赖就可以了. Dockerfile 的 RUN 命令后面加 -mod vendor\n1  RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -mod vendor   或者还有一种办法, 比如:\n如果报错\n1  go: golang.org/x/sys@v0.0.0-20180903190138-2b024373dcd9: unrecognized import path \u0026#34;golang.org/x/sys\u0026#34; (https fetch: Get https://golang.org/x/sys?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)   可以用 go mod edit 把对应的源换成 github 的,\n1  go mod edit -replace=golang.org/x/sys@v0.0.0-20180903190138-2b024373dcd9=github.com/golang/sys@v0.0.0-20180903190138-2b024373dcd9  ","permalink":"https://xguox.me/golang-111-vendor-dockerfile.html/","tags":["Go"],"title":"go build timeout in docker"},{"categories":["Go"],"contents":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;sync\u0026#34; ) func main() { var pool = \u0026amp;sync.Pool{ New: func() interface{} { return new(bytes.Buffer) }} b := pool.Get().(*bytes.Buffer) b.Write([]byte(\u0026#34;hello\u0026#34;)) pool.Put(b) b = pool.Get().(*bytes.Buffer) fmt.Println(b) // hello \tfmt.Println(pool.Get().(*bytes.Buffer)) // 再去 Get 什么也没有, 因为只有一个, 且前一条语句已经 Get 了又没 Put 回去  b.Write([]byte(\u0026#34;42082\u0026#34;)) pool.Put(b) fmt.Println(pool.Get().(*bytes.Buffer)) b.Write([]byte(\u0026#34;42039\u0026#34;)) pool.Put(b) runtime.GC() // 手动 GC \tfmt.Println(pool.Get().(*bytes.Buffer)) // 已被回收, 啥也没输出  c := pool.Get().(*bytes.Buffer) c.Write([]byte(\u0026#34;2nd hello\u0026#34;)) d := pool.Get().(*bytes.Buffer) d.Write([]byte(\u0026#34;3rd hello\u0026#34;)) pool.Put(c) pool.Put(d) fmt.Println(pool.Get().(*bytes.Buffer)) // 2nd hello \tfmt.Println(pool.Get().(*bytes.Buffer)) // 3rd hello }   对比用 channel 实现类似 sync.Pool 的 benchmark\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  // x_test.go package main import ( \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) type ChanPool chan interface{} type A struct{} var syncPool = sync.Pool{ New: func() interface{} { return new(A) }, } var chanPool ChanPool = make(chan interface{}, 100) func get() interface{} { select { case e := \u0026lt;-chanPool: return e default: return new(A) } } func put(a interface{}) { select { case chanPool \u0026lt;- a: default: } return } func BenchmarkPool(b *testing.B) { for i := 0; i \u0026lt; 20; i++ { syncPool.Put(new(A)) } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { syncPool.Put(syncPool.Get()) } } func BenchmarkBenchmarkChanPool(b *testing.B) { for i := 0; i \u0026lt; 20; i++ { put(new(A)) } b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { a := get() put(a) } } // goos: darwin // goarch: amd64 // pkg: test // BenchmarkPool-8 100000000\t22.5 ns/op\t0 B/op\t0 allocs/op // BenchmarkBenchmarkChanPool-8 20000000\t78.4 ns/op\t0 B/op\t0 allocs/op // PASS // coverage: 0.0% of statements // ok test\t3.941s // Success: Benchmarks passed.  ","permalink":"https://xguox.me/golang-sync-pool.html/","tags":["Go"],"title":"sync.Pool"},{"categories":["Elasticsearch"],"contents":"Logstash 配置文件的 output 加了一句\n1 2  stdout { codec =\u0026gt; rubydebug }    本地测试时候看输出用的, 发上去没把这句删掉然后差点把服务器的硬盘撑坏了.\n=。 =\n去掉这句就不在把正常输出的写进日志, 但是, 还是会有一些 WARN 和 ERROR 的, 在 /etc/logstash/startup.options 里边的 LS_OPTS 加上 --quiet 以后就只剩 ERROR 的.\n\u0026ndash;quiet Quieter Logstash logging. This causes only errors to be emitted.\n\u0026ndash;verbose More verbose logging. This causes info level logs to be emitted.\n\u0026ndash;debug Most verbose logging. This causes debug level logs to be emitted.\n这个配置改完还得重装一遍 logstash\n# After changing anything here, you need to re-run $LS_HOME/bin/system-install\nLS_HOME 在 startup.options 这个文件里边有定义\n","permalink":"https://xguox.me/elk-logstash-conf.html/","tags":["Elasticsearch"],"title":"Logstash 的日志输出"},{"categories":["Go"],"contents":"当两个(或以上)的 goroutine 并发访问同一个变量, 且至少其中一个是写操作的时候就会发生数据竞争. 像其他语言比如 Ruby 一样, Go 也提供了互斥锁 Mutex 来避免发生这一情况.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var ( counter int wg sync.WaitGroup mutex sync.Mutex ) func main() { wg.Add(2) go incCounter(1) go incCounter(2) wg.Wait() fmt.Printf(\u0026#34;Final Counter: %d\\\\n\u0026#34;, counter) } func incCounter(id int) { defer wg.Done() for count := 0; count \u0026lt; 2; count++ { mutex.Lock() { value := counter value++ counter = value } mutex.Unlock() } }   一个互斥锁可以被用来保护一个临界区或者一组临界区. 有了互斥锁, 在同一时刻只有一个 goroutine 进入到该临界区里面执行. 每当有 goroutine 想进入临界区时, 都需要先通过 Lock 将互斥锁进行锁定, 每个 goroutine 离开临界区时, 都要立即通过 Unlock 进行解锁. 当临界区比较精简的时候可能不会忘了解锁, 但是, 当临界区比较复杂的时候, 比如出现分叉或者提前返回, 往往容易忘记解锁, 这时候可以使用 defer mutext.Unlock(), 临界区会延伸到函数作用域的最后一行, 当函数返回甚至发生 panic 以后用 recover 恢复都会执行解锁.\n因为造成竞争的一个原因是同时写数据, 这也就意味着, 如果只是并发读的话是不会发生竞争的. 所以, go 提供了更细粒度的读写锁 RWMutex. 顾名思义, 就是把读锁跟写锁分开, 通过 RLock 与 RUnlock 对读锁进行锁定与解锁, 而写锁的锁定与解锁操作则还是Lock 和 Unlock. 在读锁被锁定的情况下, 如果锁定写锁则会阻塞当前 goroutine, 但是, 再对读锁锁定不会阻塞.\n就是说, 多个读操作可以同时进行, 但当有正在读的操作发生以后, 不能进行写操作, 直到读锁被释放.\n看了 RWMutex 的源码这里又得回顾一下类型别名和类型定义 = 。 =\n1 2 3 4 5 6 7 8 9 10  // RLocker returns a Locker interface that implements // the Lock and Unlock methods by calling rw.RLock and rw.RUnlock. func (rw *RWMutex) RLocker() Locker { return (*rlocker)(rw) } type rlocker RWMutex func (r *rlocker) Lock() { (*RWMutex)(r).RLock() } func (r *rlocker) Unlock() { (*RWMutex)(r).RUnlock() }   配合 sync.Cond 使用的时候, 如果需要传读锁的话可以用这个 RLocker\nsnyc.Cond 需要配合锁一起使用, *Mutex 或者 *RWMutex 都可以. 通过 sync.NewCond 函数初始化得到一个 *sync.Cond 值.\n1 2 3 4  // NewCond returns a new Cond with Locker l. func NewCond(l Locker) *Cond { return \u0026amp;Cond{L: l} }   *sync.Cond 的主要方法有三个, Wait, Signal, Broadcast\n1 2 3 4 5 6 7 8 9 10  type Cond struct { noCopy noCopy // L is held while observing or changing the condition \tL Locker notify notifyList checker copyChecker } ...   调用 newCond 的时候传的一般是锁的指针, 所以, 下面的 c.L.Lock() 其实跟直接在锁上面调 Lock() 是一样的.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // c.L.Lock() // for !condition() { // c.Wait() // } // ... make use of condition ... // c.L.Unlock() // func (c *Cond) Wait() { c.checker.check() t := runtime_notifyListAdd(\u0026amp;c.notify) c.L.Unlock() runtime_notifyListWait(\u0026amp;c.notify, t) c.L.Lock() }   Wait 会自动 Unlock 锁, 所以, 得先锁定了互斥锁的前提下才能调用 Wait. 同时还会暂停执行当前 goroutine, 当稍后唤醒通知来了, 恢复该 goroutine 的执行以后, 会再次上锁.\n源码的注释示例这里用的是 for, 而不是 if, 因为在恢复执行了以后, condition 不一定就满足条件可以跳出等待, 那么就需要继续等待, 而不是让 goroutine 执行下去.\n对应上面的唤醒通知的是, Signal, Broadcast这俩方法, Signal 唤醒的任意一个被悬起的 goroutine(Cond 的 notifyList), Broadcast 则是唤醒所有的 goroutine, 再分别检验条件是否继续 Wait.\n","permalink":"https://xguox.me/sync-in-go.html/","tags":["Go"],"title":"sync.Mutex, sync.RWMutex, snyc.Cond"},{"categories":["Go"],"contents":"与 Ruby 一样, Go 在调用函数(方法)的时候都是值传递的, 即参数的副本. 在 Go 里边, 即使传的是指针类型的变量也一样, 实际上传的是指针的副本, 指向的是同样的内存地址. 因此调用时候, 传指针可能就会改变参数变量的值.\n此外, 当把元素塞进 array, slice, map, channel 的时候也是类似的道理. 查看示例\n As in all languages in the C family, everything in Go is passed by value. That is, a function always gets a copy of the thing being passed, as if there were an assignment statement assigning the value to the parameter. For instance, passing an int value to a function makes a copy of the int, and passing a pointer value makes a copy of the pointer, but not the data it points to.\n 还有, 作为方法的接收者和返回值也都是. 定义方法时候用值还是指针可以参考以下几个因素:\n First, and most important, 该方法是否会修改接收者的值. Second is the consideration of efficiency., 如果接收者是一个「巨大」的结构体对象, 那还是用指针吧. Next is consistency. 如果同一个结构体下有其他方法是指针接收者的话, 那就统一全都用指针吧. 其余, 如基础类型, slices, 或小型结构体则用值作为接收者更高效, 清晰  *T 的方法集(method set)包含所有接收者是 *T 以及接收者为 T 的方法. 但是, 反过来是不成立的, T 所拥有的方法集仅限于接收者为 T.\n之所以会出现这种区别, 是因为如果一个接口值是 *T, 调用方法时候 Go 可以自动取指针值完成调用, 但是, 如果接口值是 T, 是没有安全的方式取得 T 值的指针[因为这样做的话方法就可以修改接口(不是类型)的值, 这是 Go 的语言规范所不允许的]\n Even in cases where the compiler could take the address of a value to pass to the method, if the method modifies the value the changes will be lost in the caller. As an example, if the Write method of bytes.Buffer used a value receiver rather than a pointer, this code:\n 1 2  var buf bytes.Buffer io.Copy(buf, os.Stdin)   would copy standard input into a copy of buf, not into buf itself. This is almost never the desired behavior.\n","permalink":"https://xguox.me/go-pointer.html/","tags":["Go"],"title":"Go 的指针"},{"categories":["Go"],"contents":" 在链表中查找数据的时间复杂度是 O(n), 在上面这个链表, 假设要查找到节点 37, 就得从第一个节点开始, 遍历 7 次,\n如果通过给链表节点加一层索引, 每两个节点提取出来一个, 组成一个新的链表, 抽取出来的每一个节点, 除了原本有的指向原始链表(最下面一层)的下一节点的指针外, 还有另一个指针, 指向新一级的链表中对应的下一个节点, 通过这个跳表结构, 还是查找节点 37, 就只需要遍历 4 次 (7 -\u0026gt;\u0026gt; 19 -\u0026gt;\u0026gt; 26 -\u0026gt; 37).\n以此类推, 在上面这个提取出来的新一级链表之上, 再一次提取出来一级新的链表. 依旧同样查找节点 37 , 现在只要遍历 3 次.\n类似二分查找的, 每提取一级链表, 节点数都是前一级的一半, 所以, 查找的时间复杂度就从原始链表的 O(n) 降低到 O(log n)\n那么, 问题来了, 每次新增或者删除一个节点, 就会打乱原本的跳表结构节点数的比例, 极端情况下, 可能最后又变回一条巨大的链表.\n实际上, 跳表并不要求上下相邻两层链表之间的节点个数有严格的对应关系, 当插入数据时候, 通过生成一个随机整数来决定这个节点会被插入到哪几层链表中, 比如, 生成的随机数是 K, 那么这个结点就插入到从第 1 到 第 K 层链表中(以及原始链表), 节点最大的层数不允许超过一个特定的最大值 MaxLevel, 而插入操作不影响其他节点的层数. 删除同理.\n skiplist, 翻译成中文, 可以翻译成\u0026rdquo;跳表\u0026rdquo;或\u0026rdquo;跳跃表\u0026rdquo;, 指的就是除了最下面第1层链表之外, 它会产生若干层稀疏的链表, 这些链表里面的指针故意跳过了一些节点(而且越高层的链表跳过的节点越多). 这就使得我们在查找数据的时候能够先在高层的链表中进行查找, 然后逐层降低, 最终降到第1层链表来精确地确定数据位置. 在这个过程中, 我们跳过了一些节点, 从而也就加快了查找速度.\n 或者看这个维基百科给的 gif\n相关来源:\n https://en.wikipedia.org/wiki/File:Skip_list_add_element-en.gif https://commons.wikimedia.org/wiki/File:Skip_list.svg http://zhangtielei.com/posts/blog-redis-skiplist.html  用 Go 实现跳表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) const MaxLevel = 32 const Probability = 0.25 // 基于时间与空间综合 best practice 值, 越上层概率越小  func randLevel() (level int) { rand.Seed(time.Now().UnixNano()) for level = 1; rand.Float32() \u0026lt; Probability \u0026amp;\u0026amp; level \u0026lt; MaxLevel; level++ { // fmt.Println(rand.Float32())  } // fmt.Printf(\u0026#34;up to %d level\\n\u0026#34;, level)  return } type node struct { forward []*node key int } type skipList struct { head *node level int } func newNode(key, level int) *node { return \u0026amp;node{key: key, forward: make([]*node, level)} } func newSkipList() *skipList { return \u0026amp;skipList{head: newNode(0, MaxLevel), level: 1} } func (s *skipList) insert(key int) { current := s.head update := make([]*node, MaxLevel) // 新节点插入以后的前驱节点  for i := s.level - 1; i \u0026gt;= 0; i-- { if current.forward[i] == nil || current.forward[i].key \u0026gt; key { update[i] = current } else { for current.forward[i] != nil \u0026amp;\u0026amp; current.forward[i].key \u0026lt; key { current = current.forward[i] // 指针往前推进  } update[i] = current } } level := randLevel() if level \u0026gt; s.level { // 新节点层数大于跳表当前层数时候, 现有层数 + 1 的 head 指向新节点  for i := s.level; i \u0026lt; level; i++ { update[i] = s.head } s.level = level } node := newNode(key, level) for i := 0; i \u0026lt; level; i++ { node.forward[i] = update[i].forward[i] update[i].forward[i] = node } } func (s *skipList) delete(key int) { current := s.head for i := s.level - 1; i \u0026gt;= 0; i-- { for current.forward[i] != nil { if current.forward[i].key == key { tmp := current.forward[i] current.forward[i] = tmp.forward[i] tmp.forward[i] = nil } else if current.forward[i].key \u0026gt; key { break } else { current = current.forward[i] } } } } func (s *skipList) search(key int) *node { // 类似 delete  return nil } func (s *skipList) print() { fmt.Println() for i := s.level - 1; i \u0026gt;= 0; i-- { current := s.head for current.forward[i] != nil { fmt.Printf(\u0026#34;%d \u0026#34;, current.forward[i].key) current = current.forward[i] } fmt.Printf(\u0026#34;***************** Level %d \\n\u0026#34;, i+1) } } func main() { list := newSkipList() for i := 0; i \u0026lt; 20; i++ { list.insert(rand.Intn(100)) } list.print() fmt.Println(\u0026#34;\\n--------------------------------------\u0026#34;) list.delete(10) list.print() fmt.Println(\u0026#34;\\n--------------------------------------\u0026#34;) }  ","permalink":"https://xguox.me/go-skip-list.html/","tags":["Go"],"title":"Go 实现 Skip List(跳表)"},{"categories":[],"contents":"坐\n","permalink":"https://xguox.me/sitting.html/","tags":[],"title":"Sitting"},{"categories":["Lego"],"contents":"买礼物送亲戚小孩, 想来想去适合的最后还是乐高, 然后给买了 42075, 收到货时候发现自己拉自己下水了, 购物车已经填满了好几款喜欢的.\n综合各种考虑, 最后入了 42077,\n 外观满意, 其实星战和幻影忍者系列好几款的外观都很喜欢, 但是, 出于考虑后期就没选了, 大小满意, 要说外观的话那还是布加迪来的拉风, 但是, 比起本体的售卖价格, 更在意的是大小, 看介绍长度和重量都玩不来, 尤其长度, 按相机器材经验, 越是大件玩的机会就越低, 除此之外, 比本体更昂贵的, 是得付出更昂贵的代价去容纳之. 后期可玩性, 改装成车灯, 车门, 遥控驱动什么的. 价格可以(700+)  小时候没那个条件玩, 玩具都没多一件, 等知道有乐高这东西的时候已经不小了, 所以, 也谈不上情怀, 但是拿到手还是很兴奋的呢.\n拆开盒子倒出来以后, 兴奋的同时, 多了一份茫然,虽然有说明书, 但是, 打开说明书一看, 别说中文, 英文都没几个. 研究了一会才知道说明书怎么看 = 。 = 智商堪忧\n之前了解过说明书会按步骤把零件分装好, 并在包装袋上有标记, 看漏了一句话, 只是其中一部分, 明显这个 42077 不在这部分里边, 猜测 Technic 的应该都没标? \u0008(还是特意各种搜索了一番别人的开箱, 证实都是 unnumbered)\n1 : 1 真的就是一比一, 方便不用去数有几个孔(虽然旁边的确标着有几个), 拿到看着像的零件放在上面比对一致就是了.\n有些数字就不一定是代表几个孔, 比如这里, 22 对应的是零件上的 22.\n1x, 2x, 3x\u0026hellip; 标识的是这个步骤会用到几个.\n原本应该有更多图得瑟的, 唉\u0026hellip;\n最后只剩成品图片了\n打开就看到我的 「V6」\n近距离看看避震\n鸟瞰\n从开箱到拼好, 断断续续拼了几个晚上加起来估计有十多个小时了 (包括拼几下拍个照得瑟, 还有贴贴纸的)\nLast but not least, 一定要先把收纳盒准备好, 尤其这种没分好步骤的, 先分类好再砌, 不然找到眼瞎, 分类好再拼目测可以节省一半的时间.\n某宝上买的收纳盒, 感觉还是买小了, 42077 的 1000 片出头勉强能塞得进去吧, 四颗车轮除外, 质量还可以, 但是提着总怕闪架碎了一地.\n最最后, 手指拼残了, 机械系列的结构跟其他的乐高砖块差别还是大大的.\n拼完以后还剩这么点了\n更新: 42039 的收纳\n","permalink":"https://xguox.me/lego-42077-open-box.html/","tags":["Lego"],"title":"入坑乐高 42077"},{"categories":["Tools","Linux"],"contents":"查看 TCP 端口被哪个进程(PID)占用了\n1  lsof -nP -i4TCP:$PORT | grep LISTEN  ","permalink":"https://xguox.me/tcp-port-pid.html/","tags":["Tools","Linux"],"title":"查看 TCP 端口号占用"},{"categories":["Go"],"contents":"有一些 model 结构没有嵌套 gorm.Model, 只是自己自定义了 CreatedAt UpdatedAt, 然后插入数据的时候都会插两次, 原因是漏了 ID, 准确的说是漏了 primary_key = . =\n","permalink":"https://xguox.me/gorm-model-primary-key.html/","tags":["Go"],"title":"Gorm 笔记"},{"categories":["Go"],"contents":" NSQ 是一个基于 Go 写的实时分布式消息平台, 打开 NSQ 的官网可以看到简单粗暴的排版介绍四大优势, Distributed(分布式), Scalable(可拓展), Ops Friendly(对运维友好), Integrated(易集成).\nmacOS 上安装 NSQ 用 brew install nsq 一句就可以了, 或者到 NSQ 的 Github Releases 下载下来把可执行文件复制到 PATH 也行.\n官方文档的快速使用:\n 打开第一个 Shell  1  $ nsqlookupd    打开第二个 Shell  1  $ nsqd --lookupd-tcp-address=127.0.0.1:4160    第三个 Shell  1  $ nsqadmin --lookupd-http-address=127.0.0.1:4161    发布一条初始化数据, 并在集群中创建一个 topic(如果不存在):  1  $ curl -d \u0026#39;hello world 1\u0026#39; \u0026#39;http://127.0.0.1:4151/pub?topic=test\u0026#39;    再开一个 Shell 执行 nsq_to_file  1  $ nsq_to_file --topic=test --output-dir=/tmp --lookupd-http-address=127.0.0.1:4161    发布更多消息到 nsqd  1 2  $ curl -d \u0026#39;hello world 2\u0026#39; \u0026#39;http://127.0.0.1:4151/pub?topic=test\u0026#39; $ curl -d \u0026#39;hello world 3\u0026#39; \u0026#39;http://127.0.0.1:4151/pub?topic=test\u0026#39;    打开 http://127.0.0.1:4171/ 可以看到管理的 UI, 验证刚刚执行的一些数据, 也可以查看 /tmp 里边 test.*.log 的内容  Topic and Channels NSQ 的消息传递支持 multicast(多播) 和 load-balanced(负载均衡) 两种方式组合的消息路由\nmulticast: 一则消息的发布会被所有订阅者接收到 load-balanced: 一则消息的发布会只会被其中的一个订阅者接收到\n当一个 consumer 被创建以后, 订阅的是 topic/channel 的组合, 而当 producer 被创建以后, 所发布的消息是到 topic 这一层的, 然后再复制到每一个不同的 channel.\n比如, 有 Consumer1, Consumer2, Consumer3 都订阅了 a_topic/a_channel, 当 Producer1 发布消息到 a_topic 时, 每一则只会被这三个 Consumer 之中的一个接收到, 当发布三则消息的时候, 每个 Consumer 都收到一个. 这是 load-balanced(负载均衡).\n再有, 假设当 Consumer1 订阅了 a_topic/channel1, Consumer2 订阅了 a_topic/channel2, Consumer3 订阅了 a_topic/channel3, 此时, 每次 Producer1 发布消息到 a_topic, 这三个 Consumer 都将接收到. 这是 multicast(多播).\n组合起来, 官网的例子,\n Consumer1 订阅了 clicks/metrics Consumer2 订阅了 clicks/metrics Consumer3 订阅了 clicks/metrics Consumer4 订阅了 clicks/spam_analytics Consumer5 订阅了 clicks/archives  当 Producer 发布一则消息 A, Consumer 1/2/3 之中的一个(动图中的第二个 Consumer)会接收到, 另外 Consumer4 和 Consumer5 也都会收到消息 A. 当发布消息 B 时, Consumer4 和 Consumer5 都会接收到消息 B, 而 Consumer 1/2/3 依然只有一个会接收到消息 B(动图中的第一个 Consumer),\nNSQ 自带有一系列的 helper 应用, nsqlookupd, 用来管理 nsqd 所发布的 topics 以方便客户端发现与查找并且对感兴趣的 topic 进行订阅. 解耦发布与订阅之间的依赖关系, 各自做好自己的事就够了, 有什么都冲着 nsqlookupd 这个进程去.\n\u0008可以通过 nsqlookupd --help 查询到详细的用法, 各个参数的作用. 最主要的就是 -http-address(nsqadmin 各种管理用, 默认 0.0.0.0:4161) 和 -tcp-address(nsqd 用, 默认 0.0.0.0:4160)\nnsqd 是一个负责处理消息的接收, 排队, 以及投递给客户端的守护进程. 尽管 nsqd 可以独立运行, 但是通常和 nsqlookupd 实例配置再一个集群中. 这样的 nsqd 进程会有一个与 nsqlookupd 的 TCP 长连接, 间隔定时往 nsqlookupd 推送自己的状态信息, 从而 nsqlookupd 就可以告知用户 nsqd 的地址信息.\nnsqd 会默认监听一个 tcp 端口(4150)和一个 http 端口(4151), 和可选的 https 端口.\nnsqadmin, 简单拿 bootstrap 包装了一下各种管理统计数据的 Web UI.\nhello world 例子中的 nsq_to_file 创建一个订阅指定 topic 的 Consumer, 并写入指定的 file, 除了这个之外还有 nsq_to_http, nsq_to_nsq\ngo-nsq 官方包 https://github.com/nsqio/go-nsq, 上面提到的 nsq_to_file 的, 本身就是拿这个官方包写的 = 。 = 好吧, nsq 就是 Go 写的.\nProducer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // NewConfig: // This must be used to initialize Config structs. The only valid way to create a Config is via NewConfig. Values can be set directly, or through Config.Set() // c.Set(\u0026#34;tls_v1\u0026#34;, true) // c.Set(\u0026#34;tls-insecure-skip-verify\u0026#34;, true) // c.Set(\u0026#34;tls-min-version\u0026#34;, \u0026#34;tls1.2\u0026#34;) // c.Set(\u0026#34;local_addr\u0026#34;, \u0026#34;1.2.3.4:27015\u0026#34;) // c.Set(\u0026#34;dial_timeout\u0026#34;, \u0026#34;5s\u0026#34;) config := nsq.NewConfig() // After Config is passed into NewProducer the values are no longer mutable (they are copied). p, err := nsq.NewProducer(\u0026#34;127.0.0.1:4150\u0026#34;, config) if err != nil { log.Fatal(err) } // if err := p.Publish(\u0026#34;fuji\u0026#34;, []byte(\u0026#34;X-T3\u0026#34;)); err != nil { // log.Fatal(\u0026#34;publish error: \u0026#34; + err.Error()) // } for { // synchronously publishes a message body to the specified topic  if err := p.Publish(\u0026#34;test\u0026#34;, []byte(\u0026#34;test message\u0026#34;)); err != nil { log.Fatal(\u0026#34;publish error: \u0026#34; + err.Error()) } time.Sleep(1 * time.Second) }   Consumer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  config := nsq.NewConfig() // NewConsumer creates a new instance of Consumer for the specified topic/channel // After Config is passed into NewConsumer the values are no longer mutable (they are copied). consumer, _ := nsq.NewConsumer(\u0026#34;fuji\u0026#34;, \u0026#34;channel1\u0026#34;, config) // AddHandler sets the Handler for messages received by this Consumer. This can be called // multiple times to add additional handlers. Handler will have a 1:1 ratio to message handling goroutines. consumer.AddHandler(nsq.HandlerFunc(func(message *nsq.Message) error { log.Printf(\u0026#34;收到: %v\u0026#34;, message) log.Printf(\u0026#34;Body: %v\u0026#34;, string(message.Body)) if string(message.Body) == \u0026#34;X-T3\u0026#34; { consumer.Stop() } return nil })) // ConnectToNSQLookupd adds an nsqlookupd address to the list for this Consumer instance. // // If it is the first to be added, it initiates an HTTP request to discover nsqd // producers for the configured topic. // // A goroutine is spawned to handle continual polling.  err := consumer.ConnectToNSQLookupd(\u0026#34;127.0.0.1:4161\u0026#34;) if err != nil { log.Panic(\u0026#34;连接失败\u0026#34;) } // func (r *Consumer) ConnectToNSQLookupds(addresses []string) error {  // ConnectToNSQD takes a nsqd address to connect directly to. // func (r *Consumer) ConnectToNSQD(addr string) error {  // read from this channel to block until consumer is cleanly stopped \u0026lt;-consumer.StopChan // Stop will initiate a graceful stop of the Consumer (permanent) // // NOTE: receive on StopChan to block until this process completes // func (r *Consumer) Stop() {  ","permalink":"https://xguox.me/go-nsq-note-i.html/","tags":["Go"],"title":"NSQ 笔记"},{"categories":["Go"],"contents":" 当缓存的空间即将达到临界值的时候, 需要将一些旧的数据清理掉, 哪些该去, 哪些该留, 常用的缓存淘汰策略有下面三种:\n FIFO(First In，First Out) 先进先出策略 LFU(Least Frequently Used) 最少使用策略 LRU(Least Recently Used) 最近最少使用策略  这里基于 Go 分别使用单向链表(Singly Linked List)和双向链表(Doubly Linked List)实现最后的这个 LRU 最近最少使用策略.\n各种术语加上英文缩写看着好像很流弊, 用人话描述这个策略的基本思路其实也不难理解:\n 新的数据插入到链表头部 当链表中的数据被访问以后, 将该数据重置到链表头部 当链表达到临界值时候, 将链表尾部的数据丢弃  单向链表(Singly Linked List) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115  package main import \u0026#34;fmt\u0026#34; type SinglyLink struct { head *Node capacity int size int } type Node struct { Value int Next *Node } func (s *SinglyLink) prependNode(val int) { currentHead := s.head s.size++ if currentHead == nil { s.head = \u0026amp;Node{val, nil} return } s.head = \u0026amp;Node{val, currentHead} } func (s *SinglyLink) deleteLastNode() { if s.size \u0026lt;= 1 { s.head = nil s.size = 0 return } tmp := s.head for tmp.Next.Next != nil { tmp = tmp.Next } tmp.Next = nil s.size-- } func (s *SinglyLink) deleteNode(val int) { if s.size == 0 { return } currentNode := s.head if currentNode.Value == val { s.head = currentNode.Next s.size-- return } for currentNode.Next != nil { if currentNode.Next.Value == val { currentNode.Next = currentNode.Next.Next s.size-- return } currentNode = currentNode.Next } } func (s *SinglyLink) searchNode(val int) { currentNode := s.head for currentNode != nil { if currentNode.Value == val { s.deleteNode(val) s.prependNode(val) return } nextNode := currentNode.Next if nextNode == nil { s.checkFull(val) return } currentNode = nextNode } } func (s *SinglyLink) checkFull(val int) { isFull := s.size \u0026gt;= s.capacity if isFull { s.deleteLastNode() } s.prependNode(val) } func (s *SinglyLink) print() { tmp := s.head for tmp != nil { fmt.Printf(\u0026#34;%d -\u0026gt; \u0026#34;, tmp.Value) tmp = tmp.Next } fmt.Println() } func newSinglyLink(capacity int) *SinglyLink { return \u0026amp;SinglyLink{nil, capacity, 0} } func main() { lru := newSinglyLink(3) lru.prependNode(1) lru.prependNode(0) lru.prependNode(2) lru.searchNode(7) lru.print() // 7 -\u0026gt; 2 -\u0026gt; 0 -\u0026gt;  lru.searchNode(4) lru.print() // 4 -\u0026gt; 7 -\u0026gt; 2 -\u0026gt;  lru.searchNode(22222) lru.searchNode(2) lru.searchNode(99) lru.print() // 99 -\u0026gt; 2 -\u0026gt; 22222 -\u0026gt; }   实际上, 也不会有人只用一个单向链表来实现 LRU (练习用 Go 写链表 = 。 = ), 插入和删除本身的时间复杂度虽然都是 O(1), 但是, 如果要插入或者删除特定位置的节点, 还得遍历查找, 时间复杂度 O(n). 所以, 一般都会连同使用哈希表来优化查找.\n双向链表(Doubly Linked List) 事实是, Go 已经内建有双向链表, 所以, 直接用就是了. 😜\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92  package main import ( \u0026#34;container/list\u0026#34; \u0026#34;fmt\u0026#34; ) type LruCache struct { capacity int items map[string]*list.Element link *list.List } type Item struct { key string value interface{} } func NewLruCache(capacity int) *LruCache { return \u0026amp;LruCache{ capacity: capacity, items: make(map[string]*list.Element), link: list.New(), } } func (l *LruCache) Get(key string) (value interface{}, exists bool) { if item, exists := l.items[key]; exists { l.link.MoveToFront(item) // moves element e to the front of list l. If e is not an element of l, the list is not modified. The element must not be nil. \tvalue = item.Value.(*Item).value } return } func (l *LruCache) Set(key string, value interface{}) { if item, exists := l.items[key]; exists { // 移到头结点并更新值 \tl.link.MoveToFront(item) item.Value.(*Item).value = value return } item := l.link.PushFront(\u0026amp;Item{key, value}) // inserts a new element at the front of list \tl.items[key] = item if l.link.Len() \u0026gt; l.capacity { l.DeleteOldest() } } func (l *LruCache) DeleteOldest() { item := l.link.Back() // returns the last element of list or nil if the list is empty. \tif item != nil { l.removeItem(item) } } func (l *LruCache) delete(key string) { if item, exists := l.items[key]; exists { l.removeItem(item) } } func (l *LruCache) removeItem(el *list.Element) { l.link.Remove(el) // removes el from list if el is an element of the list. It returns the element value el.Value. The element must not be nil.  item := el.Value.(*Item) delete(l.items, item.key) } func (l *LruCache) print() { tmp := l.link.Front() // returns the first element of list or nil if the list is empty. \tfor tmp != nil { fmt.Printf(\u0026#34;%v -\u0026gt; \u0026#34;, tmp.Value) tmp = tmp.Next() // returns the next list element \t} fmt.Println() } func main() { lru := NewLruCache(5) lru.Get(\u0026#34;s\u0026#34;) lru.print() lru.Set(\u0026#34;o\u0026#34;, 90) lru.Set(\u0026#34;n\u0026#34;, 200) lru.Set(\u0026#34;y\u0026#34;, \u0026#34;fujifilm\u0026#34;) lru.Get(\u0026#34;n\u0026#34;) lru.print() // \u0026amp;{n 200} -\u0026gt; \u0026amp;{y fujifilm} -\u0026gt; \u0026amp;{o 90} -\u0026gt; \tlru.Set(\u0026#34;x\u0026#34;, \u0026#34;xt3\u0026#34;) lru.Set(\u0026#34;gfx\u0026#34;, \u0026#34;50s\u0026#34;) lru.Set(\u0026#34;sony\u0026#34;, \u0026#34;a7r\u0026#34;) lru.Get(\u0026#34;gfx\u0026#34;) lru.print() // \u0026amp;{gfx 50s} -\u0026gt; \u0026amp;{sony a7r} -\u0026gt; \u0026amp;{x xt3} -\u0026gt; \u0026amp;{n 200} -\u0026gt; \u0026amp;{y fujifilm} -\u0026gt; }   最后, 这里的实现并不是并发安全的 = 。 =\n","permalink":"https://xguox.me/go-linked-list-lru-cache.html/","tags":["Go"],"title":"Golang, 链表, LRU 缓存淘汰策略"},{"categories":[],"contents":"因为两位大佬的相继离开, 加之最近不太平, 想起从又一客走的时候, 然后倒回去一看, 2016-10-14, 两年后的 10 月 14, 又来说一句 Farewell, 只是这一次不是我离开.\n大概是 2016 的9月末, 老板说是用 Skype 面试, 心想, 要不要这么高端 = 。 = 这玩意听倒是听过不少, 只是没用过啊, 不能被唬住啊, 先装个试试.\n遗憾当时视频没有截图, 不过还记得 GTO 穿着一件球衣, 哦, 还有 GTO 的丈母娘(那会还不是). 问过啥「has_many 和 has_and_belongs_to_many 的区别」这 ** 问题, 我现在都不会回答怎么办.\n后面还转了半天车去了一趟 905, 了解了些啥不记得了, 但是, YIO 被 GTO 吐槽比较菜这还记得. 最后因为期望薪资被老板砍了 2K, 加之交通好蛋疼, 加之社保问题, 还有类似第一份工作的环境, \u0008然后还是没有去成.\n每次 GTO 说我当时嫌弃咱公司小的时候, 我都回应一句, 说的好像我现在不嫌弃那样.\n最后, 还是与炮哥一起各种被神撮合, 几经折腾以后, 阴差阳错的还是来了. 这其中就有了一直坐在我旁边的追风少年菜大神, 哦, 还有车神拓海.\n每一份工作, 遇到各种各样的同事, 有一些话没多几句, 微信都没加, 有一些疯言疯语, 吃喝玩乐. 差别甚大.\n天下无不散筵席, 这些的都随着 905 离开了.\n","permalink":"https://xguox.me/farewell-2.html/","tags":[],"title":"Farewell 2"},{"categories":["Go"],"contents":"Go 可以给任何的常量, 变量, 函数, 类型设置别名. 对于写惯 Ruby 的 Alias(其实用的也不多), 第一感觉应该是不带等号的才是类型别名, 然后, Go 的累习惯别名刚好相反.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  import \u0026#34;fmt\u0026#34; type A struct{} type AliasA = A // 类型别名 type B A // 类型定义  type C int type D = int func (a AliasA) testA() { fmt.Println(\u0026#34;test A\u0026#34;) } func (b B) testB() { fmt.Println(\u0026#34;test B\u0026#34;) } func (c *C) testC() { fmt.Println(\u0026#34;test C\u0026#34;) } // cannot define new methods on non-local type int  // func (d *D) testD() { // fmt.Println(\u0026#34;test D\u0026#34;) // }  func main() { var a = A{} var aa AliasA = a // var b1 B = a // cannot use a (type A) as type B in assignment  var b2 B a.testA() // test A  aa.testA() // test A  b2.testB() // test B  var c C c.testC() // var d D  // d.testD() // d.testD undefined (type int has no field or method testD)  }   如果是被别名的类型(= . = )跟别名不是再同一个包的话, 则不能为其添加新的方法, 比如这里 type D = int, 不能再给 D 定义任何的方法了, 因为 D 和基础数据类型 int 不在同一个包里边. (我理解的non-local type)\n类型声明与类型别名最大区别在于: 类型别名和原类型是相同的, 而类型声明和原类型则是完全不同的两个东西, 只不过, 类型声明的新类型拥有与原类型相同的字段结构, 但, 不拥有任何原类型的方法.\n https://go.googlesource.com/proposal/+/master/design/16339-alias-decls.md  ","permalink":"https://xguox.me/go-type-alias-vs-type-definition.html/","tags":["Go"],"title":"Golang 类型声明与类型别名"},{"categories":["多肉与植物"],"contents":"","permalink":"https://xguox.me/succulent-plant-i.html/","tags":["多肉与植物"],"title":"第一批来到的多肉 I"},{"categories":["多肉与植物"],"contents":"","permalink":"https://xguox.me/succulent-plant-ii.html/","tags":["多肉与植物"],"title":"第一批来到的多肉 II"},{"categories":["多肉与植物"],"contents":"","permalink":"https://xguox.me/succulent-plant-iii.html/","tags":["多肉与植物"],"title":"第一批来到的多肉 III"},{"categories":["Go"],"contents":" Decorator(修饰, 装饰, 虽然都是这么翻译, 但是怎么看怎么别扭, 所以还是用英文好了) Pattern, 是之前提到的代理模式(Proxy Pattern)的大兄弟. 用起来挺相似的. 都是在不改变原本的旧有类型的代码前提下, 对该类型的功能进行拓展或者修改.\nhttps://zh.wikipedia.org/wiki/修饰模式#介绍 \u0026gt; 通过使用修饰模式，可以在运行时扩充一个类的功能。原理是：增加一个修饰类包裹原来的类，包裹的方式一般是通过在将原来的对象作为修饰类的构造函数的参数。装饰类实现新的功能，但是，在不需要用到新功能的地方，它可以直接调用原来的类中的方法。修饰类必须和原来的类有相同的接口。\n 修饰模式是类继承的另外一种选择。类继承在编译时候增加行为，而装饰模式是在运行时增加行为。\n当有几个相互独立的功能需要扩充时，这个区别就变得很重要。在有些面向对象的编程语言中，类不能在运行时被创建，通常在设计的时候也不能预测到有哪几种功能组合。这就意味着要为每一种组合创建一个新类。相反，修饰模式是面向运行时候的对象实例的,这样就可以在运行时根据需要进行组合。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  package main import \u0026#34;fmt\u0026#34; // step1: 编写基础功能，刚开始不需要定义接口 type Base struct { } func (b *Base) Call() string { return \u0026#34;base is called\u0026#34; } // step2: 将上面的方法声明为接口类型，基础功能中的 Call() 调用自动满足下面的接口 type DecoratorI interface { Call() string } // step3: 编写新增功能，结构中保存接口类型的参数 type Decorator struct { derorator DecoratorI } func (d *Decorator) Call() string { return \u0026#34;decorator: \u0026#34; + d.derorator.Call() } func main() { base := \u0026amp;Base{} fmt.Println(base.Call()) decorator := Decorator{base} fmt.Println(decorator.Call()) }   咋一看其实还真跟 Proxy Pattern 就是一个东西的样子, 但是, 其实 Decorator Pattern 胜在更灵活一些. Proxy Pattern 在编译时候就要定义好新功能的样子并且不能改变, 而 Decorator Pattern 可以在运行时动态的根据需要进行组合. 换一个纬度说, Proxy Pattern 更像是对旧有类型做的一个访问控制.\nDecorator Pattern 的实际用例 1 2 3 4 5 6 7 8 9 10 11  type MyServer struct{} func (m *MyServer) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello Decorator!\u0026#34;) } func main() { http.Handle(\u0026#34;/\u0026#34;, \u0026amp;MyServer{}) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) }   加上 Logger 中间件, 修饰这个 MyServer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  type LoggerServer struct { Handler http.Handler LogWriter io.Writer } func (s *LoggerServer) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(s.LogWriter, \u0026#34;Request URI: %s\\n\u0026#34;, r.RequestURI) fmt.Fprintf(s.LogWriter, \u0026#34;Host: %s\\n\u0026#34;, r.Host) fmt.Fprintf(s.LogWriter, \u0026#34;Content Length: %d\\n\u0026#34;, r.ContentLength) fmt.Fprintf(s.LogWriter, \u0026#34;Method: %s\\n\u0026#34;, r.Method)fmt.Fprintf(s.LogWriter, \u0026#34;--------------------------------\\n\u0026#34;) s.Handler.ServeHTTP(w, r) } func main() { http.Handle(\u0026#34;/\u0026#34;, \u0026amp;LoggerServer{ LogWriter:os.Stdout, Handler:\u0026amp;MyServer{}, }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) }   如果需要可以再继续嵌套更多的中间件, 最后都是调用 s.Handler.ServeHTTP(w, r)\n","permalink":"https://xguox.me/go-decorator-design-pattern.html/","tags":["Go"],"title":"Go 的修饰模式(Decorator Pattern)"},{"categories":["Go"],"contents":"相比之前写的组合模式, 代理模式实现起来并不需要太费劲.\n主要特性:\n 隐藏或限制被代理对象 易于为被代理的对象提供新的抽象层(拦截, 重定义)  https://github.com/tmrts/go-patterns/blob/master/structural/proxy.md\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // To use proxy and to object they must implement same methods type IObject interface { ObjDo(action string) } // Object represents real objects which proxy will delegate data type Object struct { action string } // ObjDo implements IObject interface and handel\u0026#39;s all logic func (obj *Object) ObjDo(action string) { // Action behavior  fmt.Printf(\u0026#34;I can, %s\u0026#34;, action) } // ProxyObject represents proxy object with intercepts actions type ProxyObject struct { object *Object } // ObjDo are implemented IObject and intercept action before send in real Object func (p *ProxyObject) ObjDo(action string) { if p.object == nil { p.object = new(Object) } if action == \u0026#34;Run\u0026#34; { p.object.ObjDo(action) // Prints: I can, Run  } }   https://github.com/bvwells/go-patterns/blob/master/structural/proxy.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  package structural import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; ) var outputWriter io.Writer = os.Stdout // modified during testing  // ITask is an interface for performing tasks. type ITask interface { Execute(taskType string) } // Task implements the ITask interface for performing tasks. type Task struct { taskName string } // Execute implements the task. func (t *Task) Execute(taskType string) { fmt.Fprint(outputWriter, \u0026#34;Performing task type: \u0026#34;+taskType) } // ProxyTask represents a proxy task with re-routes tasks. type ProxyTask struct { task *Task } // NewProxyTask creates a new instance of a ProxyTask. func NewProxyTask() *ProxyTask { return \u0026amp;ProxyTask{task: \u0026amp;Task{}} } // Execute intercepts the Execute command and re-routes it to the Task Execute command. func (t *ProxyTask) Execute(taskType string) { if taskType == \u0026#34;Run\u0026#34; { t.task.Execute(taskType) } }   https://github.com/monochromegane/go_design_pattern/blob/master/proxy/proxy.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  package proxy type printable interface { SetPrinterName(name string) GetPrinterName() string Print(str string) string } type printer struct { name string } func (self *printer) SetPrinterName(name string) { self.name = name } func (self *printer) GetPrinterName() string { return self.name } func (self *printer) Print(str string) string { return self.name + \u0026#34;:\u0026#34; + str } type PrinterProxy struct { Name string real *printer } func (self *PrinterProxy) SetPrinterName(name string) { if self.real != nil { self.real.SetPrinterName(name) } self.Name = name } func (self *PrinterProxy) GetPrinterName() string { return self.Name } func (self *PrinterProxy) Print(str string) string { self.realize() return self.real.Print(str) } func (self *PrinterProxy) realize() { if self.real == nil { self.real = \u0026amp;printer{self.Name} } }  ","permalink":"https://xguox.me/go-proxy-design-pattern.html/","tags":["Go"],"title":"Go 的代理模式(Proxy Pattern)"},{"categories":["Go"],"contents":" Go 没有传统面向对象语言(如 Ruby, Java) 的继承特性, 取而代之, 更多的是用 组合模式 来达到类似效果.\n组合设计模式 Composite Design Pattern 组合构建的是一个树形的层级对象, 一个对象包含有其他一些拥有各自独立的字段和方法的对象. 换一个说法讲, 组合代表「拥有 has」 关系, 而继承则代表「是 is」关系. 这种模式可以解决(多)继承的问题, 典型的比如, 两个实体分别继承自两个不同的类, 而这两个实体之间实际上并没有任何关联关系.\n举个例子, Athlete(运动员)类有一个 Train(训练)方法, 然后 SwimmerAthlete(游泳运动员) 继承自 Athlete, 并有自己的 Swim(游泳)方法. \u0008\u0008可能还会有一个 Rider(骑手)类也继承自 Athlete, 然后有一个自己的 Ride(骑行)方法.\n另外的, 还有一个 Animal(动物)类, 有 Eat, Sleep 等方法. 然后类似有 Dog(🐶) 类继承于它, 并有着特有的 Bark(叫)方法 一切看起来都没什么特别之处, 很常规的面向对象的继承就可以实现. 但是, 问题来了, 如果有一个 Fish(鱼) 类, 同样也有一个 Swim 方法, 那么就不好办了, Fish 不是 Athlete 啊. Fish 会 Swim 但不会 Train (不抬杠哈). 这时候的最佳实践可能会是定义一个需要实现 Swim 方法的 Swimmer 接口, 然后 SwimmerAthlete 和 Fish 都分别实现这个接口. 但结果即使是一模一样的方法, Swim 还是要被实现两次. 如果有一个 Triathlete(三项全能运动员)类呢? 又得再实现一次一模式样的?\n运用组合模式则可以很好的解决这个问题, 用 Go 有两种实现这种组合的方式, 先来看第一种.\n1 2 3 4 5  type Athlete struct{} func (a *Athlete) Train() { fmt.Println(\u0026#34;Training\u0026#34;) }   接下来建立一个 Swimmer 结构体, 这里加上 A 用以标识是第一种区别于后边的第二种.\n1 2 3 4  type CompositeSwimmerA struct{ MyAthlete Athlete MySwim func() }   CompositeSwimmerA 这个类型有一个 Athlete 类型的 MyAthlete 字段, 和一个 func() 类型的 MySwim 字段.\n定义一个 Swim 方法之后可以赋值到上述的 MySwim 字段.\n1 2 3 4 5 6 7 8 9 10  func Swim(){ fmt.Println(\u0026#34;Swimming!\u0026#34;) } swimmer := CompositeSwimmerA{ MySwim: Swim, } swimmer.MyAthlete.Train() swimmer.MySwim()   这里的 swimmer 对象因为没有赋值给 MyAthlete 字段, 所以默认为Athlete 类型的零值.\n1 2 3  $ go run main.go Training Swimming!   那么鱼呢?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  type Animal struct{} func (r *Animal)Eat() { println(\u0026#34;Eating\u0026#34;) } type Fish struct{ Animal Swim func() } nimo := Fish{ Swim: Swim, } nimo.Eat() nimo.Swim()   「继承」了对应的类型并拥有公用的 Swim 方法\n1 2 3  $ go run main.go Eating Swimming!   这里 Fish 的实现与 Athlete 有一些不一样的地方是用了嵌套类型. 顺便就有了我们前面提的第二种方式.\n定义一个需要实现 Swim 方法的 Swimmer 接口, 以及一个实现了该接口的结构体 SwimmerImpl, 最后嵌套在 CompositeSwimmerB 这个结构体里边(相应鱼的实现也类似).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  type Swimmer interface { Swim() } type Trainer interface { Train() } type SwimmerImpl struct{} func (s *SwimmerImpl) Swim(){ println(\u0026#34;Swimming!\u0026#34;) } type CompositeSwimmerB struct{ Trainer Swimmer }   这种做法的好处是, 里层字段对象更可控, 不会一不小心变成了零值, 使用方式:\n1 2 3 4 5 6 7  swimmer := CompositeSwimmerB{ \u0026amp;Athlete{}, \u0026amp;SwimmerImpl{}, } swimmer.Train() swimmer.Swim()  ","permalink":"https://xguox.me/go-composite-design-pattern.html/","tags":["Go"],"title":"Go 的组合模式(Composite Pattern)"},{"categories":["Go"],"contents":"            一个简单的示例        Gin 默认用就是 go-playground/validator 这个库, 通过 tag 可以设置结构体字段的校验规则, go-playground/validator 自带了差不多一百种吧, 比如必须有值(required), 验证长度(len), 有效邮箱(email), 如果这些都还不能满足的话还能够根据需要自定义.\n1 2 3 4  type Category struct { Name string `form:\u0026#34;name\u0026#34; json:\u0026#34;name\u0026#34; binding:\u0026#34;required\u0026#34;` Slug string `form:\u0026#34;slug\u0026#34; json:\u0026#34;slug\u0026#34; binding:\u0026#34;required\u0026#34;` }   这里 binding 是 v8 版本的写法, 也就是 gin 当前(2018.9)引用的 validator 版本, 但是, go-playground/validator 早就更新到了 v9 了, 并且 binding 换成了 validate, 还有其他一些用法因为变动比较大, 虽然老早有人提了 PR 但是目前貌似还合不了. go-playground/validator 索性自己给了一个升级方案出来.\nv8 的自定义规则类似长这样的\n1 2 3 4 5 6 7 8 9 10 11 12  func bookableDate( v *validator.Validate, topStruct reflect.Value, currentStructOrField reflect.Value, field reflect.Value, fieldType reflect.Type, fieldKind reflect.Kind, param string, ) bool { if date, ok := field.Interface().(time.Time); ok { today := time.Now() if today.Year() \u0026gt; date.Year() || today.YearDay() \u0026gt; date.YearDay() { return false } } return true }   一堆反射的参数看着头都晕了, 换成了 v9 以后简洁多了, 反射什么的按需自取就是了, 结合 gorm 写的一个判断数据库字段唯一的自定义规则\n1 2 3 4 5 6 7 8 9 10 11  func ValidateUniq(fl validator.FieldLevel) bool { var result struct{ Count int } currentField, _, _ := fl.GetStructFieldOK() table := modelTableNameMap[currentField.Type().Name()] // table name  value := fl.Field().String() // value  column := fl.FieldName() // column name  sql := fmt.Sprintf(\u0026#34;select count(*) from %s where %s=\u0026#39;%s\u0026#39;\u0026#34;, table, column, value) db.PG.Raw(sql).Scan(\u0026amp;result) dup := result.Count \u0026gt; 0 return !dup }   一个简单的示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  package model import ( \u0026#34;coconut/db\u0026#34; \u0026#34;fmt\u0026#34; validator \u0026#34;gopkg.in/go-playground/validator.v9\u0026#34; \u0026#34;github.com/gin-gonic/gin/binding\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/jinzhu/gorm\u0026#34; ) type Category struct { gorm.Model Name string `form:\u0026#34;name\u0026#34; json:\u0026#34;name\u0026#34; binding:\u0026#34;required,is-uniq\u0026#34;` Slug string `form:\u0026#34;slug\u0026#34; json:\u0026#34;slug\u0026#34; binding:\u0026#34;required\u0026#34;` } // CATEGORY VALIDATOR type CategoryValidator struct { CategoryModel Category `json:\u0026#34;category\u0026#34;` } func (s *CategoryValidator) Bind(c *gin.Context) error { b := binding.Default(c.Request.Method, c.ContentType()) err := c.ShouldBindWith(s, b) if err != nil { return err } return nil } func ValidateUniq(fl validator.FieldLevel) bool { var result struct{ Count int } currentField, _, _ := fl.GetStructFieldOK() table := modelTableNameMap[currentField.Type().Name()] // table name  value := fl.Field().String() // value  column := fl.FieldName() // column name  sql := fmt.Sprintf(\u0026#34;select count(*) from %s where %s=\u0026#39;%s\u0026#39;\u0026#34;, table, column, value) db.PG.Raw(sql).Scan(\u0026amp;result) dup := result.Count \u0026gt; 0 return !dup }   方法 Bind 返回的 error 是结构体下所有违规的字段错误, 所以可以这么处理(v9)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  type CommonError struct { Errors map[string]interface{} `json:\u0026#34;errors\u0026#34;` } func NewValidatorError(err error) CommonError { res := CommonError{} res.Errors = make(map[string]interface{}) errs := err.(validator.ValidationErrors) for _, e := range errs { res.Errors[e.Field()] = e.ActualTag() } return res }  ","permalink":"https://xguox.me/go-gin-validator.html/","tags":["Go"],"title":"Gin 模型验证 Validator"},{"categories":["Go"],"contents":" Go 的包依赖管理, 终于在前几天发布的 1.11 有了一个初步官方定数. 再也不用纠结于离不开 $GOPATH, 1.11 发布之前试用了一下 vgo, 文档比较少, 踩了一些坑就没深入继续了. 这次正式发布的 1.11 其实就是把 vgo 给正式合并进来了.\n在 $GOPATH 之外使用 go modules, 如果是现有项目的话可以直接 go mod init, 现有项目会根据 git remote 自动识别 module 名, 但是新项目的话就会报 go: cannot determine module path for source directory, 需要带上 module 名.\n$GOPATH/pkg/ 下会多了一个 mod 目录, 再也不是像 go get 下载到 $GOPATH/src\nRelated: Introduction to Go Modules\nBrad Fitzpatrick\n","permalink":"https://xguox.me/go-111-modules-vgo.html/","tags":["Go"],"title":"Go 1.11 modules"},{"categories":[],"contents":"翻身\n","permalink":"https://xguox.me/turn-over.html/","tags":[],"title":"Turn Over"},{"categories":["Go"],"contents":"用 Gin 开始写点 API 的东西, 然后就陷入了不断\n⌘ + c\ngo build .\n./xxx\n= . =\n然后搜索一把发现有不少推荐 realize 这个工具的, 刚开始看文档半天都不知道这是怎么个玩意, 用法总是说的模棱两可的, 捣腾一番也算能用了. 吐槽文档的其实也不少的.\n不过其实要做的功夫也很少, 在项目根目录初始化一个 .realize.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  settings:legacy:force:falseinterval:0sschema:-name:whateverpath:.commands:run:status:truewatcher:extensions:-gopaths:-/scripts:-type:aftercommand:./your_binary_file_nameoutput:trueignored_paths:-.git-.realize-vendor   就完事了, 讲真那堆参数大部分都是靠蒙的 = . =\n启动就跑\nrealize start --run\n结果就是酱紫\n","permalink":"https://xguox.me/golang-gin-hot-reload.html/","tags":["Go"],"title":"gin-gonic/gin Hot Reload"},{"categories":["Go"],"contents":" Effective Go\nnew 用来分配内存的内建函数, 不会初始化内存, 只会将内存置零. new(T) 会为类型为 T 的新项分配已置零的内存空间, 并返回它的地址, 也就是一个类型为 *T 的值. 用 Go 的术语来说, 它返回一个指针, 该指针指向新分配的, 类型为 T 的零值.\n既然 new 返回的内存已置零, 那么当你设计数据结构时, 每种类型的零值就不必进一步初始化了, 这意味着该数据结构的使用者只需用 new 创建一个新的对象就能正常工作. 例如, bytes.Buffer 的文档中提到 \u0026ldquo;零值的 Buffer 就是已准备就绪的缓冲区. \u0026rdquo; 同样, sync.Mutex 并没有显式的构造函数或 Init 方法, 而是零值的 sync.Mutex 就已经被定义为已解锁的互斥锁了.\nmake 内建函数 make(T, args) 的目的不同于 new(T). 它只用于创建切片(slice)、映射(map)和信道(channel), 并返回类型为 T(而非 *T)的一个已初始化(而非置零)的值. 出现这种用法差异的原因在于, 这三种类型本质上为引用数据类型, 它们在使用前必须初始化. 例如, 切片是一个具有三项内容的描述符, 包含一个指向(数组内部)数据的指针、长度以及容量, 在这三项被初始化之前, 该切片为 nil. 对于切片、映射和信道, make 用于初始化其内部的数据结构并准备好将要使用的值. 例如,\n1  make([]int, 10, 100)   会分配一个具有 100 个 int 的数组空间, 接着创建一个长度为 10, 容量为 100, 并指向该数组中前 10 个元素的切片结构. (生成切片时, 其容量可以省略) 与此相反, new([]int) 会返回一个指向新分配的, 已置零的切片结构, 即一个指向 nil 切片值的指针.\nmake 只适用于映射、切片和信道且不返回指针. 若要获得明确的指针, 请使用 new 分配内存.\n","permalink":"https://xguox.me/golang-new-make-diff.html/","tags":["Go"],"title":"new 与 make 的区别 [Go]"},{"categories":["Protobuf","Go","RPC"],"contents":" 示例代码从官方示例提取过来的.\n1 2 3 4 5 6  // addressbook.proto  syntax = \u0026#34;proto3\u0026#34;; package tutorial; import \u0026#34;google/protobuf/timestamp.proto\u0026#34;; // 导入 proto3 新加的 timestamp 包   对于目标语言为 Go, 除非显式的提供 option go_package 在 .proto 文件中, 否则, 如果指定了 package, 则编译后就是对应的 Go 文件所属的 package 了. 不过官方建议, 不管是否定义了 option go_package, 又或者是 Go 之外其他语言也最好指定这个 package, 这样可以用于防止不同的消息类型有命名冲突.\nThe Protocol Buffer API protoc 编译生成的 addressbook.pb.go 提供了下面这些有用的类型:\n An AddressBook structure with a People field. A Person structure with fields for Name, Id, Email and Phones. A Person_PhoneNumber structure, with fields for Number and Type. The type Person_PhoneType and a value defined for each value in the Person.PhoneType enum.  Writing a Message Protocol Buffers 的出现就是为了序列化数据使其能在其他地方使用. 对于 Go 来说, 序列化数据用的是 proto 库的 Marshal 函数. 对应于 .proto 中的 Message, 在 Go 的呈现是一个指向实现了 proto.Message 接口的结构体的指针. 通过调用 proto.Marshal 得到编码成二进制格式的数据(wire format). 比如这里的 add_person\n1 2 3 4 5 6 7 8 9 10 11  book := \u0026amp;pb.AddressBook{} // ...  // Write the new address book back to disk. out, err := proto.Marshal(book) if err != nil { log.Fatalln(\u0026#34;Failed to encode address book:\u0026#34;, err) } if err := ioutil.WriteFile(fname, out, 0644); err != nil { log.Fatalln(\u0026#34;Failed to write address book:\u0026#34;, err) }   Reading a Message 当需要解码数据的时候, 则用的是 proto 库的 Unmarshal 函数, 比如 list_people 这里从上面的写入文件中读取数据并序列化为 Go 的结构体(book pb.AddressBook).\n1 2 3 4 5 6 7 8 9  // Read the existing address book. in, err := ioutil.ReadFile(fname) if err != nil { log.Fatalln(\u0026#34;Error reading file:\u0026#34;, err) } book := \u0026amp;pb.AddressBook{} if err := proto.Unmarshal(in, book); err != nil { log.Fatalln(\u0026#34;Failed to parse address book:\u0026#34;, err) }   Extending a Protocol Buffer 如果希望在更新了 .proto 能 向后兼容, 那么在新版本的 .proto 中需要做到以下几点:\n 所有已存在的字段的唯一标识号不能改变 可以删除字段, 但是请看第三条 可以添加字段, 但需要使用全新的标识号(从未用过的, 已经用过但字段被删除了也不可以)  Hello World, gRPC Server 示例代码在安装 gRPC go get -u google.golang.org/grpc 的时候就已经有了的,\n1 2 3 4 5 6 7 8 9  cd $GOPATH/src/google.golang.org/grpc/examples/helloworld go run greeter_server/main.go // From a different terminal:  go run greeter_client/main.go =\u0026gt; 2018/08/09 19:59:19 Greeting: Hello world   hello.proto\n如果想要将 Message 用在 RPC(远程过程调用)系统中, 可以在 .proto 文件中定义一个 rpc 服务接口, Protocol Buffer 编译器会像 message 一样编译成对应语言的代码. 比如下面的 helloworld.proto, 定义一个 rpc 方法, 接收 HelloRequest 消息并返回一个 HelloReply 消息:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // The greeting service definition. service Greeter { // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; }   编译成 Go 以后是这里的 helloworld.pb.go. 相比起前半部分的例子, 主要区别是这个\n1 2 3 4  service Greeter { // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) {} }   对应的, 生成的 Go 代码主要多了两个接口:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // GreeterClient is the client API for Greeter service. // // For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream. type GreeterClient interface { // Sends a greeting  SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) } type greeterClient struct { cc *grpc.ClientConn } func NewGreeterClient(cc *grpc.ClientConn) GreeterClient { return \u0026amp;greeterClient{cc} } func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) { out := new(HelloReply) err := c.cc.Invoke(ctx, \u0026#34;/helloworld.Greeter/SayHello\u0026#34;, in, out, opts...) if err != nil { return nil, err } return out, nil } // GreeterServer is the server API for Greeter service. type GreeterServer interface { // Sends a greeting  SayHello(context.Context, *HelloRequest) (*HelloReply, error) }   server 端 main.go\n这里的 pb 是 helloworld 包的别名\n1 2 3 4 5 6 7  // server is used to implement helloworld.GreeterServer. type server struct{} // SayHello implements helloworld.GreeterServer func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { return \u0026amp;pb.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.Name}, nil }   lis, err := net.Listen(\u0026quot;tcp\u0026quot;, port) 绑定端口，然后监听，当有消息到来时，读取并回复给客户端\nGo 的 net 包提供了网络 I/O(TCP/IP, UDP, domain name resolution, Unix domain sockets) 需要用到的接口\n1  s := grpc.NewServer()   1 2 3  pb.RegisterGreeterServer(s, \u0026amp;server{}) // Register reflection service on gRPC server.  reflection.Register(s)   helloworld.pb.go 的函数 RegisterGreeterServer, 实际上调的是 grpc.Server 的实例方法 RegisterService,\n_Greeter_serviceDesc 看名字就是对这个 Greeter rpc 服务的一些描述了,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  var _Greeter_serviceDesc = grpc.ServiceDesc{ ServiceName: \u0026#34;helloworld.Greeter\u0026#34;, HandlerType: (*GreeterServer)(nil), Methods: []grpc.MethodDesc{ { MethodName: \u0026#34;SayHello\u0026#34;, Handler: _Greeter_SayHello_Handler, }, }, Streams: []grpc.StreamDesc{}, Metadata: \u0026#34;helloworld.proto\u0026#34;, } ... func RegisterGreeterServer(s *grpc.Server, srv GreeterServer) { s.RegisterService(\u0026amp;_Greeter_serviceDesc, srv) }   ht := reflect.TypeOf(sd.HandlerType).Elem() 这里也就是 reflect.TypeOf((*pb.GreeterServer)(nil)).Elem() 结果是 helloworld.GreeterServer\nst := reflect.TypeOf(ss) 对应的是 reflect.TypeOf(\u0026amp;server{}), 返回结果是 *main.server\n也就是我们在 main.go 的 server 结构体实现了 helloworld.GreeterServer 这个接口, 所以, st.Implements(ht) 返回 true\n深入下去就不够 hello world 了, 往后看看再深入分析一下, 总的来说就是注册这个服务呗.\n最后这里 s.Serve(lis), 本来想贴源码的, 差不多一百行算了, 贴描述好了\n Serve accepts incoming connections on the listener lis, creating a new ServerTransport and service goroutine for each. The service goroutines read gRPC requests and then call the registered handlers to reply to them. Serve returns when lis.Accept fails with fatal errors. lis will be closed when this method returns. Serve will return a non-nil error unless Stop or GracefulStop is called.\n Client 主要代码就下面这三句:\n1 2 3 4 5 6  // Set up a connection to the server. conn, err := grpc.Dial(address, grpc.WithInsecure()) c := pb.NewGreeterClient(conn) r, err := c.SayHello(ctx, \u0026amp;pb.HelloRequest{Name: name})  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // Dial creates a client connection to the given target. func Dial(target string, opts ...DialOption) (*ClientConn, error) { return DialContext(context.Background(), target, opts...) } // DialContext creates a client connection to the given target. By default, it\u0026#39;s // a non-blocking dial (the function won\u0026#39;t wait for connections to be // established, and connecting happens in the background). To make it a blocking // dial, use WithBlock() dial option. // // In the non-blocking case, the ctx does not act against the connection. It // only controls the setup steps. func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) { ... } // ClientConn represents a client connection to an RPC server. type ClientConn struct { ... }   helloworld.pb.go\n1 2 3 4 5 6 7  type greeterClient struct { cc *grpc.ClientConn } func NewGreeterClient(cc *grpc.ClientConn) GreeterClient { return \u0026amp;greeterClient{cc} }  1 2 3  r, err := c.SayHello(ctx, \u0026amp;pb.HelloRequest{Name: name}) ... log.Printf(\u0026#34;Greeting: %s\u0026#34;, r.Message)   对应的 SayHello 方法和 HelloReply 结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // The response message containing the greetings type HelloReply struct { Message string `protobuf:\u0026#34;bytes,1,opt,name=message,proto3\u0026#34; json:\u0026#34;message,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) { out := new(HelloReply) err := c.cc.Invoke(ctx, \u0026#34;/helloworld.Greeter/SayHello\u0026#34;, in, out, opts...) if err != nil { return nil, err } return out, nil }  ","permalink":"https://xguox.me/grpc-go-protobuf-hello-world-example-explain.html/","tags":["Protobuf","Go","RPC"],"title":"Protobuf, Go, gRPC 的 Hello World 理解"},{"categories":["Protobuf"],"contents":"What are protocol buffers? Protocol Buffers(简称为 Protobuf) 是一种灵活, 高效, 可自动化, 而又不依赖于语言, 不依赖于平台的, 可扩展的用于序列化结构化数据的存储格式. (Google 自己说的, 听完觉得好像很流弊, 不过还是不知道怎么玩.)\n使用后缀为 .proto 文件定义一次数据结构以后, 就可以用自带的代码生成工具, 自动生成各种语言版本的读写这些数据结构的代码. v2 版本的 Protocol Buffers 原生只支持 c++, java, python, 现在的 v3 可以支持语言还多了有 Go, Ruby, Objective-C, JavaScript, PHP 和 C#.\nProtobuf 甚至还可以不用停掉已经部署到线上的程序直接更新数据结构的定义.\n官方的 .proto 简单示例, 定义 Person 类型的 message(在 protobuf 的术语中, 结构化数据被称为 message)\nperson.proto\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  syntax = \u0026#34;proto3\u0026#34;; import \u0026#34;google/protobuf/timestamp.proto\u0026#34;; message Person { string name = 1; int32 id = 2; // Unique ID number for this person. string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 4; google.protobuf.Timestamp last_updated = 5; } // Our address book file is just one of these. message AddressBook { repeated Person people = 1; }   每个 message 类型都有一个或者多个带有唯一编号的字段, 每个字段都有对应的名称和值类型(numbers[integer|floating-point], booleans, strings, raw bytes, 或者其他的 message 类型). proto2 可以指定字段为可选(optional), 必须(required), 或者是重复(repeated), 在 proto3 只剩下 repeated 能用, 默认都是 optional, 没有 required, 而且也不能设置默认值.\n默认值有点类似 Go 的零值设定:\n For strings, the default value is the empty string. For bytes, the default value is empty bytes. For bools, the default value is false. For numeric types, the default value is zero. For enums, the default value is the first defined enum value, which must be 0. For message fields, the field is not set. Its exact value is language-dependent. See the generated code guide for details.  要生成对应语言的数据结构表示, 得先装个 protoc 的编译器. 设置好环境变量以后执行\n1 2 3  protoc -I ./ person.proto --ruby_out=./ # protoc --proto_path=IMPORT_PATH --cpp_out=DST_DIR --java_out=DST_DIR --python_out=DST_DIR --go_out=DST_DIR --ruby_out=DST_DIR --objc_out=DST_DIR --csharp_out=DST_DIR path/to/file.proto   就会自动生成一个 person_pb.rb 的文件, 好奇试了一下生成语言, Java 生成了差不多三千行代码, Go 和 Py 则都是 两百多行, JS 也有好几百行, 唯独 Ruby 只生成了下面这三十来行(是不是图省事对我大 Ruby 不上心啊喂, 好方啊 Σ( ° △ °|||)︴).\n Unlike C++ and Java, Ruby generated code is unaffected by the optimize_for option in the .proto file; in effect, all Ruby code is optimized for code size.\n Py 的指南也有这句, 但是 py 生成的代码也不少的啊.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Generated by the protocol buffer compiler. DO NOT EDIT! # source: person.proto require \u0026#39;google/protobuf\u0026#39; require \u0026#39;google/protobuf/timestamp_pb\u0026#39; Google::Protobuf::DescriptorPool.generated_pool.build do add_message \u0026#34;Person\u0026#34; do optional :name, :string, 1 optional :id, :int32, 2 optional :email, :string, 3 repeated :phones, :message, 4, \u0026#34;Person.PhoneNumber\u0026#34; optional :last_updated, :message, 5, \u0026#34;google.protobuf.Timestamp\u0026#34; end add_message \u0026#34;Person.PhoneNumber\u0026#34; do optional :number, :string, 1 optional :type, :enum, 2, \u0026#34;Person.PhoneType\u0026#34; end add_enum \u0026#34;Person.PhoneType\u0026#34; do value :MOBILE, 0 value :HOME, 1 value :WORK, 2 end add_message \u0026#34;AddressBook\u0026#34; do repeated :people, :message, 1, \u0026#34;Person\u0026#34; end end Person = Google::Protobuf::DescriptorPool.generated_pool.lookup(\u0026#34;Person\u0026#34;).msgclass Person::PhoneNumber = Google::Protobuf::DescriptorPool.generated_pool.lookup(\u0026#34;Person.PhoneNumber\u0026#34;).msgclass Person::PhoneType = Google::Protobuf::DescriptorPool.generated_pool.lookup(\u0026#34;Person.PhoneType\u0026#34;).enummodule AddressBook = Google::Protobuf::DescriptorPool.generated_pool.lookup(\u0026#34;AddressBook\u0026#34;).msgclass   要在 Ruby 中使用这个自动生成的数据结构还得 gem install google-protobuf 一下\n强烈不建议给 message 创建自己的子类\n1 2 3 4 5 6 7 8 9 10 11 12 13  require \u0026#39;./person_pb.rb\u0026#39; person = Person.new({ name: \u0026#34;XguoX\u0026#34;, id: 99, email: \u0026#34;xguox@xguox.xguox\u0026#34;, last_updated: Google::Protobuf::Timestamp.new({seconds: Time.now.to_i, nanos: 0}), phones: [Person::PhoneNumber.new({number: \u0026#39;10086\u0026#39;, type: Person::PhoneType::MOBILE})] }) encoded = Person.encode(person) person = Person.decode(encoded) puts person.last_updated   实例方法:\n Message#dup, Message#clone: Performs a shallow copy of this message and returns the new copy. Message#==: Performs a deep equality comparison between two messages. Message#hash: Computes a shallow hash of the message\u0026rsquo;s value. Message#to_hash, Message#to_h: Converts the object to a ruby Hash object. Only the top-level message is converted. Message#inspect: Returns a human-readable string representing this message. Message#[], Message#[]=: Gets or sets a field by string name. In the future this will probably also be used to get/set extensions.  类方法:\n Message.decode(str): Decodes a binary protobuf for this message and returns it in a new instance. Message.encode(proto): Serializes a message object of this class to a binary string. Message.decode_json(str): Decodes a JSON text string for this message and returns it in a new instance. Message.encode_json(proto): Serializes a message object of this class to a JSON text string. Message.descriptor: Returns the Google::Protobuf::Descriptor object for this message.  其他一些语法 Oneof 1 2 3 4 5 6  message SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; } }   oneof 中的字段可以是任意不带 repeated 关键字的类型, 设置 oneof 会自动清除其它 oneof 字段的值.\n1 2 3 4 5  SampleMessage message; message.set_name(\u0026#34;name\u0026#34;); CHECK(message.has_name()); message.mutable_sub_message(); // Will clear name field. CHECK(!message.has_name());   Map Fields Google::Protobuf::Map 类似 Ruby 的 Hash, 与常规的 Hash 不同的是, Map 是由特定类型的键和值构造成的, 所有映射的键和值都必须是指定的的类型.\n1 2 3 4 5 6 7 8 9 10 11 12  int_string_map = Google::Protobuf::Map.new(:int32, :string) # Returns nil; items is not in the map. print int_string_map[5] # Raises TypeError, value should be a string int_string_map[11] = 200 # Ok. int_string_map[123] = \u0026#34;abc\u0026#34; message.int32_string_map_field = int_string_map   分配标识号(转) 在一个 Message 中的每个字段都有类似 \u0026ldquo; = 1\u0026rdquo;, \u0026ldquo; = 2\u0026rdquo; 的唯一标识号, 这些标识号是用来在消息的二进制格式中识别各个字段的, 一旦开始使用就不能够再改变. 注: [1,15]之内的标识号在编码的时候会占用一个字节. [16,2047]之内的标识号则占用2个字节. 所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号. 切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号.\n最小的标识号可以从 1 开始, 最大到 2^29 - 1, or 536,870,911. 不可以使用其中的19000－19999的标识号, Protobuf协议实现中对这些进行了预留. 如果非要在.proto文件中使用这些预留标识号, 编译时就会报警.\n","permalink":"https://xguox.me/protocol-buffers-learning.html/","tags":["Protobuf"],"title":"学点 Protocol Buffers"},{"categories":["Elasticsearch"],"contents":"许久没仔细使用 Elasticsearch 以后, 发现版本号已经从之前用的 2.X 升级到 6.3, 发布 5 的时候有关注过一会. 之前那堆管理工具打开一看也都好久没更新了, 可能是因为 Site plugins are not supported in Elasticsearch 5.0 了吧.\n新发现一个叫 dejavu 更新的还挺活跃, 自己 clone 下来 yarn 安装一下就可以跑起来了, 因为之前写了一些前端也就「标配」了 yarn 在系统里边. 又或者直接用 chrome 的拓展(elasticsearch-head 也可以)\n在 elasticsearch.yml 还得修改一下跨域限制\n1 2 3 4  http.cors.allow-origin: \u0026#34;http://localhost:1358\u0026#34; http.cors.enabled: true http.cors.allow-headers : X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization http.cors.allow-credentials: true   不爽的地方就是一次只能连着一个 index, 然而我又属于 index 党而不是 type 党 = . = 看在 UI 比 elasticsearch-head 好看一些先用着\n","permalink":"https://xguox.me/web-ui-admin-elasticsearch.html/","tags":["Elasticsearch"],"title":"Elasticsearch 5/6 的 Web UI 管理工具"},{"categories":["Go"],"contents":" main 函数保存在名为 main 的包里. 如果 main 函数不在 main 包里, 构建工具就不会生成可执行的文件.\n 常量只能是 numeric 或者 string\n   There are boolean constants, rune constants, integer constants, floating-point constants, complex constants, and string constants. Rune, integer, floating-point, and complex constants are collectively called numeric constants.\n  := 只能在函数/方法中使用   non-declaration statement outside of function body\n  [arrLength]int 初始化 array 的容量不能通过计算得出, 需要计算得出的话得用 make, left := make([]int, arrLength) 否则会报 non-constant array bound\n Go语言中的源代码定义为 UTF-8 文本, 不允许其他的表示. 在代码中写下字符 `⌘` 时, 用于创建程序的文本编辑器将符号 ⌘ 的 UTF-8 编码放入文本中. 当打印十六进制字节时, 我们只是将文件中的数据打印出来.\n runtime.GOMAXPROCS(1) 不代表就是单线程. 如果 Golang 运行时只分配一个逻辑处理器给调度器使用(runtime.GOMAXPROCS(1))来执行所有 goroutine. 即使只有这一个逻辑处理器也可以让多个 goroutine 并发运行.\n   基于调度器的内部算法, 一个正运行的goroutine在工作结束前, 可以被停止并重新调度. 调度器这样做的目的是防止某个goroutine长时间占用逻辑处理器. 当goroutine占用时间过长时, 调度器会停止当前正运行的goroutine, 并给其他可运行的goroutine运行的机会.\n  Conversions   Conversions are expressions of the form T(x) where T is a type and x is an expression that can be converted to type T.\nIf the type starts with the operator * or \u0026lt;-, or if the type starts with the keyword func and has no result list, it must be parenthesized when necessary to avoid ambiguity:\n 1 2 3 4 5 6 7 8  *Point(p) // same as *(Point(p)) (*Point)(p) // p is converted to *Point \u0026lt;-chan int(c) // same as \u0026lt;-(chan int(c)) (\u0026lt;-chan int)(c) // c is converted to \u0026lt;-chan int func()(x) // function signature func() x (func())(x) // x is converted to func() (func() int)(x) // x is converted to func() int func() int(x) // x is converted to func() int (unambiguous)   Converting a constant yields a typed constant as result.\n1 2 3 4 5 6 7 8 9 10 11 12  uint(iota) // iota value of type uint float32(2.718281828) // 2.718281828 of type float32 complex128(1) // 1.0 + 0.0i of type complex128 float32(0.49999999) // 0.5 of type float32 float64(-1e-1000) // 0.0 of type float64 string(\u0026#39;x\u0026#39;) // \u0026#34;x\u0026#34; of type string string(0x266c) // \u0026#34;♬\u0026#34; of type string MyString(\u0026#34;foo\u0026#34; + \u0026#34;bar\u0026#34;) // \u0026#34;foobar\u0026#34; of type MyString string([]byte{\u0026#39;a\u0026#39;}) // not a constant: []byte{\u0026#39;a\u0026#39;} is not a constant (*int)(nil) // not a constant: nil is not a constant, *int is not a boolean, numeric, or string type int(1.2) // illegal: 1.2 cannot be represented as an int string(65.0) // illegal: 65.0 is not an integer constant   A non-constant value x can be converted to type T in any of these cases:\n x is assignable to T. ignoring struct tags (see below), x\u0026rsquo;s type and T have identical underlying types. ignoring struct tags (see below), x\u0026rsquo;s type and T are pointer types that are not defined types, and their pointer base types have identical underlying types. x\u0026rsquo;s type and T are both integer or floating point types. x\u0026rsquo;s type and T are both complex types. x is an integer or a slice of bytes or runes and T is a string type. x is a string and T is a slice of bytes or runes.  ","permalink":"https://xguox.me/golang-note.html/","tags":["Go"],"title":"Golang 笔记"},{"categories":["Photo"],"contents":"能睡是福\n像团团一样简单可爱, 成长\n","permalink":"https://xguox.me/budding-pop-baby.html/","tags":["Photo"],"title":"长草颜萌萌"},{"categories":["Go"],"contents":"从 Ruby 切换到 Go 一个不顺手的地方是没有 REPL 随手实验, 虽说有 Go Playground, 但还是喜欢 Local 多一些, 而且有一些操作线上很难甚至完成不了的. 另外一个不顺手地方是「没有」 pry 这样的 debug 利器. 其实不是没有, 只是我对 Go 的生态没了解多少. 所以今天扯一点 Delve, 其实官方有 GDB, 但是貌似对 goroutine 不友好, 好吧, 官方自己说的, 所以, 直奔了 Delve.\n作为测试用直接拿之前看的一篇短文 源码, 提取出来放到 gist 了.\n1  go get -u github.com/derekparker/delve/cmd/dlv   如无意外, which dlv 在 $GOPATH/bin 下就会多了一个 dlv 的二进制可执行文件, 执行 dlv 就有挺详细的文档了. 再深入一些还可以执行类似 dlv attach --help\n最简单的用法,\n1 2 3  ➜ dlv debug scraper.go Type \u0026#39;help\u0026#39; for list of commands. (dlv)   这提示信息比 pry 要详细多得多, 毕竟只是看 help 文档描述功能也多不少, 还有一大票 alias, 瞬间感动.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  (dlv) break main.main Breakpoint 1 set at 0x12c42eb for main.main() ./scraper.go:76 (dlv) continue \u0026gt; main.main() ./scraper.go:76 (hits goroutine(1):1 total:1) (PC: 0x12c42eb) 71:\t} 72:\t} 73:\t} 74:\t} 75: =\u0026gt; 76:\tfunc main() { 77:\tfoundUrls := make(map[string]bool) 78:\tseedUrls := os.Args[1:] 79:\tfmt.Println(seedUrls) 80:\t// Channels  81:\tchUrls := make(chan string) (dlv)   到了这里很经常习惯性就 enter 下一步, 然后其实就退出了 = . =, 这里还得多一步 next 操作先.\n或者直接定位到哪一行,不过还是得先 next, 只要 next 了一次以后就可以直接一直 enter\u0026hellip;这个倒是很方便的呢\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  (dlv) break scraper.go:78 Breakpoint 1 set at 0x12c439d for main.main() ./scraper.go:78 (dlv) continue \u0026gt; main.main() ./scraper.go:78 (hits goroutine(1):1 total:1) (PC: 0x12c439d) 73:\t} 74:\t} 75: 76:\tfunc main() { 77:\tfoundUrls := make(map[string]bool) =\u0026gt; 78:\tseedUrls := os.Args[1:] 79:\tfmt.Println(seedUrls) 80:\t// Channels  81:\tchUrls := make(chan string) 82:\tchFinished := make(chan bool) 83:   习惯于 pry 的我到这里很自然就输入变量名获取值,然后就歇菜了,得用 print 或者 p\n1 2 3 4  (dlv) foundUrls Command failed: command not available (dlv) p foundUrls map[string]bool []   进入某个方法的 step 或者缩写 s, 甚至是输出所有局部变量的 locals, 总之在 Go 里边找到了 pry 更好的感觉了.\n然后是 Visual Studio Code, 在 VS Code 里边写 Go 的话基本都会装 M$ 的官方 go 拓展吧, 点 调试 \u0026gt; 打开配置 说是要配置啥不过我这里默认的就可行了.\n在行号前面打个断点, 然后启动调试, 或者 Fn + F5, 之后如果想查看某个变量的值 直接 hover 到变量名就可以了.\n不过这里要注意的是启动调试得把编辑器 focus 在 main.go 才可以, 断点还是可以打在任意地方的, 不然就会报 Failed to continue: Check the debug console for details. Can not debug non-main package Process exiting with code: 1\n","permalink":"https://xguox.me/golang-delve-visual-studio-code.html/","tags":["Go"],"title":"Golang, Delve \u0026 Visual Studio Code"},{"categories":["Photo"],"contents":"又花了一些时间研究 X-H1 的视频拍摄, 因为是新手入坑, 上手还是慢, 零零碎碎的拍了一些以后, 看来没入手柄应该是正确的, 增强的连拍, 视频半小时基本都用不上, 续航的提升拿多两块电池揣兜里就够了, 加上手柄以后体积重量就彻底变味了.\n4K 画质很赞, 慢动作拍摄很好玩, 但是对焦跟网络上其他很多评测的人一样, AF-C 让人略捉急, 也许是我本身使用方式也有问题? Eterna 的胶片模拟大部分时候都挺喜欢的, 色彩的确是有一些电影感.\n定焦拍视频好像不太适合? 有点想入 X-T2 组成双机. 实在不想用变焦 = . =\n续航是比较菜, 但是三块电池足够了.\n最后, 玩视频后期费的功夫比拍照片多了几个级别啊, 即便假设我现在对 Final Cut Pro X 这些剪辑软件很熟练, 时间上也比开个 Lightroom 修修图多得多. 更别说现阶段还是在摸索的 FCPX 的使用. 另外, 深感硬盘空间吃紧得很. 所以是时候上 NAS 了吗?\n此处应该有视频样片, 然而并没有, 想剪辑出几分钟的让自己满意的视频都不是件易事. 先当是积累素材吧.\n","permalink":"https://xguox.me/x-h1-about-video.html/","tags":["Photo"],"title":"用 FUJIFILM X-H1 拍了一些视频以后"},{"categories":["Photo"],"contents":"接上一篇 经过几个月的观察思考后, 最后还是选择了 FUJIFILM X-H1. 顺丰陆运运了两天总算拿到手了. 这一次比之前拿到 A7R 要更兴奋一些, 也许是因为太久没机子在身边(之前的 6D 和 A7R 交接不到一个月就完成了), 比起新婚入坑全幅, 小别相机来的更强烈一些.\n没有用自带的背带, 随意淘了一根应该是冒牌的 cam-in . 内人说你买的是狗绳吗？2333333\n颜值担当\n搭上 XF 16mm F1.4 以后, 总重量大约就在 1kg 左右, 跟以前用朋友的 A7 II + Batis 25mm 差不多, 都比 A7R + 接环 + Sigma 24mm F1.4 Art 要轻不少.\n在 XF 16 和 XF23 之间犹豫了一下, 想着要不要尝试一些许多人迷之喜欢的 35mm 视角. 最后, 还是选择了 XF16, 看中了 LM 和 WR , 还有 15mm 的最近对焦距离. 腹肌的对焦环推拉切换手动自动这个设计要赞一个\n功能菜单的分配似乎比 A7R 的要合理, 可定制按钮以及操作比大法的也要丰富得多, 之前拿到 A7R 基本不用看说明书, 拿到 X-H1 以后还得研究了好一会. (为嘛我感觉富士的说明书我看的有些费劲不大懂 = 。 =) ISO 的上下限加最低快门速度就可以设置三套. 自己设置的色彩组合配置可以有七套. 等等的一大堆一大堆的自定义, 短时间, 也许长时间都玩不过来. 玩操控暂时是用过各家最强大的一台.\n摇杆和触屏都好评. 选对焦点切换对焦区域预览照片放大缩小等等. 总体来说操控的确比 A7R 要强大得多. 不过回放按键放在左上角有点不习惯.\n另外, 快门轻轻一按就触发了, 比用过的其他机型都要轻, 是好是坏暂时还不能下定论吧.\n手动对焦可以开启自动放大查看对焦区域的功能很赞, 裂像则用不习惯了, 还是峰值来的直观熟手一些.\n肩屏，也许用到的机会不多，但逼格杠杠的\n然而, 不管前面吹的多么流弊哄哄, 当初阻拦我选择 X-H1 的最大问题, 也是上手以后不满意的地方, 电池续航. 理论上纯拍照的话应该跟 A7R 差不多续航. 但是, 录视频的话就呵呵哒了. 问题是, 这是在体积变大了以后的呀. 怎么和 X-T2 那体积用同样的电池, 富士都在想啥呢?\n本来是想着入手柄套装的, 但是缺货不想等就直接入了单机. 套装直接送多两个原厂电池, 之后再单买手柄的话就没了, 摔. 卖家送的什么贴膜清洁套装手绳啥的都没有要, 让换了两副厂电池(W126的电池会在屏幕用黄色标识).\n关于 XF 16mm f/1.4 R WR, 做工样子啥的没槽点, F1.4 最大光圈, 论锐利肯定是比不过焊接厂的 24 Art 的了, 反正也不追求傻锐, 自己感觉能用就行. 印象中大部分 24mm 视角的头最近对焦也都是 20+ cm 左右, 之前了解过最流弊的应该是大法 A 口的 Zeiss 24mm F2, 也只是到 19cm. XF 16mm f/1.4 R WR 的 15cm, 减去镜身本身长 73mm, 也就是说离前镜组 8cm 左右都可以合焦, 基本可以说可以贴着拍了! 而且最大放大倍率 0.21倍呢, 100mm stf 号称有微距模式也就 0.25, 广角长焦拿来对比好像不太对. 总而言之近摄能力杠杠的.\n一些直出样片, 如果是说默认的色彩什么的, 暂时还没感觉到有多流弊, 相对 A7R 貌似没那么灰 = . = 但是, 配合各种自定义操作和一些胶片模拟, 应该可以玩出花, 只是得更多实战去总结经验吧. 下回主要测试一下视频.\n1\u0026frasl;20 F5.6\n1\u0026frasl;20 F2.8\n1\u0026frasl;400 F2.8\n1\u0026frasl;90 F2.8\n1\u0026frasl;300 F4\n1\u0026frasl;500 F5.6\n1\u0026frasl;750 F5.6\n1\u0026frasl;160 F5.6\n1\u0026frasl;340 F5.6\nVelvia\nClassic Chrome\n1\u0026frasl;170 F1.8\n","permalink":"https://xguox.me/x-h1-unboxing-simple-review.html/","tags":["Photo"],"title":"FUJIFILM X-H1 with XF 16mm f/1.4 R WR"},{"categories":["Photo"],"contents":"因为觉得未来一大段时间(至少半年以上)不强烈需要相机所以就把 A7R 给出掉了. 结果发现手头上没有一台相机就经常跑去关注新产品. 随时都想着要端一台回来的节奏.\n感觉自己只是为了想有而有, 买回来抚摸摆着. 过了新鲜劲估计又得放防潮箱. 所以, 一直忍着到现在. 其实还是穷, 不然 X1D 和 GFX 50S 什么的一锅端了. (醒醒\u0026hellip; 还有 bug 没修啊)\n对比今时今日推出各路主力旗舰, 发现孱弱的 A7R 也照样用了那么长时间.\n比起现在的新机动不动就满屏的 相位检测自动对焦点 和 对比检测自动对焦点, A7R 那 25个对比检测自动对焦点 怎么好像也没觉得那么不堪用啊, 眼部对焦还不错呢, 主要本身也没太多拍摄运动类型的题材. 电池的渣续航倒是个坑, 但是好歹可以充电宝直接怼, 虽然我没有充电宝. 最大不爽的大概是写入回放每次都要弹窗提示正在写入, 也不知道那将近三万的快门是怎么按过来的.\n但是也着实体验了一把最轻全幅的瘾.(不到500g!)\n视频也渣? 视频是啥? 能吃吗? 买了到现在拍视频的次数一个巴掌就数清了. 不过近段时间我的看法更倾向于多一些视频了. 照片拍了, 等一些时间以后再次回看, 跟幻灯片那样一张一张的, 容易看着看着把观看的热情下降了. 觉得是时候学学视频剪辑了.\n说这么多, 也是给这一次选机的一些铺垫.\n5D Mark IV, 挺好的, 佳能的镜头群也流弊, 不用转接就有我想要的 24mm F1.4, 各方面都挺均衡的, 不喜欢的只是佳能的挤牙膏作风.\nD850, 杠杠的性能, 参数上提升很大, Nikon 的镜头群也不输佳能的.\n最后放弃考虑的原因, 单反始终太招风了, 还是希望可以稍微低调一些, 而且以自己的经历看, 如果不是特别有拍照需求目的的出门, 单反始终不会成为随手携带的一样东西.\nA7R3, A7M3, A9 大法黑科技依旧杠杠的, 以前的弱点几乎都扫光了, 二代开始就有了流弊哄哄的五轴防抖, 镜头群该有的都有了, 还有焊接厂的帮助. 唯独不喜欢的还是外形, 机身本身的外形还可以接受, 但是, FE 的好多镜头都是屁股忽然瘦下来一大圈, 实在接受不了, 比如 85GM, Batis 系列. 如果选这个选项的话, 也许搭 loxia 21 或者 25 以及 55mm F1.8 会是归宿. 35mm F1.4 其实外观还行, 只是如果是 24mm 就好了. 好像大家都很喜欢 35mm 的焦距?\nLeica Q 基本也符合想要的, 除了不能换镜头可玩性没那么高, 就是发布年代有点远了. 双机的话可以考虑的.\nand, 富士的 X-T2 机身搭上镜头整体造型都很符合. 有自己需要的定焦. 虽然 APS-C 画幅, ISO 200 起, 还被人诟病虚标 ISO, 也许谈画质还比不上好些年前的 A7R, 但是, 这么些年来发现 100% 数毛并不是我的第一需要啊, 也不是第二第三呢\u0026hellip;\n虽然知道色彩是很主观的, 但也想瞧瞧富士吹上天的色彩是怎么回事. 虽然这一点上没外形来得重要\n= 。 =\n以前憧憬全画幅, 后来中画幅都轻量化以后渐渐地也不再唯画幅论了, 最后还反倒是觉得, 只有用起来不躺防潮箱的相机才是最好的, 对吧? 所以, 我应该选 X-H1???\n","permalink":"https://xguox.me/being-addicted-to-fujifilm.html/","tags":["Photo"],"title":"长草 Fujifilm"},{"categories":["Photo"],"contents":"大约两年前, 日常打开邮箱无意中发现, 500px 上有一张照片的 license 被买下了? 加起来好像还是 $100+,\n照片是这 https://500px.com/photo/70067871/working-in-the-dark-by-xguox\n拍于 2014.05.11\n两年后的今天, 还是一如往常的打开邮箱, 无意中发现, 500px 上有一张照片的 license 被买下了? 加起来是 $88.8\n照片是这 https://500px.com/photo/71821379/liede-bridge-by-xguox\n拍于 2014.5.27\n都是 A7R 拍的, 一台用的最久的数码产品(截止目前), 第一批上市就入手.\n2013年 11月至今, 整整四年, 28000+ 快门.\n以上.\n","permalink":"https://xguox.me/sold-photo-on-500px.html/","tags":["Photo"],"title":"记第二次在 500px 上卖出照片"},{"categories":["Jabber"],"contents":"写在前面, 在 9 月 12 Apple 的发布会前一天, 入了 iPhone 7 plus, 不折腾了.\n第一次买票 RubyConf China 是 2012 还在上海的时候, 然后就呵呵哒了没去成,\n直到 2014 才真正参加了一次, 在北京.\n这是第二次, 在杭州.\n原本只是想着和内人一起去杭州玩玩的, 结果当地游不成团, 参加满了两天的 RubyConf China.\n这是被各种踩场的欢乐的 RubyConf China.\nBTW, 搜索说动车的高级卧铺有独立卫生间, 还可以洗澡什么的, 回程特意选择奢侈了一把, 结果, 除了是两人包厢以外, 其他都是呵呵哒的, 连独立卫生间都没, 更别谈洗澡了.\n","permalink":"https://xguox.me/rubyconf-china-2017.html/","tags":["Jabber"],"title":"RubyConf China 2017 杭州"},{"categories":["Jabber"],"contents":"手机像往常一样, 还剩 20% 左右的电量, 插上充电没在意. 回头一看, 不对劲, 怎么一直重启进不去系统.\n搜了一下才发现貌似是 Nexus 5X 和 6p 的通病？ WTF!\n搜索并没带来什么好的解决方法, 看到一个治标不治本的方案, 说把 fastboot 把 810 的 A57 核禁用掉, 只用低功耗的 A53 , 想着死马当活马医, 呵呵哒, 也许有人用这方法凑效了, 也许是我的 Nexus 6p 前不久 OTA 升级了 8.0, 并没有起作用, 反而, 从原本的无限重启, 到耗尽电量以后, 彻底变砖, 连无限重启画面都没得看了.\n如果摔坏了, 碎屏了, 系统变卡了, 我不会有太多想法. 但是, 这种方式的坑, 实在接受不能.\n看来真的要换一台手机了, 只是还能用啥. 之前就长草的 Pixel 也瞬间破灭了好感.\nMoGi 最近也跟我抱怨了好多 Nexus 的. 他比我用的 Nexus 多的多, 什么 Moto, 4, 5X 一路过来, 甚至他的 6p 还是我的怂恿之下入手的. 我已经不记得跟多少人推荐 Nexus 系列, 然而结果是那么的尴尬, 失望.\nGoodbye.\n","permalink":"https://xguox.me/nexus-6p-broke-my-android-heart.html/","tags":["Jabber"],"title":"Nexus 6p 让我失望透了"},{"categories":["Ruby"],"contents":" 关于并发与并行, 前不久刚好真实发生. 同事一行人去 Family Mart 买午餐, 拿回来公司只有一个微波炉加热, 在 Family Mart 有两个微波炉可以加热. 也就是说, 我们一行人一起去买午餐这是一个并发的程序, 然后在 Family Mart 可以并行加热, 但是, 如果拿回公司的话, 因为只有一个微波炉(单核), 所以是只能一个接一个顺序执行.\nProcesses 串行执行 给一个 range, 转成 array 以后获取某个特定数字的 index\n1 2 3 4 5 6 7  range = 0...100_000_000 number = 99_999_999 puts range.to_a.index(number) ➜ time ruby sequential.rb 99999999 ruby sequential.rb 4.04s user 0.56s system 98% cpu  4.667 total   执行这段代码大约耗时 5s, 利用了 99% 的CPU(单核).\n并行执行 通过分割 range 以及 Ruby 标准库中的 fork 方法, 我们可以使用多个进程来执行上面的代码.\n1 2 3 4 5 6 7 8 9 10 11 12 13  range1 = 0...50_000_000 range2 = 50_000_000...100_000_000 number = 99_999_999 puts \u0026#34;Parent #{Process.pid}\u0026#34; fork { puts \u0026#34;Child1 #{Process.pid}: #{range1.to_a.index(number)}\u0026#34; } fork { puts \u0026#34;Child2 #{Process.pid}: #{range2.to_a.index(number)}\u0026#34; } Process.wait ➜ time ruby parallel.rb Parent 5086 Child2 5100: 49999999 Child1 5099: ruby parallel.rb 3.73s user 0.43s system 192% cpu  2.162 total   在多核 CPU 下, 因为是把 range 对半拆开, 所以, 处理时间也快了. (数字有时候会有一些差距). Process.wait 是等待所有子进程完成的意思.\n因为 GIL(global interpreter lock) 的存在, Ruby MRI 要利用 CPU 的核数唯一方法就是多进程.\nUnicorn 就是通过 fork Rails 主进程衍生出多个 workers 来处理 HTTP 请求的.\n这种多进程的方式最大优点是充分的利用多核 CPU, 且进程间不共享内存, 因此 debug 也会容易一些, 但是因为不共享内存, 也就意味着内存消耗会变大. 不过从Ruby 2.0 开始使用系统的 COW(Copy On Write) 功能可以让 fork 的进程共享内存数据, 只有在数据改变时候才复制数据.\nThreads Ruby 从 1.9 开始, 线程的实现改为系统的 Native 线程, 线程也由操作系统来完成调度. 但是由于 GIL 的存在, 同一时间一个 Ruby 进程只会有一个线程获取到 GIL 在跑. GIL 的存在意义就是线程安全, 防止发生竞争条件, 比起在数据结构上实现线程安全, GIL 的实现方式更为容易一些.\n然而, 其实, GIL 并不能完全阻止竞争条件的发生 = . =\nRace Condition\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # threads.rb @executed = false def ensure_executed unless @executed puts \u0026#34;executing!\u0026#34; @executed = true end end threads = 10.times.map { Thread.new { ensure_executed } } threads.each(\u0026amp;:join) # 主线程等待子线程执行完毕再继续往后面跑, 比如在后面 p \u0026#39;done\u0026#39;. ➜ ruby threads.rb executing! executing! executing! executing!   因为所有的线程共享一个 @executed 变量, 并且读(unless @executed)写(@executed = true)操作又不是原子级的, 也就是说当在某个线程中读这个 @executed 的值的时候, 可能在别的线程已经把 @executed 给改写了.\nGIL and Blocking I/O\nGIL 不能让多个线程同时跑的话那还要多线程来干啥? 其实还是有其高端的地方的. 当线程遇到阻塞 I/O时就会释放 GIL 给到其他线程, 比如 HTTP 请求, 数据库查询, 磁盘读写, 甚至 sleep .\n1 2 3 4 5 6 7 8  # sleep.rb threads = 10.times.map do |i| Thread.new { sleep 1 } end threads.each(\u0026amp;:join) ➜ time ruby sleep.rb ruby sleep.rb 0.09s user 0.03s system 9% cpu  1.146 total   十个线程执行 sleep 并不需要执行 10s. 线程执行到 sleep 执行权就会递交出去. 相比起使用进程, 线程更轻量一些, 你甚至可以跑好几千个线程, 处理阻塞I/O 的操作也非常有用. 但是, 得要小心 race-conditions. 如果用了互斥锁(Mutex)来避免的话, 又得注意死锁之类的出现. 此外, 线程之间的切换也是会有消耗的, 所以, 太多线程的话, 会把时间都花在切换线程上了.\nPuma 允许在每个进程中使用多线程, 每个进程都有各自的线程池. 大部分时候不会遇到上面说的竞争问题, 因为每个 HTTP 请求都是在不同的线程处理.\nEventMachine EventMachine(EM) 是一个基于 Reactor 设计模式提供事件驱动(event-driven) I/O 的 gem. 使用 EventMachine 的一个好处就是当处理大量 IO 操作的时候, 不需要手工处理多线程, EM 可以在一个线程里边处理多个 HTTP 请求.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # em.rb require \u0026#39;eventmachine\u0026#39; EM.run do EM.add_timer(1) do puts \u0026#39;sleeping...\u0026#39; EM.system(\u0026#39;sleep 1\u0026#39;) { puts \u0026#34;woke up!\u0026#34; } puts \u0026#39;continuing...\u0026#39; end EM.add_timer(3) { EM.stop } end ➜ ruby em.rb sleeping... continuing... woke up!   EM.system 模拟了 I/O 操作, 并传入一个 block 作为回调, 在等待回调期间可以继续响应别的操作(事件). 但是复杂的系统因为要处理成功与失败的回调, 而且可能回调内部嵌套着其他事件和回调, 因此, 很容易就会陷入 Callback Hell\nFiber 用的巨少, 语法掌握了又忘, 语法掌握了又忘 = . =\nFiber 是 Ruby 1.9 开始新增的轻量级运行单元. 类似线程的暂停, 恢复执行代码, 最大的区别在于, 线程是由于操作系统调度执行, 而 Fiber 是由程序员自己处理调度.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # fiber.rb fiber = Fiber.new do # 3. Fiber.yield 交出执行权, 并返回 1 (在这里) Fiber.yield 1 # 5. 执行完毕, 返回 2 2 end # 1. fiber 创建以后不会执行里边的代码 # 2. 调用 resume 才会执行 puts fiber.resume # 4. Go on here... puts fiber.resume # 5. fiber 已挂, 报错 puts fiber.resume ➜ ruby fiber.rb 1 2 dead fiber called (FiberError)   Fiber 结合 EventMachine 可以避免 Callback Hell\n1 2 3 4 5 6 7 8 9 10  EventMachine.run do page = EM::HttpRequest.new(\u0026#39;https://google.ca/\u0026#39;).get page.errback { puts \u0026#34;Google is down\u0026#34; } page.callback { url = \u0026#39;https://google.ca/search?q=universe.com\u0026#39; about = EM::HttpRequest.new(url).get about.errback { ... } about.callback { ... } } end   使用 Fiber 重写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  EventMachine.run do Fiber.new { page = http_get(\u0026#39;http://www.google.com/\u0026#39;) if page.response_header.status == 200 about = http_get(\u0026#39;https://google.ca/search?q=universe.com\u0026#39;) # ... else puts \u0026#34;Google is down\u0026#34; end }.resume end def http_get(url) current_fiber = Fiber.current http = EM::HttpRequest.new(url).get http.callback { current_fiber.resume(http) } http.errback { current_fiber.resume(http) } Fiber.yield end   如果在 Fiber 中执行 I/O 操作, 整个线程将会被这个 fiber 阻塞住, 终归还是 GIL 的原因, 其实并没有真正的解决并发的问题, 而且 Fiber 的语法估计我后天又忘了.\nCelluloid Celluloid 借鉴了 Erlang 给 Ruby 带来了相似的 Actor 模型. 每个 include 了 Celluloid 的类都变成了一个拥有自己的执行线程的 Actor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  class Sheen include Celluloid def initialize(name) @name = name end def set_status(status) @status = status end def report \u0026#34;#{@name}is #{@status}\u0026#34; end end irb(main):009:0\u0026gt; charlie = Sheen.new \u0026#34;Charlie Sheen\u0026#34; =\u0026gt; #\u0026lt;Celluloid::Proxy::Cell(Sheen:0x3ffd8ac62b50) @name=\u0026#34;Charlie Sheen\u0026#34;\u0026gt; irb(main):010:0\u0026gt; charlie.set_status \u0026#34;winning!\u0026#34; =\u0026gt; \u0026#34;winning!\u0026#34; irb(main):011:0\u0026gt; charlie.report =\u0026gt; \u0026#34;Charlie Sheen is winning!\u0026#34; irb(main):012:0\u0026gt; charlie.async.set_status \u0026#34;asynchronously winning!\u0026#34; =\u0026gt; #\u0026lt;Celluloid::Proxy::Async(Sheen)\u0026gt; irb(main):013:0\u0026gt; charlie.report =\u0026gt; \u0026#34;Charlie Sheen is asynchronously winning!\u0026#34;   Celluloid::Proxy::Async 对象会截获方法的调用, 然后保存到 Actor 并发对象的调用队列中, 程序不必等待响应就可以往下执行(异步). 每个并发对象都有一个自己调用队列, 并且按顺序地一个接一个执行里边的方法调用.\nActor 之间通过发送消息来交流, 而与 Erlang 的 Actor 模型最大的区别就在于 Erlang 的变量是不可变的, 而 Ruby 没有这个限制, 所以, 消息(对象)在传递的过程就可能会被修改了, 除非 freeze?\nhttps://engineering.universe.com/introduction-to-concurrency-models-with-ruby-part-i-550d0dbb970\nhttps://www.jstorimer.com/blogs/workingwithcode/8085491-nobody-understands-the-gil\nhttp://merbist.com/2011/02/22/concurrency-in-ruby-explained/\nhttps://www.slideshare.net/mperham/actors-and-threads\nhttp://practicingruby.com/articles/gentle-intro-to-actor-based-concurrency\n","permalink":"https://xguox.me/concurrency-in-ruby.html/","tags":["Ruby"],"title":"Ruby 的并发, 进程, 线程, GIL, EventMachine, Celluloid"},{"categories":["Erlang"],"contents":" 为了让几十个任务能同时执行, Erlang 采用了 Actor 模型, 每个 actor 都是虚拟机中的一个独立进程. 在 Erlang 中, 并发的基本单位是进程(Actor). 每个进程代表一个持续的活动, 是某段程序代码的执行代理, 与其他按各自的节奏执行自身代码的进程一起并发运行, 进程之间靠消息来通信. Erlang 的并发是很廉价的, 派生一个进程就跟在 OOP 中分配一个对象的开销差不多. 更形象一些, 如果你是 Erlang 世界中的一个 actor, 你将会是一个孤独的人, 独自坐在一个没有窗户的黑屋子里, 在你的邮箱旁等待着消息. 当你收到一条消息时, 会用特定的方式来响应这条消息：收到账单就要进行支付；收到生日卡, 就回一封感谢信；对于不理解的消息, 就完全忽略. 人和人之间只能通过写信进行交流, 就是这样.\n理解进程之前先要说的是 pid 这个特殊的 Erlang 数据类型, Erlang 支持进程编程, 任何代码都需要一个进程作为载体才能执行, 每个进程都有一个唯一标识符, 也即是 pid. 在 erlang shell 中, 进程的 pid 会以类似的格式打印, 这个格式只用于调试比较目的.\nself() 会告诉你当前进程(也就是调用 self() 的那个进程)的 pid.\nPROCESS OPERATIONS 派生进程 派生进程的函数有两个: spawn/1, spawn/3. 第一个仅有一个参数, 就是用作新进程入口的 fun 函数, 这里要注意的是, 我们无法得到函数F的返回值. 我们只能得到它的 pid, 因为进程不会返回任何东西; 另一个则需要模块名, 函数名, 参数列表三个参数.\n1 2 3 4 5 6 7 8 9 10  %% Erlang/OTP 20 [erts-9.0]  %% Eshell V9.0 (abort with ^G) 1\u0026gt; F = fun() -\u0026gt; 2 + 2 end. #Fun\u0026lt;erl_eval.20.99386804\u0026gt; 2\u0026gt; spawn(F). \u0026lt;0.63.0\u0026gt; 3\u0026gt; spawn(io, format, [\u0026#34;Hello World!~n\u0026#34;]). Hello World! \u0026lt;0.68.0\u0026gt;   消息传递 Erlang 的进程之间可以互相使用 ! 运算符发送消息.\n该操作符的左边是一个 pid, 右边可以是任意 Erlang 数据项. 这个数据项会被发送给左边的 pid 所代表的进程, 这个进程就可以访问它了.\n1 2 3  %% 给当前进程发送一个 hello 原子 1\u0026gt; self() ! hello. hello   发送一些无人会看的消息的用处就和写一些表达自我情绪的诗一样(换句话说, 就是不大有用), 呵呵哒.\n因此, 我们还需要 receive 表达式来接收消息.\nreceive 的语法形式如下:\n1 2 3 4 5  receive Pattern1 when Guard1 -\u0026gt; Expr1; Pattern2 when Guard2 -\u0026gt; Expr2; Pattern3 -\u0026gt; Expr3 end   举个例子:\n1 2 3 4 5 6 7 8 9 10 11 12  -module(dolphins). -compile(export_all). dolphin1() -\u0026gt; receive do_a_flip -\u0026gt; io:format(\u0026#34;How about no?~n\u0026#34;); fish -\u0026gt; io:format(\u0026#34;So long and thanks for all the fish!~n\u0026#34;); _ -\u0026gt; io:format(\u0026#34;Heh, we\u0026#39;re smarter than you humans.~n\u0026#34;) end.  1 2 3 4 5 6 7 8 9  1\u0026gt; c(dolphins). {ok,dolphins} 2\u0026gt; Dolphin = spawn(dolphins, dolphin1, []). \u0026lt; 0.39.0\u0026gt; 3\u0026gt; Dolphin ! \u0026#34;oh, hello dolphin!\u0026#34;. Heh, we\u0026#39;re smarter than you humans. \u0026#34;oh, hello dolphin!\u0026#34; 4\u0026gt; Dolphin ! fish fish   函数执行到了 receive 表达式. 因为进程邮箱为空, 所以海豚进程会一直处于等待消息状态.\n收到了消息\u0026rdquo;oh, hello dolphin!\u0026ldquo;. 函数会先对 do_aflip 进行模式匹配. 失败了. 然后再尝试模式fish, 也失败了. 最后遇到了匹配一切的子句(), 匹配成功.\n进程打印消息\u0026rdquo;Heh, we’re smarter than you humans.\u0026ldquo;, 接着, 进程也随之结束. 因此, 再次发送 fish 给 Dolphin 的时候, 就不会对该消息做任何反应.\n超时接收 上面提到的, 当进程执行到 receive 表达式以后会一直处于等待消息的状态, 而且如果不做处理就会一直等下去. 出现这样情况的原因有很多, 比如准备发送消息的进程在消息发出之前就崩溃掉了. 为了避免超时问题的出现, 可以在 receive 增加超时设置, 如下:\n1 2 3 4 5  receive Match -\u0026gt; Expression1 after Delay -\u0026gt; Expression2 end.   等待 Delay 毫秒以后如果没有 Match 的消息就会执行 after 中的内容, 在这里就是 Expression2\n注册进程 每个 Erlang 系统都有一个本地进程注册表用于注册进程的简单命名服务. 一个名称一次只能用于一个进程. 可以使用函数 erlang:register(Name, Pid) 为进程命名. 如果进程死亡了, 它会自动失去自己的名字. 也可以使用函数unregister/1手工解除进程的名字注册. 可以调用 registered/0 得到所有已注册进程的列表, 或者通过shell命令 regs() 得到更详细的信息. 使用内置函数 whereis/1 可以查找当前与指定注册名对应的 pid.\n假设某个进程崩溃了, 对应的服务被重启, 新的服务进程 pid 将会改变, 此时无需逐个通知给系统中的所有进程, 只要更新进程注册表就可以了.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  1\u0026gt; registered(). [application_controller,user,standard_error,erl_prim_loader, inet_db,init,erts_code_purger,rex,error_logger,user_drv, kernel_sup,global_name_server,erl_signal_server, standard_error_sup,file_server_2,global_group, kernel_safe_sup,code_server] 2\u0026gt; regs(). ** Registered procs on node nonode@nohost ** Name Pid Initial Call Reds Msgs application_controlle \u0026lt;0.33.0\u0026gt; erlang:apply/2 551 0 code_server \u0026lt;0.38.0\u0026gt; erlang:apply/2 121988 0 erl_prim_loader \u0026lt;0.6.0\u0026gt; erlang:apply/2 139520 0 erl_signal_server \u0026lt;0.47.0\u0026gt; gen_event:init_it/6 51 0 error_logger \u0026lt;0.32.0\u0026gt; gen_event:init_it/6 322 0 erts_code_purger \u0026lt;0.1.0\u0026gt; erts_code_purger:start/0 4 0 file_server_2 \u0026lt;0.46.0\u0026gt; file_server:init/1 113 0 global_group \u0026lt;0.45.0\u0026gt; global_group:init/1 74 0 global_name_server \u0026lt;0.41.0\u0026gt; global:init/1 63 0 inet_db \u0026lt;0.44.0\u0026gt; inet_db:init/1 348 0 init \u0026lt;0.0.0\u0026gt; otp_ring0:start/2 818 0 kernel_safe_sup \u0026lt;0.55.0\u0026gt; supervisor:kernel/1 83 0 kernel_sup \u0026lt;0.37.0\u0026gt; supervisor:kernel/1 2162 0 rex \u0026lt;0.40.0\u0026gt; rpc:init/1 32 0 standard_error \u0026lt;0.49.0\u0026gt; erlang:apply/2 11 0 standard_error_sup \u0026lt;0.48.0\u0026gt; supervisor_bridge:standar 50 0 user \u0026lt;0.52.0\u0026gt; group:server/3 87 0 user_drv \u0026lt;0.51.0\u0026gt; user_drv:server/2 2703 0 ** Registered ports on node nonode@nohost ** Name Id Command ok 4\u0026gt; whereis(user). \u0026lt;0.52.0\u0026gt;   链接 Link 链接(link)是两个进程之间的一种特殊关系. 当在两个进程间建立了这种关系后, 如果其中一个进程由于意外的抛出, 出错或者退出而死亡时, 另外一个进程也会死亡, 把这两个进程独立的生存期绑定成一个关联在一起的生存期.\nErlang 中有一个原生函数 link/1, 用于在两个进程间建立一条链接, 它的参数是进程的pid. 当调用它时, 会在当前进程和参数pid标识的进程之间建立一条链接. 要去除链接, 可以使用 unlink/1.\n当链接进程中的一个死亡时, 会发送一条特殊的消息, 其中含有死亡原因相关的信息. 如果进程正常死亡了(函数正常执行完毕), 就不会发送这条消息.\n注意, link(spawn(Function)) 或者 link(spawn(M,F,A)) 并不是一个原子操作. 有时, 进程会在链接建立成功之前死亡, 从而导致不期望的行为. 因此, Erlang 中增加了 spawn_link/1 和spawn_link/3 函数, 对应 spawn/1 和 spawn/3, 创建一个进程, 并和它建立链接, 这个操作是原子级的, 也就意味着要么成功, 要么失败.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  -module(linkmon). -export([myproc/0]). myproc() -\u0026gt; timer:sleep(5000), exit(reason). 1\u0026gt; self(). \u0026lt;0.60.0\u0026gt; 2\u0026gt; c(linkmon). {ok,linkmon} 3\u0026gt; spawn_link(fun linkmon:myproc/0). \u0026lt;0.68.0\u0026gt; ** exception error: reason 4\u0026gt; self(). \u0026lt;0.70.0\u0026gt;   监控器 Monitor 有时候链接可能不是你想要的, 也许只是想知道目标进程挂了而不想牵连着一起被杀死, 那么这时候就要用监控器(Monitor). 监控器其实是一个单向的链接, 可以让一个进程在不影响目标进程的情况下对目标进程进行监视.\n创建监控器的函数是 erlang:monitor/2, 它的第一个参数永远是原子 process, 第二个参数是进程的 Pid.\n1 2 3 4 5 6  1\u0026gt; erlang:monitor(process, spawn(fun() -\u0026gt; timer:sleep(500) end)). #Ref\u0026lt;0.3856117142.833355780.165079\u0026gt; 2\u0026gt; flush(). Shell got {\u0026#39;DOWN\u0026#39;,#Ref\u0026lt;0.3856117142.833355780.165079\u0026gt;,process,\u0026lt;0.62.0\u0026gt;,normal} ok 3\u0026gt;   每当被监控的进程死亡时, 监控进程都会收到一条消息, 格式为 {'DOWN', MonitorReference, process, Pid, Reason}\n其中, MonitorReference 可以用来解除对一个进程的监控. 记住, 监控器是可叠加的, 因此会收到多条 DOWN 消息. 引用可以唯一确定一条 DOWN 消息. 类似 spawn_link 的也有原子级的 spawn_monitor\n","permalink":"https://xguox.me/erlang-weekly-note-07.html/","tags":["Erlang"],"title":"Erlang weekly note 07 - Processes 进程"},{"categories":["Erlang"],"contents":" Let it crash!\nErlang 的异常有三类: 出错(error), 退出(exit), 抛出(throw), 针对每种异常, 都有一个与之对应的用于抛出异常内置函数, throw(Exception), exit(Exception)或者 error(Exception) 触发.\n error: 这类是运行时异常, 在发生除零错误, 匹配运算失败, 找不到匹配的函数子句等情况时触发, 这些异常的特点在于一旦它们促使某个进程崩溃, Erlang 的错误日志管理器便会将之记录在案.  1 2 3 4 5 6 7  2 div 0. ** exception error: an error occurred when evaluating an arithmetic expression in operator div/2 called as 2 div 0 lists:reverse(123). ** exception error: no function clause matching lists:reverse(123) (lists.erl, line 147)    exit: 这类异常用于通报 「进程即将停止」. 它们会在迫使进程崩溃的同时将进程崩溃的原因告知给其他进程, exit 也在进程正常终止的时候使用. 当你确定想要终止当前进程时就用 exit(Why), 如果这个异常错误没有被捕获到, 信号 {'EXIT', Pid, Why} 就会被广播给当前进程链接的所有进程\n throw: 当期望程序员处理所发生的异常时, 可以使用抛出（throw）异常. 与错误异常和退出异常不同, 抛出异常并没有\u0026rdquo;Let it crash!\u0026ldquo;的意思, 只是为了改变控制流. 如果进程没能捕获 throw 异常便会转变为一个原因为 nocatch 的 error 异常, 迫使异常终止并记录日志\n  1 2  1\u0026gt; throw(permission_denied). ** exception throw: permission_denied   抛出异常, 出错异常和退出异常都可以使用try ... catch来处理. 跟很多语言类似, after 相当于 finally, ensure 的.\n1 2 3 4 5 6 7 8 9 10 11 12 13  try Expression of SuccessfulPattern1 [Guards] -\u0026gt; Expression1; SuccessfulPattern2 [Guards] -\u0026gt; Expression2 catch TypeOfError:ExceptionPattern1 -\u0026gt; Expression3; TypeOfError:ExceptionPattern2 -\u0026gt; Expression4 after AfterExpr end.   或者\n1 2 3 4 5 6  try Expression1, Expression2, Expression3 catch end.   Erlang weekly note 07 - Processes 进程\n并发程序中的异常 设想一个只有单一顺序进程的系统. 如果这个进程挂了, 麻烦可能就大了, 因为没有其他进 程能够帮忙. 出于这个原因, 顺序语言把重点放在故障预防上, 强调进行防御式编程.\n而 Erlang 关于构建容错式软件的理念可以总结成两个容易记忆的短句:\u0026ldquo;让其他进程修复错 误”(link 和 monitor)和\u0026rdquo;任其崩溃\u0026rdquo;.\n在 Erlang 里, 我们有大量的进程可供支配, 因此任何单进程故障都不算特别重要. 通常只需 编写少量的防御性代码, 而把重点放在编写纠正性代码上.\n捕捉退出信号 跨进程的错误(崩溃)传播对进程来说和消息传递类似, 不过使用的是一种称为信号(signal)的特殊消息.\nErlang 的进程有两种: 普通进程和系统进程. spawn 创建的是普通进程. 普通进程可以通过执行内置函数 process_flag(trap_exit, true). 变成系统进程.\n当系统进程收到错误信号时, 该信号会被转换成 {'EXIT', Pid, Why} 形式的消息. Pid 是终止进程的标识, Why 是终止原因(有时候被称为退出原因). 如果进程是无错误终止, Why 就会是原子 normal, 否则 Why 会是错误的描述.\n当普通进程收到错误信号时, 如果退出原因不是normal, 该进程就会终止. 当它终止时, 同样会向它的连接组广播一个退出信号.\n默认情况下, 一旦接收到来自相互链接的其他进程的退出信号, 进程也就会跟着退出. 而变为系统进程以后, 除了无法捕捉的信号(kill)以外, 外来的退出信号都会被转换成无害的消息. 系统进程收到摧毁信号(kill signal)时会终止. 摧毁信号是通过调用 exit(Pid, kill)生成的. 这种信号会绕过常规的错误信号处理机制, 不会被转换成消息. 摧毁信号只应该 用在其他错误处理机制无法终止的顽固进程上.\n任何执行 exit(Why) 的进程都会终止(如果代码不是在 catch 或 try 的范围内执行的话), 并向它的连接组广播一个带有原因 Why 的退出信号. 此外, 进程可以通过执行 exit(Pid, Why) 来发送一个 虚假 的错误信号. 在这种情况下, Pid 会收到一个带有原因 Why 的退出信号. 调用exit/2的进程则不会终止.\n异常和退出信号捕获  异常源：spawn_link(fun() -\u0026gt; ok end)  未捕获时的结果：无任何结果\n捕获时的结果：{'EXIT', \u0026lt; 0.61.0\u0026gt;, normal}\n 异常源：spawn_link(fun() -\u0026gt; exit(reason) end)  未捕获时的结果：** exception exit: reason\n捕获时的结果：{'EXIT', \u0026lt; 0.55.0\u0026gt;, reason}\n 异常源：spawn_link(fun() -\u0026gt; exit(normal) end)  未捕获时的结果：无任何结果\n捕获时的结果：{'EXIT', \u0026lt; 0.58.0\u0026gt;, normal}\n 异常源：spawn_link(fun() -\u0026gt; 1/0 end)  未捕获时的结果：Error in process \u0026lt; 0.44.0\u0026gt; with exit value: {badarith, [{erlang, '/', [1,0]}]}\n捕获时的结果：{'EXIT', \u0026lt; 0.52.0\u0026gt;, {badarith, [{erlang, '/', [1,0]}]}}\n 异常源：spawn_link(fun() -\u0026gt; erlang:error(reason) end)  未捕获时的结果：Error in process \u0026lt; 0.47.0\u0026gt; with exit value: {reason, [{erlang, apply, 2}]}\n捕获时的结果：{'EXIT', \u0026lt; 0.74.0\u0026gt;, {reason, [{erlang, apply, 2}]}}\n 异常源：spawn_link(fun() -\u0026gt; throw(rocks) end)  未捕获时的结果：Error in process \u0026lt; 0.51.0\u0026gt; with exit value: { {nocatch, rocks},[{erlang, apply, 2}]}\n捕获时的结果：{'EXIT', \u0026lt; 0.79.0\u0026gt;, { {nocatch, rocks}, [{erlang, apply, 2}]}}\n","permalink":"https://xguox.me/erlang-weekly-note-06.html/","tags":["Erlang"],"title":"Erlang weekly note 06 - Exceptions 异常"},{"categories":["Erlang"],"contents":"元组是大部分 Erlang 结构化数据的基石, 而 Erlang 的记录(Record)是元组之上的语法糖, 记录可以让你避免了使用元组时增减字段所带来的麻烦以及必须记住各个字段在元组中的顺序的问题.\n使用记录之前首先要定义记录, 记录是以模块属性的形式来声明的, 比如:\n1  -record(user, {uuid=\u0026#34;\u0026lt;xguox\u0026gt;\u0026#34;, address, phone}).   这里的 user 相当于记录的名称, 这个记录有三个字段, 分别是: uuid, address, phone, 其中 uuid 具有默认值 \u0026quot;\u0026lt;xguox\u0026gt;\u0026quot;.\n接着就可以这么使用记录:\n1 2 3 4 5 6 7 8  #user{} #user{phone=\u0026#34;10086\u0026#34;} #user{uuid=\u0026#34;\u0026lt;another\u0026gt;\u0026#34;, phone=\u0026#34;10086\u0026#34;, address=\u0026#34;Mars\u0026#34;} T = #user{phone=\u0026#34;10086\u0026#34;, address=\u0026#34;Earth\u0026#34;} T#user.address   记录可以直接在函数头中对它们进行模式匹配, 譬如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  -record(user, {id, name, group, age}). %% 使用模式匹配进行过滤 admin_panel(#user{name=Name, group=admin}) -\u0026gt; Name ++ \u0026#34; is allowed!\u0026#34;; admin_panel(#user{name=Name}) -\u0026gt; Name ++ \u0026#34; is not allowed\u0026#34;. %% 可以随意扩展 user 记录, 函数无需修改  adult_section(U = #user{}) when U#user.age \u0026gt;= 18 -\u0026gt; allowed; adult_section(_) -\u0026gt; forbidden.   编译以后运行:\n1 2 3 4 5 6 7 8  records:admin_panel(#user{id=1, name=\u0026#34;ferd\u0026#34;, group=admin, age=96}). %% \u0026#34;ferd is allowed!\u0026#34; records:admin_panel(#user{id=2, name=\u0026#34;you\u0026#34;, group=users, age=66}). %% \u0026#34;you is not allowed\u0026#34; records:adult_section(#user{id=21, name=\u0026#34;Bill\u0026#34;, group=users, age=72}). %% allowed records:adult_section(#user{id=22, name=\u0026#34;Noah\u0026#34;, group=users, age=13}). %% forbidden   更新记录\n因为记录都定义都是在编译时, 而不是运行时, 所以, 要在 erl 中使用记录, 需要用 rd 手工来定义一次.\nrd(Name,Definition), 定义一个记录, 方式和模块中使用的 -record(Name,Definition)类似.\n1 2 3 4  U1 = #user{uuid=\u0026#34;XguoX\u0026#34;, phone=\u0026#34;10000\u0026#34;, address=\u0026#34;Test address\u0026#34;}. #user{uuid = \u0026#34;XguoX\u0026#34;,address = \u0026#34;Test address\u0026#34;,phone = \u0026#34;10000\u0026#34;} U2 = U1#user{uuid=\u0026#34;MM\u0026#34;,phone=\u0026#34;12345\u0026#34;}. #user{uuid = \u0026#34;MM\u0026#34;,address = \u0026#34;Test address\u0026#34;,phone = \u0026#34;12345\u0026#34;}   官方长示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  %% File: person.hrl  %%----------------------------------------------------------- %% Data Type: person %% where: %% name: A string (default is undefined). %% age: An integer (default is undefined). %% phone: A list of integers (default is []). %% dict: A dictionary containing various information %% about the person. %% A {Key, Value} list (default is the empty list). %%------------------------------------------------------------ -record(person, {name, age, phone = [], dict = []}). -module(person). -include(\u0026#34;person.hrl\u0026#34;). -compile(export_all). % For test purposes only.  %% This creates an instance of a person. %% Note: The phone number is not supplied so the %% default value [] will be used.  make_hacker_without_phone(Name, Age) -\u0026gt; #person{name = Name, age = Age, dict = [{computer_knowledge, excellent}, {drinks, coke}]}. %% This demonstrates matching in arguments  print(#person{name = Name, age = Age, phone = Phone, dict = Dict}) -\u0026gt; io:format(\u0026#34;Name: ~s, Age: ~w, Phone: ~w~n\u0026#34; \u0026#34;Dictionary: ~w.~n\u0026#34;, [Name, Age, Phone, Dict]). %% Demonstrates type testing, selector, updating.  birthday(P) when record(P, person) -\u0026gt; P#person{age = P#person.age + 1}. register_two_hackers() -\u0026gt; Hacker1 = make_hacker_without_phone(\u0026#34;Joe\u0026#34;, 29), OldHacker = birthday(Hacker1), % The central_register_server should have  % an interface function for this.  central_register_server ! {register_person, Hacker1}, central_register_server ! {register_person, OldHacker#person{name = \u0026#34;Robert\u0026#34;, phone = [0,8,3,2,4,5,3,1]}}.   键值映射(Maps), 又叫映射组(Programming Erlang). 是从 Erlang R17 开始时新增的一种数据类型.\n映射组的语法与记录相似, 不同之处是省略了记路名, 并且键值分隔符是 =\u0026gt; 或 :=, 此外, 映射组有着明确的顺序,\n举个例子, 假设要创建一个包含a, b两个键的映射组.\n1  M1 = #{a =\u0026gt; 1, b =\u0026gt; 10}.   映射组在系统内部是作为有序集合存储的, 打印时总是使用各键排序后的顺序, 与映射组的 创建方式无关.譬如:\n1 2 3 4 5 6  M1 = #{a =\u0026gt; 1, b =\u0026gt; 10}. %% #{a =\u0026gt; 1,b =\u0026gt; 10} M2 = #{b =\u0026gt; 10, a =\u0026gt; 1}. %% #{a =\u0026gt; 1,b =\u0026gt; 10} M1 = M2. %% #{a =\u0026gt; 1,b =\u0026gt; 10}   分隔符号 =\u0026gt; 或 := 的区别在于\n表达式K =\u0026gt; V有两种用途, 一种是将现有键 K 的值更新为新值 V, 另一种是给映射组添加一 个全新的 K-V 键值对. 这个操作总是成功的.\n表达式K := V的作用是将现有键 K 的值更新为新值 V. 如果被更新的映射组不包含键 K, 这个 操作就会失败.\n","permalink":"https://xguox.me/erlang-weekly-note-05.html/","tags":["Erlang"],"title":"Erlang weekly note 05 - Records \u0026 Maps 记录与键值对"},{"categories":["Ruby"],"contents":"都知道的, 在 Rails 的 View 里边渲染集合的时候, 会用到 render 方法参数的 collection 选项\n1  \u0026lt;%= render partial: \u0026#34;product\u0026#34;, collection: @products %\u0026gt;   而不是遍历集合来渲染单个模板.\n渲染集合还有个简写形式. 假设 @products 是 product 实例集合, 在 index.html.erb中可以直接写成下面的形式, 得到的结果是一样的:\n1  \u0026lt;%= render @products %\u0026gt;   这里, Rails 做的魔法其实是去找遍历成员的 to_partial_path\naction_view/renderer/partial_renderer (Rails 4.2)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def partial_path(object = @object) object = object.to_model if object.respond_to?(:to_model) path = if object.respond_to?(:to_partial_path) object.to_partial_path else raise ArgumentError.new(\u0026#34;\u0026#39;#{object.inspect}\u0026#39; is not an ActiveModel-compatible object. It must implement :to_partial_path.\u0026#34;) end if @view.prefix_partial_path_with_controller_namespace prefixed_partial_names[path] ||= merge_prefix_into_object_path(@context_prefix, path.dup) else path end end   打开 rails console 可以试试\n1 2  [1] pry(main)\u0026gt; User.new.to_partial_path =\u0026gt; \u0026#34;users/user\u0026#34;   这里也可以把 user 这个 model 的 to_partial_path 重写, 返回表示渲染路径的字符串,\n如果你的某个 PORO 实现了 to_partial_path, 那对应的 collection 也可以直接用类似的方式去 render\n","permalink":"https://xguox.me/rails-render-collection.html/","tags":["Ruby"],"title":"Rails render collection 的魔法"},{"categories":["Erlang"],"contents":"            尾递归        Erlang 没有 for 或者 each 之类的循环迭代结构, 循环迭代靠的是递归(Recursion).\n比如, 计算 0 到 N 的累加和, 循环迭代可以这么写:\n1 2 3 4 5 6 7 8 9 10  def sum(n) total = 0 while n != 0 total += n n -= 1 end return total end # [*(0..n)].inject(\u0026amp;:+)   Erlang 用递归这么写:\n1 2  sum(0) -\u0026gt; 0; sum(N) -\u0026gt; sum(N-1) + N.   像 Ruby, JavaScript 等语言, 其实也可以用递归方式做同样的事情, 但是由于实现层面的限制和低效, 递归的作用难以得到发挥, 所以从效率上考虑, 还是用循环迭代才是正道.\n上面的递归写法看起来没啥大问题, 但是, 当数字 N 非常大的时候, 就会发现, 占用的内存会膨胀起来.\n比如, 在我自己的机器上试了一下, 计算零到一亿的和,\n内存使用最高时候飙到了 7 个 G.\n因为, 在 sum 中, sum(N-1) 调用完成以后, 在返回之前还有事情没做完, 那就是加 N, 一直都没做完就得一直保持着内存.\n所以, Erlang 还能好好的用递归来替代循环来玩耍吗? 答案肯定是可以的. 那么, 也就要用到接下来说的尾递归了.\n尾递归 先把用尾递归改写结果奉上:\n1 2 3 4  tail_sum(N) -\u0026gt; tail_sum(N, 0). tail_sum(0, Total) -\u0026gt; Total; tail_sum(N, Total) -\u0026gt; tail_sum(N-1, Total + N   尾递归是尾调用的一个特例, 那么什么又是尾调用呢? 简单的说, 函数在调用完成以后就没事可做了(除了返回), 那么就是尾调用.\n比如这里, 上面说了, 在 sum 中, sum(N-1) 调用完成以后还要加上 N, 所以, sum 不能算作尾调用, 而后面的, tail_sum, 在 tail_sum(N-1, Total+N) 调用完成了以后就可以拍拍屁股完事了, 而调用的返回值也就是整个函数的返回值了.\n另一方面, Erlang 采用了尾调用优化(last call optimization),\n 当编译器识别出尾调用(函数返回前的最后一个任务)时, 会生成一段特殊代码, 这段代码会在执行尾调用之前从栈中扔掉当前调用的所有信息. 此时, 当前调用基本无事可做, 只需告知被调用的函数后续即将发生一次尾调用: \u0026ldquo;嘿! 完事儿的时候直接把结果告诉我的调用者就好了, 我收工了哦.\u0026rdquo; 因此, 尾调用不会导致栈的膨胀.\n 在 tail_sum 中, Total 扮演的是累加器参数的角色, 用于在单个变量中完成信息累加(而不是将信息记在栈上回头再取). 编写尾递归函数时, 往往至少需要一个这样的额外参数, 有时候可能会是多个. 这个变量必须在 循环启动的时候初始化, 因此这里需要两个函数(Erlang 不支持参数默认值). 一个用作前端接口, 一个用作主循环. 最终, 用于保存循环过程中的临时信息的参数将被丢弃, 其他参数则成为最终返回值的一部分.\n再多一个例子演示尾递归, 来自 Learn You Some Erlang for great good! 的题目,\n编写一个列表拼合函数（zipping function）. 这个函数以两个长度相同的列表为参数, 把它们合并成一个元组列表, 每个元组中有两个数据项.\n普通版本的递归很容易实现:\n1 2  zip([], []) -\u0026gt; []; zip([H1|T1], [H2|T2]) -\u0026gt; [{H1,H2}|zip(T1, T2)].   同上面说的一样, 当参数是两个数不尽的列表时候, 这个写法会很容易就耗尽内存, 因为最后的调用在执行完函数以后还要和先前得到的元组组合成新的列表, 再没到最后一次调用之前, 栈中都要保存着已生成的元组.\n要写尾递归, 我的做法比较简单粗暴,\n 首先是观察基本情况, 就是输入的参数最简单的时候会是什么样的呢? 毫无疑问, 上面的例子最简单的情况就是两个空列表, tail_zip([], []) -\u0026gt; []., 次简单的时候是长度为 1 的列表, tail_zip([X],[Y]) -\u0026gt; [{X, Y}].\n 基本观察到这, 写出前端的接收函数 tail_zip(L1,L2) -\u0026gt; tail_zip(L1,L2, []).\n 写出得出结果的函数子句 tail_zip([], [], L) -\u0026gt; L;\n 接着是主函数的定义 tail_zip([H1|T1], [H2|T2], L)\n 因为是尾递归, 所以, 主函数的函数体都是尾调用的形式, 对应着上面的得出结果的函数子句, 基本就可以写出 tail_zip(T1, T2, [{H1,H2}|L])\n  1 2 3 4  tail_zip(L1, L2) -\u0026gt; reverse(tail_zip(L1, L2, [])). tail_zip([], [], L) -\u0026gt; L; tail_zip([H1|T1], [H2|T2], L) -\u0026gt; tail_zip(T1, T2, [{H1,H2}|L]).    最后, 由于是尾递归, 如果不做一次 reverse 的话得到的其实是 [{c,3},{b,2},{a,1}]  虽然多了一次 reverse, 但是, 内存上还是节省的, 对于短列表来说, 可能会发现普通递归版本的运行速度要快于尾递归版本, 但是随着列表规模增大, 反转列表占据的时间比重会越来越小.\n好吧, 写完出来好像很轻松的样子, 其实理解这也花了不少时间的, 刚开始觉得超级烧脑的, 甚至直接跳过了这一部分的内容了.\nErlang and OTP in Action 里边有教如何编写递归函数的窍门.\n在 Ruby 上测试了一下, 不管是普通的递归还是尾递归的写法, 加到某个数就耗尽内存报错 SystemStackError: stack level too deep. 比如我当时的执行环境, 大概是累加到 10000 多一点就歇菜不能加下去了.\nJavaScript 貌似也是类似, 不过据说 ECMAScript 2015 的严格模式支持尾调用优化, 有待测试\n","permalink":"https://xguox.me/erlang-weekly-note-04.html/","tags":["Erlang"],"title":"Erlang weekly note 04 - Recursion 递归"},{"categories":["Erlang"],"contents":"这是一段 Ruby 方法的定义, 主流的语言语法都差不多.\n1 2 3 4 5 6 7 8 9 10  def greet(gender, name) case gender when :male puts(\u0026#34;Hello, Mr. #{name}!\u0026#34;) when :female puts(\u0026#34;Hello, Mrs. #{name}!\u0026#34;) else print(\u0026#34;Hello, #{name}!\u0026#34;) end end   但是, 如果换做 Erlang 来写的话:\n1 2 3 4 5 6  greet(male, Name) -\u0026gt; io:format(\u0026#34;Hello, Mr. ~s!\u0026#34;, [Name]); greet(female, Name) -\u0026gt; io:format(\u0026#34;Hello, Mrs. ~s!\u0026#34;, [Name]); greet(_, Name) -\u0026gt; io:format(\u0026#34;Hello, ~s!\u0026#34;, [Name]).   在 Erlang 中一个很重要的概念就是模式匹配. 调用函数时, Erlang 会自上而下地尝试对子句进行模式匹配. 首先, 用输入参数去匹配第一个子句中的模式；若匹配失败, 就尝试下一个子句(这个例子中, 就是 greet(female, Name))进行匹配, 如果匹配成功, 就运行该匹配对应的语句\u0026hellip; 虽然用 Erlang 也能写出类似第一种 Ruby 的写法, 但是, 使用模式匹配同时完成了函数选择和变量绑定两件事情, 也就无需先绑定变量, 然后再去比较它们.\n在 Erlang 的这个函数写法中, 每一条函数声明都被称作一个函数子句, 函数子句之间使用分号(;)分隔, 所有的函数子句一起形成一个完整的函数定义, 而整个函数作为一个语句, 需要用 句号(.)结尾. BTW, 每个表达式用逗号分隔, 之前提到过了.\n模式匹配\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  %% erl  Point = {4,5}. % {4,5} {X,Y} = Point. % {4,5} X. % 4 Y. % 5 {Z,_} = Point. % {4,5} Z. % 4   {4,5} 是一个元组, 一开始, X和Y没有值, 所以都是未绑定的变量. 当把它们放在=操作符左边的元组{X,Y}中时, = 操作符会去比较两边的值: {X,Y} 和 {4,5}. Erlang 会把值从元组中取出, 并把它们分配给左边的未绑定变量. 接下来, 就变成{4,5}={4,5}.的比较了, 显然是成立的. {Z,_} = Point. 使用了无需关心型变量 _ .意思是把本来应该放置在这个位置的值丢弃掉, 因为我们不会用到这个值(很多语言其实也都这么干). _ 在模式匹配中充当通配符的作用. 只要两边元组中元素的个数(元组的长度)相等, 就可以通过模式匹配来提取元组中的元素.\n1 2 3 4  {_,_} = {a,b}. % {a,b} {_,_} = {a,b,c}. ** exception error: no match of right hand side value {a,b,c}   列表也可以通过进行模式匹配, 从而获取它的头和尾\n1 2 3 4 5  % functions.erl  -module(functions). -compile(export_all). head([H|_]) -\u0026gt; H.   上面编写的 head/1 和 erlang:hd/1 其实一样的, 以一个列表为参数, 返回这个列表的第一个元素. functions:head([7,6,5,4]) 得到的是返回值是 7\n如果想得到列表的第二个元素, 可以创建下面的函数:\n1  second([_,X|_]) -\u0026gt; X.   使用函数和模式匹配, 可以比较出传给函数的两个参数是否完全一样. 为此, 我们创建一个名为same/2 的函数, 它有两个参数, 返回两个参数是否一样:\n1 2 3 4  same(X,X) -\u0026gt; true; same(_,_) -\u0026gt; false.   当调用same(a,a)时, 第一个 X 未绑定, 所以会自动获取值 a. 然后, 当 Erlang 处理第二个参数时, 发现X已经绑定了. 于是, Erlang 就把 X 已经绑定的值和作为第二个参数传递给函数的 a 进行比较, 检查它们是否匹配. 模式匹配成功, 所以函数返回 true. 否则进入第二个子句返回 false.\n高阶函数 higher-order function\nErlang 是一门函数式编程语言, 这类语言的一个显著特征就是可以像处理数据一样处理函数, 也就是说, 函数可以是别的函数的输入或者输出, 可以把函数存在数据结构中供后续使用. 其实, Ruby 和 JavaScript 这些语言也有相似的 Lambda 概念, 所以, 还是很好理解的.\n作为现有函数别名的 fun 函数\n若要引用当前模块内的函数, 比如 greet/2, 并告知程序的其他部分: \u0026ldquo;请调用这个函数\u0026rdquo;. 那么可以创建一个这样的 fun 函数\n1  fun greet/2   和各种其他类型的值一样, 可以将之与变量绑定 F = fun greet/2\n或者直接传给别的函数 another_fun(fun greet/2)\n匿名 fun 函数(Lambda 表达式)\n下面是一个简单的匿名函数, 不接受任何参数, 且总是返回零:\n1  fun () -\u0026gt; 0 end   以 fun 关键字开头, end 关键字结尾, 之间可以是一个或者多个不带函数名的函数子句.\n比如, 再一个稍复杂一点的例子:\n1 2 3 4 5 6 7  fun ({circle, Radius}) -\u0026gt; Radius * Radius * math:pi(); ({square, Side}) -\u0026gt; Side * Side; ({rectangle, Height, Width}) -\u0026gt; Height * Width; end   闭包\n闭包指的就是可以让函数引用到它所携带的某些环境(作用域中的值部分). 换句话说, 当匿名函数、作用域的概念以及可以携带变量的能力结合在一起时, 闭包就出现了.\n1 2 3 4 5 6  a() -\u0026gt; Secret = \u0026#34;pony\u0026#34;, fun() -\u0026gt; Secret end. b(F) -\u0026gt; \u0026#34;a/0\u0026#39;s password is \u0026#34;++F().   编译运行\n1 2 3 4  c(hhfuns). % {ok, hhfuns} hhfuns:b(hhfuns:a()). % \u0026#34;a/0\u0026#39;s password is pony   由于定义在 a/0 中的匿名函数继承了 a/0 的作用域, 所以, 根据前面的解释, 当在 b/1 中执行这个匿名函数时, 它仍然携带着 a/0 的作用域.\n","permalink":"https://xguox.me/erlang-weekly-note-03.html/","tags":["Erlang"],"title":"Erlang weekly note 03 - Function 函数"},{"categories":["Erlang"],"contents":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  % erl % Eshell V9.0 (abort with ^G)  % 列表中的第一个元素称为头(head)  hd([1,2,3,4]). 1 % 剩余的部分称为尾(tail)  tl([1,2,3,4]). [2,3,4] % 列表的长度 length([1,2,3,4]). 4   模块(module)是一个具有名字的文件, 其中包含一组函数. 像上面这些 hd, tl, length 都是属于 erlang 模块的 BIF(built-in functions 内建函数).\nerlang 模块中BIF函数和其他函数不同, 因为在启动Erlang时, 它们会被自动引入. 模块中的其他所有函数都必须用 Module:Function(Arguments) 这样的形式进行调用\n1 2 3 4 5 6 7 8  erlang:element(2, {a,b,c}). b element(2, {a,b,c}). b lists:seq(1,4). [1,2,3,4] seq(1,4). ** exception error: undefined shell command seq/2   lists 模块中的 seq 函数不会被自动引入, 而 element 函数会被自动引入.\n创建模块\n所有模块属性都采用 -Name(Attribute). 的形式, 要让你的模块能够编译, 下面的模块属性必须定义:\n1  -module(Name).   这个属性永远是文件中第一个属性(也是第一条语句).\n其中 Name 必须是原子, 且必须和模块的文件名一致. 当调用某个模块中的函数时, 所使用的模块名就是对应的这个 Name.\n1 2  % 只要有了这一行就是一个合法的模块 -module(useless).   export\n要决定模块中的哪些函数可以在别的模块中调用, 需要用 export 导出. 作用类似于 OOP 的 public 和 private 方法.\n形如\n1  -export([Function1/Arity, Function2/Arity,`...`, FunctionN/Arity]).   函数的元数 Arity 是个整数, 表示这个函数接收的参数个数. 同一个模块中, 即使函数名相同, 只要元数不一样, 就表示不同的函数. 在讨论具体某个函数时一定要说明元素只提供函数名是不够的, 还得用斜杠 / 作为分隔符带上元数.\n函数add(X,Y)和add(X,Y,Z)就是不同的函数, 分别表示成add/2和add/3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  -module(useless). -export([add/2, hello/0, greet_and_add_two/1]). add(A,B) -\u0026gt; A + B. %% 显示欢迎语 %% io:format/1是标准的文本输出函数 hello() -\u0026gt; io:format(\u0026#34;Hello, world!~n\u0026#34;). greet_and_add_two(X) -\u0026gt; hello(), add(X,2).   有 export 自然会想到有 import\n如果想用和add/2或者其他定义在当前模块中的函数一样的方式去调用io:format/1, 可以在文件的前面增加一个模块属性: -import(io,[format/1]).\n-import属性定义的一般形式如下:\n1  -import(Module, [Function1/Arity, `...`, FunctionN/Arity]).   导入函数带来了方便, 但是也会降低代码的可读性. 要判断出代码中使用的到底是哪个函数, 就要到文件的最前面去查看这个函数是从哪个模块中导入进来的.\n通常, 只有 lists 模块中的函数会被引入, 因为 lists 模块中函数的使用频率要远高于其他大多数模块中的函数.\nMACRO 宏\nErlang 的宏和 C 语言的 #define 语句类似, 主要用来定义简短的函数和常量.在代码被编译成供虚拟机使用的字节码之前, 它们会被替换成所代表的文本表达式.\n定义宏的方式也是用模块属性,\n1  -define(MACRO, some_value).   然后, 就可以在模块的任意函数中使用宏 ?MACRO 了, 这个宏在代码编译前会被替换成some_value.\n函数\u0026rsquo;宏的定义方法类似.下面是一个简单的函数宏, 进行两个数的减法:\n1  -define(sub(X,Y), X-Y).   如果调用 ?sub(23,47), 这个调用会被编译器替换成 23-47.\nErlang 中有一些预定义的宏, 比如:\n ?MODULE, 会被替换成当前模块的名字, 是一个原子; ?FILE, 会被替换成当前文件的名字, 是一个字符串; ?LINE, 会被替换成该宏所在的代码行的行号.  使用属性 -ifdef(MACRO)., -else. 和 -endif. 可以检测某个特定的宏是否已在代码中定义.\n编译运行\n在命令行中使用 erl 打开 Erlang shell, 默认情况下, shell 只会在它的启动目录和标准库中去查找文件. cd/1 函数专用于 Erlang shell, 可以更换 shell 的当前目录, 这样寻找文件就会方便一些.\n1 2 3 4 5 6  % 编译 useless 模块 1\u0026gt; c(useless). {ok,useless} 2\u0026gt; useless:greet_and_add_two(200). Hello, world! 202   示例来自 Learn You Some Erlang for Great Good!\n","permalink":"https://xguox.me/erlang-weekly-note-02.html/","tags":["Erlang"],"title":"Erlang weekly note 02 - Module 模块"},{"categories":["JavaScript","React","Ruby"],"contents":"layout: post title: \u0026ldquo;Rails 5 webpacker 部署时候报错\u0026rdquo; date: 2017-07-26 13:25:20 categories: [JavaScript, React, Ruby]\ntags: [JavaScript, React, Ruby] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  SSHKit::Command::Failed: rake exit status: 2 rake stdout: yarn install v0.21.3 [1/4] Resolving packages... [2/4] Fetching packages... info \u0026#34;fsevents@1.1.2\u0026#34; is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 152.92s. Webpacker is installed 🎉 🍰 Using /rails_apps/supply_chain/releases/20170725080931/config/webpack/paths.yml file for setting up webpack paths Compiling webpacker assets 🎉 rake stderr: WARNING: Use strings for Figaro configuration. false was converted to \u0026#34;false\u0026#34;. WARNING: Use strings for Figaro configuration. true was converted to \u0026#34;true\u0026#34;. warning No license field warning fsevents@1.1.2: The platform \u0026#34;linux\u0026#34; is incompatible with this module.   到服务器上手工执行\n1  RAILS_ENV=staging bundle exec rake assets:precompile --trace   结果报了\n1  No PostCSS Config found 一堆。。。。。。。。。。。。。   改一下 config/webpack/loaders/sass.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  const ExtractTextPlugin = require(\u0026#39;extract-text-webpack-plugin\u0026#39;) const path = require(\u0026#39;path\u0026#39;) const { env } = require(\u0026#39;../configuration.js\u0026#39;) const postcssConfig = path.resolve(process.cwd(), \u0026#39;.postcssrc.yml\u0026#39;) module.exports = { test: /\\.(scss|sass|css)$/i, use: ExtractTextPlugin.extract({ fallback: \u0026#39;style-loader\u0026#39;, use: [ { loader: \u0026#39;css-loader\u0026#39;, options: { minimize: env.NODE_ENV === \u0026#39;production\u0026#39;, } }, { loader: \u0026#39;postcss-loader\u0026#39;, options: { sourceMap: true, config: { path: postcssConfig } } }, \u0026#39;resolve-url-loader\u0026#39;, { loader: \u0026#39;sass-loader\u0026#39;, options: { sourceMap: true } } ] }) }    完事, 收工.\n","permalink":"https://xguox.me/rails-webpacker-deploy.html/","tags":["JavaScript","React","Ruby"],"title":"Rails 5 webpacker 部署时候报错"},{"categories":["JavaScript","React"],"contents":"写了一段时间 React, Redux, 发现之前对 Redux 的 Store 理解偏差了.\n每次发生 dispatch 某一个 action, 除去 action.type 的对应的 reducer, 其余都返回默认值通常是 switch 的 default 语句, Redux 在背后做了一些事情, 并不是每次传的都是定义时候传的默认值, 而是把对应的 reducer 的改动后的 state 作为参数传给了这个 reducer\n比如, 下面这个 reducer, 初始状态 state 等于 {isLoading: false, isLoggedIn: false, token: localStorage.token}, 假设 dispatch 了 AUTH_SUCCESS 以后, {isLoading: false, isLoggedIn: true, token: localStorage.token,} 就会存在了 Redux 的 Store 里边, 下一次如果 dispatch 的 action 只要对应的 action.type 不在这里边的任意一个, 自然会返回 default 值, 但是, 此时传给 auth 的默认 state 参数不在是定义方法时候的这个参数, 而是之前保存下来的 {isLoading: false, isLoggedIn: true, token: localStorage.token,}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  import {AUTH_FAILURE, AUTH_REQUEST, AUTH_SUCCESS} from \u0026#39;../actions/auth_actions\u0026#39;; export default function auth(state = { isLoading: false, isLoggedIn: false, token: localStorage.token, }, action) { switch (action.type) { case AUTH_REQUEST: return Object.assign({}, state, { isLoading: true, isLoggedIn: false, }); case AUTH_SUCCESS: return Object.assign({}, state, { isLoading: false, isLoggedIn: true, token: localStorage.token, }); case AUTH_FAILURE: return Object.assign({}, state, { isLoading: false, isLoggedIn: false, errMsg: action.msg, }); default: return state; } };    当浏览器刷新以后, Store 的值就会被清掉, 再次 dispatch 则回归到方法定义时候的 state 参数了.\n= . =\n尴尬, 之前都理解成什么了.\n","permalink":"https://xguox.me/redux-reducers-default-state.html/","tags":["JavaScript","React"],"title":"Redux 的中 Store 的默认 state"},{"categories":["Erlang"],"contents":"            一些意料之外但又在情理之中的语法.        为什么不是「如日中天」的 Elixir 呢? 我也想问, 也不知道. 就只是想先入坑 Erlang呢.\n因为「主流」的语法先入为主, 导致 Erlang 学起来比 Ruby, Java, Swift 感觉要难了不少. 倒不是说不好, 纯粹只是习惯问题. 在舒适区呆久了, 会形成思维惰性, 也会加深排他性.\nLearn You Some Erlang for great good! 和 Programming Erlang 交替看了一星期. 还只是在顺序编程这一块的语法上.\n一些意料之外但又在情理之中的语法. Erlang 的变量首字母是大写的, 虽然叫「变量」, 但一旦赋值就不能更改.\n1 2 3 4 5  % erl 1\u0026gt; X = 1. 1 2\u0026gt; X = 2. ** exception error: no match of right hand side value 2   其实, Erlang 的这等号不叫赋值, 叫绑定会正确一些. 绑定了就不能变. 刚开始也许接受困难. 但是, 好处是看代码会简单明了一些. 不然改来改去还得一步步去回溯.\n表达式序列必须要以点号(.)结尾 而不是常见的 ; 也不是像 Ruby 那样不用写\n可以用逗号来分隔表达式, 但是只会显示最后一个表达式的执行结果(其他表达式也都执行了). 也许看起来很奇怪, 但是, 其实咱们英文中文不都是用句号作为结尾的么?\n布尔操作符 and 和 or 对操作符两边的参数都会去求值. 如果想要的是一个短路操作符(只在有必要时, 才去求值右边的参数), 可以使用 andalso 和 orelse.\n原子 atom, 像 Ruby 中的 Symbol, 但省去冒号了.\npry\n1 2 3 4 5  [1] pry(main)\u0026gt; ruby NameError: undefined local variable or method `ruby\u0026#39; for main:Object from (pry):1:in `__pry__\u0026#39; [2] pry(main)\u0026gt; :ruby =\u0026gt; :ruby   erl\n1 2 3  Eshell V9.0 (abort with ^G) 1\u0026gt; ruby. ruby   但有一个真正是奇葩的语法, 小于等于的写法居然是 =\u0026lt;,\n最后, 列表与字符串的那点小事.\n1 2 3 4 5 6 7 8 9  [] [1, 2, 3, 9, 100] [zero, two, four] [{google, \u0026#34;Android\u0026#34;}, {apple, \u0026#34;iOS\u0026#34;}] % 上面这些都是一些列表, 注释用百分号  % 管道符号 | 左侧的元素 1 与右侧的空列表, 合并构成一个新的列表, 结果是 [1] [1 | []].   同理, 这里结果是[1, 10, 100]\n1  [1 | [10, 100]].   新元素是从左侧往右侧添加的\n1 2 3 4 5 6 7 8 9 10 11  % erl 1\u0026gt; [1|[]]. [1] 2\u0026gt; [[]|1]. [[]|1] [5, 4, 3 | [2, 1]]. % 这样会得到 [5, 4, 3, 2, 1], 但理解这一句应该是依次添加元素 3, 4, 5 到列表 [2, 1]的前头  [a, b, c, d] ++ [x, y, z]. % 使用 ++ 给任意列表追加元素, ++ 左侧的列表长度决定了运算的耗时, 与右侧列表长度无关    新内容(通常较短)应该尽量从左侧加入列表, 即便最终得到的是逆序也无妨. 随着新元素的加入, 列表越来越长, 与其每次都为了在列表末尾添加元素而反复遍历列表, 还不如最后用一个函数调用快速反转逆序的列表. 《Erlang OTP并发编程实战》\n 其实列表也还好理解, 但是问题来了. Erlang 居然没有字符串. 虽然, 可以用双引号输入一串字符, 比如\n1 2 3 4  1\u0026gt; \u0026#34;Hello\u0026#34;. \u0026#34;Hello\u0026#34; 2\u0026gt; \u0026#34;abcd\u0026#34;. \u0026#34;abcd\u0026#34;   然而, 其实质还是列表, 由该字符串各个字符的数字编码所对应的整数所组成的一个列表. 上面俩等价于\n1 2 3  [97, 98, 99]. [72, 101, 108, 108, 111].   在 Erlang shell 中为了区别对待字符串和普通列表, 会检查列表的元素, 是否全部为可打印字符. 如果是, 就打印成双引号字符串, 否则打印出一个整数列表.\n简单过了一遍 Erlang, 下回具体一些吧, 给自己立个 Flag, 完成自己的 Erlang 周记.\n","permalink":"https://xguox.me/this-week-with-erlang.html/","tags":["Erlang"],"title":"Erlang weekly note 01"},{"categories":["Jabber"],"contents":"上一篇是四月份, 至今两个多月没更新. 两个月好像做了很多事情， 却又感觉没收获什么值得写的样子.\n工作上, 写前端 React 比写 Ruby 的时间还多, 课余又捣腾了一会 Android, 然而, 现在也停了. 除去上班时间, 因为早睡, 晚上其实也没很丰富的时间去捣腾这些.\n总感觉是抱有一些不那么正确的目的在捣腾, 所以, 会有意识的忽然停止.\n也总会有拖延, 懒惰的时候， 停止就一发不可收拾.\n抛开那些「不正确的目的」, 选了看似脑热的决定(过程类似抛硬币).\nHello, Erlang.\n","permalink":"https://xguox.me/last-two-months.html/","tags":["Jabber"],"title":"停更的两个月"},{"categories":["Ruby","Postgres","Translation"],"contents":" 如果想知道你的数据库查询为啥变得越来越慢了， 那没有啥比 postgres 的 EXPLAIN 更好使的了。\n其实也没啥神秘的。 就是让 postgres 告诉咱们，它是怎么去执行这个查询的。你甚至可以进行实际查询然后比较实际的和预期性能差别。\nSound familiar? 也许你已经见识过 EXPLAIN. 早在 Rails 3.2 开始， Rails 会自动 **EXPLAIN 那些耗时大于 500ms 的查询。\n但问题是，其输出的结果有些隐秘。 举个例子，这是从 rails development blog 拿来的:\n1 2 3 4 5 6 7 8 9 10 11 12  % User.where(:id  =\u0026gt; 1).joins(:posts).explain EXPLAIN for: SELECT \u0026#34;users\u0026#34;.* FROM \u0026#34;users\u0026#34; INNER JOIN \u0026#34;posts\u0026#34; ON \u0026#34;posts\u0026#34;.\u0026#34;user_id\u0026#34; = \u0026#34;users\u0026#34;.\u0026#34;id\u0026#34; WHERE \u0026#34;users\u0026#34;.\u0026#34;id\u0026#34; = 1 QUERY PLAN ------------------------------------------------------------------------------ Nested Loop Left Join (cost=0.00..37.24 rows=8 width=0) Join Filter: (posts.user_id = users.id) -\u0026gt; Index Scan using users_pkey on users (cost=0.00..8.27 rows=1 width=4) Index Cond: (id = 1) -\u0026gt; Seq Scan on posts (cost=0.00..28.88 rows=8 width=4) Filter: (posts.user_id = 1) (6 rows)   所以， 这是什么鬼？\n在这篇文章我们将会分析类似这样的输出结果，及其给我们的 Ruby Web 开发所带来的影响。\nThe syntax 如果正在使用的是 Rails, 那么可以在任意 ActiveRecord 查询后面加上 .explain。\n1 2 3 4 5 6 7 8  \u0026gt; User.where(id: 1).explain User Load (10.0ms) SELECT \u0026#34;users\u0026#34;.* FROM \u0026#34;users\u0026#34; WHERE \u0026#34;users\u0026#34;.\u0026#34;id\u0026#34; = $1 [[\u0026#34;id\u0026#34;, 1]] =\u0026gt; EXPLAIN for: SELECT \u0026#34;users\u0026#34;.* FROM \u0026#34;users\u0026#34; WHERE \u0026#34;users\u0026#34;.\u0026#34;id\u0026#34; = $1 [[\u0026#34;id\u0026#34;, 1]] QUERY PLAN -------------------------------------------------------------------------- Index Scan using users_pkey on users (cost=0.29..8.30 rows=1 width=812) Index Cond: (id = 1) (2 rows)   虽然，EXPLAIN 方法使用起来很方便， 但是， 它并没有给你一些我们可以在 postgres 中可以用到的高级选项。\n要在 postgres 中直接使用 EXPLAIN， 首先用 psql -d yourdb 进入到你的数据库中，然后再执行下面这条语句:\n1 2 3 4 5 6  EXPLAIN SELECT * FROM users WHERE id=1; QUERY PLAN -------------------------------------------------------------------------- Index Scan using users_pkey on users (cost=0.29..8.30 rows=1 width=812) Index Cond: (id = 1) (2 rows)   这样我们就能得到一些关于 postgres 如何执行查询的信息，包括最少需要做多少工作来完成这个查询。\n只用 EXPLAIN 的话并不会真正的执行这条语句，想要真正执行语句， 并给出预期与实际运行结果的对比则需要用 EXPLAIN ANALYZE\n1 2 3 4 5 6  EXPLAIN ANALYZE SELECT * FROM users WHERE id=1; QUERY PLAN ------------------------------------------------------------------------------------ Index Scan using users_pkey on users (cost=0.29..8.30 rows=1 width=812) (actual time=0.043..0.044 rows=1 loops=1) Index Cond: (id = 1) Total runtime: 0.117 ms   Interpreting the output Postgres 很聪明的呢，他可以找到最高效的方式来执行你的查询。换句话说，他会生成一个「查询计划」，然后 explain只是负责把这个计划输出出来。\n考虑以下情况:\n1 2 3 4 5 6 7 8  # explain select * from users order by created_at limit 10; QUERY PLAN ------------------------------------------------------------------------- Limit (cost=892.98..893.01 rows=10 width=812) -\u0026gt; Sort (cost=892.98..919.16 rows=104 width=812) Sort Key: created_at -\u0026gt; Seq Scan on users (cost=0.00..666.71 rows=104 width=812) (4 rows)   这个输出由两部分组成:\n 「节点列表」(node list) 描述了执行该查询时候所发生的一系列动作 性能预估，描述了这个列表中的每个动作的耗时。  The node list 去掉所有的性能评估以后， 也就只剩下节点列表了，如下所示:\n1 2 3  Limit -\u0026gt; Sort (Sort Key: created_at) -\u0026gt; Seq Scan on users   这有点像是一个 Postgres 写的「程序」用来执行查询操作。 其中包含了三个动作， \u0026ldquo;limit\u0026rdquo;, \u0026ldquo;sort\u0026rdquo; 和 \u0026ldquo;seq scan\u0026rdquo;。 子节点中的输出结果会被向上传送到父节点。 用 Ruby 的话来说就是:\n1  all_users.sort(:created_at).limit(10)   Postgres 会在查询计划中使用许多不同的动作。 但你并不需要掌握所有的这些动作的含义，记住下面这几个就好了:\n Index Scan: 通过索引来获取记录。 类似在 Ruby 的 Hash 中查找某一项。 Seq Scan: 通过遍历结果集来获取记录。 Filter: 从结果集中选择那些匹配某条件的记录。 Sort: 对结果集排序 Aggregate: 当执行像 count, max, min 这类操作时候会用到 Bitmap Heap Scan: Uses a bitmap to represent matching records. Operations like and-ing and or-ing can sometimes be done more easily to the bitmap than to the actual records.  https://github.com/digoal/blog/blob/master/201702/20170221_02.md\nPerformance estimates 节点列表中， 每个节点都有一组性能评估。一般长这样子的:\n1  Limit (cost=892.98..893.01 rows=10 width=812)   这些数字代表意思:\n Cost: 该动作的执行成本。它没有具体的单位， 只能和其他同样是 Cost 的数字进行对比 Rows: 执行该操作一共需要遍历多少行记录。 Width: 每一行大概有多少字节。  最常用到的参数应该是 Rows。它可以清楚的让我们知道查询的性能。 如果这个值等于 1， 那么该查询性能杠杠的。 如果发现这个值等于表中记录的总数，那么这个查询性能就会随着表的增大变的越来越慢了。\nActual performance values 使用 EXPLAIN ANALYZE 时， 会真正的去执行查询，然后就会出现两组数字。前一组是像上面的预估值，后一组则是实际值。\n1 2 3 4 5 6 7 8 9 10  # explain analyze select * from users order by created_at limit 10; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------ Limit (cost=892.98..893.01 rows=10 width=812) (actual time=22.443..22.446 rows=10 loops=1) -\u0026gt; Sort (cost=892.98..919.16 rows=10471 width=812) (actual time=22.441..22.443 rows=10 loops=1) Sort Key: created_at Sort Method: top-N heapsort Memory: 31kB -\u0026gt; Seq Scan on users (cost=0.00..666.71 rows=10471 width=812) (actual time=0.203..15.221 rows=10472 loops=1) Total runtime: 22.519 ms (6 rows)    Actual time: 执行该动作耗费的时长(ms) Rows: 执行该操作实际遍历了多少行记录。 Loops: 有时候某个动作会发生不止一次， 此时， loops 会大于 1  More verbose output 默认情况下 EXPLAIN 给的是一个比较精简的版本。其实， 你可以让从他那得到更多更详细的信息。甚至可以让他的输出信息格式化为 JSON 或者 YAML.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  # EXPLAIN (ANALYZE, FORMAT YAML) select * from users order by created_at limit 10;QUERYPLAN-------------------------------------------Plan:+NodeType:\u0026#34;Limit\u0026#34;+StartupCost:892.98+TotalCost:893.01+PlanRows:10+PlanWidth:812+ActualStartupTime:12.945+ActualTotalTime:12.947+ActualRows:10+ActualLoops:1+Plans:+-NodeType:\u0026#34;Sort\u0026#34;+ParentRelationship:\u0026#34;Outer\u0026#34;+StartupCost:892.98+TotalCost:919.16+PlanRows:10471+PlanWidth:812+ActualStartupTime:12.944+ActualTotalTime:12.946+ActualRows:10+ActualLoops:1+SortKey:+-\u0026#34;created_at\u0026#34;+SortMethod:\u0026#34;top-N heapsort\u0026#34;+SortSpaceUsed:31+SortSpaceType:\u0026#34;Memory\u0026#34;+Plans:+-NodeType:\u0026#34;Seq Scan\u0026#34;+ParentRelationship:\u0026#34;Outer\u0026#34;+RelationName:\u0026#34;users\u0026#34;+Alias:\u0026#34;users\u0026#34;+StartupCost:0.00+TotalCost:666.71+PlanRows:10471+PlanWidth:812+ActualStartupTime:0.008+ActualTotalTime:5.823+ActualRows:10472+ActualLoops:1+Triggers:+TotalRuntime:13.001(1row)   还可以这样，EXPLAIN (ANALYZE, VERBOSE, FORMAT YAML) SELECT ...\nVisualization tools Explain 会生成一大堆的输出。 尤其当查询比较复杂的时候，更是难以解析。\n有一些免费的工具可以帮上忙。他们会帮助解析并生成一些漂亮的图表给你。甚至会给出一些标记指出潜在的性能问题。比如这个\nExamples 把上面提到的汇总一起举个例子。你是否发现某一些 Rails 的通用做法会生成一些性能不佳的数据库查询?\nThe counting conundrum 下面的用法应该是非常常见的吧:\n1  Total Faults \u0026lt;%= Fault.count %\u0026gt;   对应生成的 SQL 语句如下:\n1  select count(*) from faults;   给 EXPLAIN 一下，看看发生了些什么事情。\n1 2 3 4 5 6  # explain select count(*) from faults; QUERY PLAN ------------------------------------------------------------------- Aggregate (cost=1840.31..1840.32 rows=1 width=0) -\u0026gt; Seq Scan on faults (cost=0.00..1784.65 rows=22265 width=0) (2 rows)   Woah! 简单的一个 count 查询，竟然遍历了 22,265 行， 也就是整个表。 在 Postgres , count 都是全表查的。\nThe sorting scandal 下面这个也常见吧， 根据某个字段进行排序。 比如:\n1  Fault.order(\u0026#34;created_at desc\u0026#34;).limit(10)   其实你只想要 10 条记录而已。 但， 为了得到这 10 条记录， 你需要对整个表进行排序。 从下面可以看出 Sort 操作遍历了 22,265 行记录.\n1 2 3 4 5 6 7  # explain select * from faults order by created_at limit 10; QUERY PLAN ---------------------------------------------------------------------------- Limit (cost=2265.79..2265.81 rows=10 width=1855) -\u0026gt; Sort (cost=2265.79..2321.45 rows=22265 width=1855) Sort Key: created_at -\u0026gt; Seq Scan on faults (cost=0.00..1784.65 rows=22265 width=1855)   通过添加索引，我们就可以把 **Sort 这个操作换成更高效的 Index Scan\n1 2 3 4 5 6 7 8 9  # CREATE INDEX index_faults_on_created_at ON faults USING btree (created_at); CREATE INDEX # explain select * from faults order by created_at limit 10; QUERY PLAN --------------------------------------------------------------------------------------------------------- Limit (cost=0.29..2.66 rows=10 width=1855) -\u0026gt; Index Scan using index_faults_on_created_at on faults (cost=0.29..5288.04 rows=22265 width=1855) (2 rows)   Conclusion 强烈建议多一些使用 EXPLAIN 这个命令， 这样， 当数据越来越大，时候不至于措手无策。\n译自: A Rubyist\u0026rsquo;s Guide to Postgresql\u0026rsquo;s Explain\n","permalink":"https://xguox.me/rubyist-guide-to-postgres-explain.html/","tags":["Ruby","Postgres","Translation"],"title":"给 Rubyist 的 Postgresql Explain 教程"},{"categories":["Nginx"],"contents":" 本地开发正常的很, 上到 staging 发现歇菜了, request.env 没找着自定义的 key, 折腾老久原来是 Nginx 给抹了.\n= . =\n解决办法: 要不换个名, 要不加一句配置\n1  underscores_in_headers: on  ","permalink":"https://xguox.me/nginx-underscore_in_header.html/","tags":["Nginx"],"title":"Nginx 默认不允许 header 的 key 带下划线"},{"categories":["JavaScript","React"],"contents":"Redux 中 reducer 返回 state 的时候经常用到 Object.assign， 然后几乎所有现代浏览器都试过能正常运行以后， 发现在微信的内置浏览器里边就报这玩意，\nweixin://preInjectJSBridge/fail\nbabel 没把该语法翻译成微信认识的，添加 babel-polyfill 这个库， webpack 入口配置一下， 就可以解决了。\n","permalink":"https://xguox.me/react-babel-wechat-es6-env.html/","tags":["JavaScript","React"],"title":"weixin://preInjectJSBridge/fail [React]"},{"categories":["Tools"],"contents":"老早就听说过用 Git 的 bisect, 二分查找问题 commit, 只是一直都没机会实践运用. (项目都跑的太正常了, 手动斜眼)\n其实, 只是绝大部分时候总可以通过各种 Log 来追踪到问题所在. 今个儿遇到了个实在没法看日志的问题.\n团队的成员提交了各自提交了各种代码并合并到了 master 分支, 正准备发测试环境, 发现部署的时候 precompile 崩掉了, 一看报错日志. 一脸懵逼了\u0026hellip;\n这只说了某个等号附近有语法错误, 具体在哪一个文件哪一行也没个说法, 只给出了这一大坨, 加起来单是当天新增的 commit 少说二三十个, 总不能一个个去 reset 看哪儿改的 JavaScript 引起吧 = . =\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  ExecJS::ProgramError: SyntaxError: Unexpected token operator «=», expected punc «,» (line: 58005, col: 69, pos: 2186981) Error at new JS_Parse_Error (\u0026lt;eval\u0026gt;:3623:11948) at js_error (\u0026lt;eval\u0026gt;:3623:12167) at croak (\u0026lt;eval\u0026gt;:3623:22038) at token_error (\u0026lt;eval\u0026gt;:3623:22175) at expect_token (\u0026lt;eval\u0026gt;:3623:22411) at expect (\u0026lt;eval\u0026gt;:3623:22562) at ctor.argnames (\u0026lt;eval\u0026gt;:3623:27486) at function_ (\u0026lt;eval\u0026gt;:3623:27550) at expr_atom (\u0026lt;eval\u0026gt;:3623:31068) at maybe_unary (\u0026lt;eval\u0026gt;:3624:1752) at expr_ops (\u0026lt;eval\u0026gt;:3624:2523) at maybe_conditional (\u0026lt;eval\u0026gt;:3624:2615) at maybe_assign (\u0026lt;eval\u0026gt;:3624:3058) new JS_Parse_Error ((execjs):3623:11948) js_error ((execjs):3623:12167) croak ((execjs):3623:22038) token_error ((execjs):3623:22175) expect_token ((execjs):3623:22411) expect ((execjs):3623:22562) ctor.argnames ((execjs):3623:27486) function_ ((execjs):3623:27550) expr_atom ((execjs):3623:31068) maybe_unary ((execjs):3624:1752) expr_ops ((execjs):3624:2523) maybe_conditional ((execjs):3624:2615) maybe_assign ((execjs):3624:3058)   还好可以在本地用\n1  RAILS_ENV=staging rake assets:precompile   还原 precompile 的错误,\n想起 git bisect, 看样子可以用上了.\n1 2 3 4  git bisect start git bisect good be835ca5ce4755ea106701587c308d6add8d4f7 # be835ca5ce4755ea106701587c308d6add8d4f7 是该分支上确定不会出现这个 bug 的 commit, # 其实, 只是随意在昨天的 commit 里面捞一条出来而已   标记好了以后跑一次\n1  RAILS_ENV=staging rake assets:precompile   发现还是编译不过, 于是\n1 2 3 4 5 6 7  # 标记为 bad git bisect bad Bisecting: 36 revisions left to master after this (roughly 5 steps) [76dbb96bc3b5bfb1465f81081aeebb9a34e6f92b] ➜ Cherry git:(76dbb96)   76dbb96 这个 commit 正好是 先前设定的 be835ca5ce4755ea106701587c308d6add8d4f7 跟当前最新 commit 的中间节点.\ngit 此时已经 checkout 到 76dbb96\n说明问题出在最新 commit 到 76dbb96 commit 之间.\n刚刚已经标记好了 bad, 再继续跑\n1  RAILS_ENV=staging rake assets:precompile   发现没有问题, 能编译过, 所以,\n1 2 3 4 5  git bisect good Bisecting: 18 revisions left to master after this (roughly 4 steps) [cdc07fc05a85465797086dc0f9fe5c663b7ffcf6] ➜ Cherry git:(cdc07fc)   再一次 checkout\n标记为 good 以后可以看到, 问题出在从 cdc07fc 到当前最新的代码之间某个 commit. 继续重复上述的步骤, 其实 git 有给提示最多还要 4 次就可以找出问题 commit 在哪了.\n实际可能更少. 更快就找到了问题所在的 commit 了.\n最后, 发现是 Javascript 方法默认值这里出的问题 = . =\n1 2 3 4 5 6 7 8 9 10  (function(){ window.DrawCarState = { single_car_state: function(car_id, begin_at, end_at, draw_target = \u0026#39;\u0026#39;, format = \u0026#39;html\u0026#39;){ $.ajax({ ... }) } } }).call(this);    定位好问题分支以后可以通过\n1  git bisect reset   恢复到 bisect 前的状态\n","permalink":"https://xguox.me/git-bisect-debug.html/","tags":["Tools"],"title":"Git Bisect Debug 利器"},{"categories":["Photo"],"contents":" Only ILCE 7R + FE 28mm F2\n滇池 民族村 大理洱海 丽江古城 玉龙雪山 蓝月谷 ","permalink":"https://xguox.me/28mm-trip-to-yun-nan.html/","tags":["Photo"],"title":"28mm \u0026 云南"},{"categories":["Elasticsearch"],"contents":" 上次写了关于 Elasticsearch 的分数(_score)是怎么计算得出, 不过那是在 Elasticsearch 5.0 以前的版本用的, Elasticsearch 前不久发布了 5.0 版本, 基于 Lucene 6, 默认使用了 BM25 评分算法.\nBM25 的 BM 是缩写自 Best Match, 25 貌似是经过 25 次迭代调整之后得出的算法. 它也是基于 TF/IDF 进化来的. Wikipedia 那个公式看起来很吓唬人, 尤其是那个求和符号, 不过分解开来也是比较好理解的.\n总体而言, 主要还是分三部分, TF - IDF - Document Length\nIDF 还是和之前的一样. 公式 IDF(q) = 1 + ln(maxDocs/(docFreq + 1))\nf(q, D) 是 tf(term frequency)\n|d| 是文档的长度, avgdl 是平均文档长度.\n先不看 IDF 和 Document Length 的部分, 变成 tf * (k + 1) / (tf + k),\n相比传统的 TF/IDF (tf(q in d) = sqrt(termFreq)) 而言, BM25 抑制了 tf 对整体评分的影响程度, 虽然同样都是增函数, 但是, BM25 中, tf 越大, 带来的影响无限趋近于 (k + 1), 这里 k 值通常取 [1.2, 2], 而传统的 TF/IDF 则会没有临界点的无限增长.\n而文档长度的影响, 同样的, 可以看到, 命中搜索词的情况下, 文档越短, 相关性越高, 具体影响程度又可以由公式中的 b 来调整, 当设值为 0 的时候, 就跟之前 \u0026lsquo;TF/IDF\u0026rsquo; 那篇提到的 \u0026quot;norms\u0026quot;: { \u0026quot;enabled\u0026quot;: false } 一样, 忽略文档长度的影响.\n综合起来,\nk = 1.2\nb = 0.75\nidf * (tf * (k + 1)) / (tf + k * (1 - b + b * (|d|/avgdl)))\n最后再对所有 term 求和. 就是 Elasticsearch 5 中一般查询的得分了.\nRelated: http://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/\n","permalink":"https://xguox.me/how-elasticsearch-5-scoring-with-bm25.html/","tags":["Elasticsearch"],"title":"Elasticsearch 5.X(Lucene 6) 的 BM25 相关度算法"},{"categories":["Tools"],"contents":" 在路上的时间超长如我, 不是果粉, 只是有一台 iPad Air 2.\n这货屏幕比手中的大法机大得多, 平日看电子书较多, 最近折腾起一些开发相关的工具, 好像还是很好玩的.\n目测再败个键盘估计会更好使一些. 不是要挑战成为主力机器, 只是可能也许会有用到的时候\u0026hellip;\nTermius SSH client (Free)\n最开始因为没有服务器密码, 一直都是用的 authorized_keys, 还以为要歇菜了, 结果, 发现居然还可以在 Termius 里生成,\n然后, 是连到服务器,\nhtop 一个\n想了想, 可以 ssh 到服务器, 那理论上应该也可以 ssh 到自己的 MacBook, 随手一搜, 还真有, macOS 自带就有功能, 也不用怎么折腾, 点几下就是.\nhttps://support.apple.com/kb/PH18726?locale=en_US\n但是, 发现, 好像只能内网?\n搁置了一下想到, MacBook 如果连的 VPN 和 iPad 连的 VPN 是同一个服务器的话, 不其实也算在一个内网, 实验证明是可以的.\n上图说话, MacBook 的 ~\n各种命令行跑起,\n原生的键盘之上加多了一些开发常用到的键, 比如 ctrl, alt, tab, esc\nVim 把代码写起来 = . =\n所以, 只带个 iPad 就上班是我想太多了咯?\nDash 前不久, Mac 上著名的文档工具 Dash 开源了 iOS 版本, 不过没有上架到 app store\nhttps://github.com/Kapeli/Dash-iOS\n但是, README 上已经把整个安装流程描述的很详细了, 所以, 还是直接上图说话.\nTouch Code Lite 代码编辑器(其实主要还是拿来看不是拿来写)\n比如没事看看 Rails 的源码什么的\n不能 CtrlP 只好全局搜, 只能算凑合着.\n这货有点 Bug, 某些后缀的文件或者没有后缀的文件直接显示个未知 = . =\niOctocat Github 客户端\n(◎_◎;) 取代 MacBook 暂时是没啥可能的, 但是, 如果每天有不少时间在路上各种交通的话, 可以试试这么玩.\n","permalink":"https://xguox.me/ios-developer-tools.html/","tags":["Tools"],"title":"与开发相关的一些 iOS App (通勤时间太长怪我咯?)"},{"categories":["Photo"],"contents":"说走就走的\u0026hellip;郊游\n珠三角这些天雾霾也严重了不少, 随口一说就跑到韶关南雄, 坪田银杏森林公园. 只是辛苦了老司机开车.\n银杏树, 小一些幼一些的都金黄金黄的, 只是这个森林公园里边的, 好大一部分是粗壮的老银杏了. 正值青黄相接的时候, 没有网络上的照片那般壮观.\n先上照片再评价\n表情包\n最真实的颜色\nEND\n镜头抗眩光, 紫边都表现一般, FE 28mm F2, FE 50mm F1.8\n没选择好的时机是我们的问题, 只是, 这边的景区真的是满满的套路.\n相关的单位甚至个人各种花心思在怎么赚游客的钱, 凭空架一些所谓岗哨, 刻意为难游客, 让游客去坐观光车, 然后就再有所谓当地人, 在一旁鼓吹, 可以以当地人身份带领自驾进去, 不要 999, 不要 99, 只要 50 块一次. 呵呵哒. 直接收门票多好, 那么多套路, 唉.\n里边各种小商贩餐馆就不说了, 人家也没强买强卖. 只是, 真的都花心思怎么赚钱了吗? 各种路线指引路标什么的, 那么大一个所谓森林公园, 就不能做好一些呢?\n比起大省城, 即使是韶关的市区, 空气是明显清新的\n","permalink":"https://xguox.me/shaoguan-nanxiong-pingtian-yinxing-photo.html/","tags":["Photo"],"title":"坪田的银杏...还没黄（ ＴДＴ）"},{"categories":["Elasticsearch"],"contents":" 上次写了关于 Elasticsearch 如何分词索引, 接着继续写 Elasticsearch 怎么计算搜索结果的得分(_score).\nElasticsearch 默认是按照文档与查询的相关度(匹配度)的得分倒序返回结果的. 得分 (_score) 就越大, 表示相关性越高.\n所以, 相关度是啥? 分数又是怎么计算出来的? (全文检索和结构化的 SQL 查询不太一样, 虽然看起来结果比较\u0026rsquo;飘忽\u0026rsquo;, 但也是可以追根问底的)\n在 Elasticsearch 中, 标准的算法是 Term Frequency/Inverse Document Frequency, 简写为 TF/IDF, (刚刚发布的 5.0 版本, 改为了据说更先进的 BM25 算法)\n-\u0026gt; Elasticsearch 5.X(Lucene 6) 的 BM25 相关度算法\nTerm Frequency 某单个关键词(term) 在某文档的某字段中出现的频率次数, 显然, 出现频率越高意味着该文档与搜索的相关度也越高\n具体计算公式是 tf(q in d) = sqrt(termFreq)\n另外, 索引的时候可以做一些设置, \u0026ldquo;index_options\u0026rdquo;: \u0026ldquo;docs\u0026rdquo; 的情况下, 只考虑 term 是否出现(命中), 不考虑出现的次数.\n1 2 3 4 5 6 7 8 9 10 11 12 13  PUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;index_options\u0026#34;: \u0026#34;docs\u0026#34; } } } } }   Inverse document frequency 某个关键词(term) 在索引(单个分片)之中出现的频次. 出现频次越高, 这个词的相关度越低. 相对的, 当某个关键词(term)在一大票的文档下面都有出现, 那么这个词在计算得分时候所占的比重就要比那些只在少部分文档出现的词所占的得分比重要低. 说的那么长一句话, 用人话来描述就是 \u0026ldquo;物以稀为贵\u0026rdquo;, 比如, \u0026lsquo;的\u0026rsquo;, \u0026lsquo;得\u0026rsquo;, \u0026lsquo;the\u0026rsquo; 这些一般在一些文档中出现的频次都是非常高的, 因此, 这些词占的得分比重远比特殊一些的词(如\u0026rsquo;Solr\u0026rsquo;, \u0026lsquo;Docker\u0026rsquo;, \u0026lsquo;哈苏\u0026rsquo;)占比要低,\n具体计算公式是 idf = 1 + ln(maxDocs/(docFreq + 1))\nField-length Norm 字段长度, 这个字段长度越短, 那么字段里的每个词的相关度也就越大. 某个关键词(term) 在一个短的句子出现, 其得分比重比在一个长句子中出现要来的高.\n具体计算公式是 norm = 1/sqrt(numFieldTerms)\n默认每个 analyzed 的 string 都有一个 norm 值, 用来存储该字段的长度,\n用 \u0026ldquo;norms\u0026rdquo;: { \u0026ldquo;enabled\u0026rdquo;: false } 关闭以后, 评分时, 不管文档的该字段长短如何, 得分都一样.\n1 2 3 4 5 6 7 8 9 10 11 12 13  PUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;norms\u0026#34;: { \u0026#34;enabled\u0026#34;: false } } } } } }   最后的得分是三者的乘积 tf * idf * norm 以上描述的是最原始的针对单个关键字(term)的搜索. 如果是有多个搜索关键词(terms)的时候, 还要用到的 Vector Space Model\n如果查询复杂些, 或者用到一些修改了分数的查询, 或者索引时候修改了字段的权重, 比如 function_score 之类的,计算方式也就又更复杂一些.\nExplain 看上去 TF/IDF 的算法已经一脸懵逼吓跑人了, 不过其实, 用 Explain 跑一跑也没啥, 虽然各种开方, 自然对数的, Google一个科学计算器就是了.\n举个例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  /*先删掉索引, 如果有的话*/ curl -XDELETE \u0026#39;http://localhost:9200/blog\u0026#39; curl -XPUT \u0026#39;http://localhost:9200/blog/\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;post\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;yes\u0026#34; } } } } }\u0026#39;    存入一些文档 (Water 随手加进去测试的.)\n1 2 3 4 5 6 7 8 9 10 11 12 13  curl -s -XPOST localhost:9200/_bulk -d \u0026#39; { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; }} { \u0026#34;title\u0026#34;: \u0026#34;What is the best water temperature, Mr Water\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; }} { \u0026#34;title\u0026#34;: \u0026#34;Water no symptoms\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; }} { \u0026#34;title\u0026#34;: \u0026#34;Did Vitamin B6 alone work for you? Water?\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34; }} { \u0026#34;title\u0026#34;: \u0026#34;The ball drifted on the water.\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34; }} { \u0026#34;title\u0026#34;: \u0026#34;No water no food no air\u0026#34; } \u0026#39;    bulk insert 以后先用 Kopf 插件输出看一下, 5 个文档并不是平均分配在 5 个分片的, 其中, 编号为 2 的这个分片里边有两个文档, 其中编号为 0 的那个分片是没有分配文档在里面的.\n接下来, 搜索的同时 explain\n原本输出的 json 即使加了 pretty 也很难看, 换成 yaml 会好不少\n1 2 3 4 5 6 7 8  curl -XGET \u0026#34;http://127.0.0.1:9200/blog/post/_search?explain\u0026amp;format=yaml\u0026#34; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;water\u0026#34; } } }\u0026#39;   输出如图(json)\n可以看到五个文档都命中了这个查询, 注意看每个文档的 _shard\n整个输出 yml 太长了, 丢到最后面, 只截取了其中一部分, 如图,\n返回排名第一的分数是 _score: 0.2972674, _shard(2),\n\u0026ldquo;weight(title:water in 0) [PerFieldSimilarity], result of:\u0026rdquo; 这里的 0 不是 _id, 只是 Lucene 的一个内部文档 ID, 可以忽略.\n排名第一和第二的两个文档刚好是在同一个分片的, 所以跟另外三个的返回结果有些许不一样, 主要就是多了一个 queryWeight, 里面的 queryNorm 只要在同一分片下, 都是一样的, 总而言之, 这个可以忽略(至少目前这个例子可以忽略)\n只关注 fieldWeight, 排名第一和第二的的 tf 都是 1,\n在 idf(docFreq=2, maxDocs=2) 中, docFreq 和 maxDocs 都是针对单个分片而言, 2号分片一共有 2个文档(maxDocs), 然后命中的文档也是两个(docFreq).\n所以 idf 的得分, 根据公式, 1 + ln(maxDocs/(docFreq + 1)) 是 0.59453489189\n最后 fieldNorm, 这个 field 有三个词, 所以是 1/sqrt(3), 但是按官方给的这个公式怎么算都不对, 不管哪个文档. 后来查了一下, 说是 Lucene 存这个 lengthNorm 数据时候都是用的 1 byte来存, 所以不管怎么着都会丢掉一些精度. 呵呵哒了 = . =\n最后的最后, 总得分 = 1 * 0.5945349 * 0.5 = 0.2972674.\n同理其他的几个文档也可以算出这个得分, 只是都要被 fieldNorm 的精度问题蛋疼一把.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259  took:3timed_out:false_shards:total:5successful:5failed:0hits:total:5max_score:0.2972674hits:-_shard:2_node:\u0026#34;aVHxi_d2TYq40ISddYZF-A\u0026#34;_index:\u0026#34;blog\u0026#34;_type:\u0026#34;post\u0026#34;_id:\u0026#34;2\u0026#34;_score:0.2972674_source:title:\u0026#34;Water no symptoms\u0026#34;_explanation:value:0.2972674description:\u0026#34;sum of:\u0026#34;details:-value:0.2972674description:\u0026#34;weight(title:water in 0) [PerFieldSimilarity], result of:\u0026#34;details:-value:0.2972674description:\u0026#34;score(doc=0,freq=1.0), product of:\u0026#34;details:-value:0.99999994description:\u0026#34;queryWeight, product of:\u0026#34;details:-value:0.5945349description:\u0026#34;idf(docFreq=2, maxDocs=2)\u0026#34;details:[]-value:1.681987description:\u0026#34;queryNorm\u0026#34;details:[]-value:0.29726744description:\u0026#34;fieldWeight in 0, product of:\u0026#34;details:-value:1.0description:\u0026#34;tf(freq=1.0), with freq of:\u0026#34;details:-value:1.0description:\u0026#34;termFreq=1.0\u0026#34;details:[]-value:0.5945349description:\u0026#34;idf(docFreq=2, maxDocs=2)\u0026#34;details:[]-value:0.5description:\u0026#34;fieldNorm(doc=0)\u0026#34;details:[]-value:0.0description:\u0026#34;match on required clause, product of:\u0026#34;details:-value:0.0description:\u0026#34;# clause\u0026#34;details:[]-value:1.681987description:\u0026#34;_type:post, product of:\u0026#34;details:-value:1.0description:\u0026#34;boost\u0026#34;details:[]-value:1.681987description:\u0026#34;queryNorm\u0026#34;details:[]-_shard:2_node:\u0026#34;aVHxi_d2TYq40ISddYZF-A\u0026#34;_index:\u0026#34;blog\u0026#34;_type:\u0026#34;post\u0026#34;_id:\u0026#34;4\u0026#34;_score:0.22295055_source:title:\u0026#34;The ball drifted on the water.\u0026#34;_explanation:value:0.22295056description:\u0026#34;sum of:\u0026#34;details:-value:0.22295056description:\u0026#34;weight(title:water in 1) [PerFieldSimilarity], result of:\u0026#34;details:-value:0.22295056description:\u0026#34;score(doc=1,freq=1.0), product of:\u0026#34;details:-value:0.99999994description:\u0026#34;queryWeight, product of:\u0026#34;details:-value:0.5945349description:\u0026#34;idf(docFreq=2, maxDocs=2)\u0026#34;details:[]-value:1.681987description:\u0026#34;queryNorm\u0026#34;details:[]-value:0.22295058description:\u0026#34;fieldWeight in 1, product of:\u0026#34;details:-value:1.0description:\u0026#34;tf(freq=1.0), with freq of:\u0026#34;details:-value:1.0description:\u0026#34;termFreq=1.0\u0026#34;details:[]-value:0.5945349description:\u0026#34;idf(docFreq=2, maxDocs=2)\u0026#34;details:[]-value:0.375description:\u0026#34;fieldNorm(doc=1)\u0026#34;details:[]-value:0.0description:\u0026#34;match on required clause, product of:\u0026#34;details:-value:0.0description:\u0026#34;# clause\u0026#34;details:[]-value:1.681987description:\u0026#34;_type:post, product of:\u0026#34;details:-value:1.0description:\u0026#34;boost\u0026#34;details:[]-value:1.681987description:\u0026#34;queryNorm\u0026#34;details:[]-_shard:3_node:\u0026#34;aVHxi_d2TYq40ISddYZF-A\u0026#34;_index:\u0026#34;blog\u0026#34;_type:\u0026#34;post\u0026#34;_id:\u0026#34;1\u0026#34;_score:0.13561106_source:title:\u0026#34;What is the best water temperature, Mr Water\u0026#34;_explanation:value:0.13561106description:\u0026#34;sum of:\u0026#34;details:-value:0.13561106description:\u0026#34;weight(title:water in 0) [PerFieldSimilarity], result of:\u0026#34;details:-value:0.13561106description:\u0026#34;fieldWeight in 0, product of:\u0026#34;details:-value:1.4142135description:\u0026#34;tf(freq=2.0), with freq of:\u0026#34;details:-value:2.0description:\u0026#34;termFreq=2.0\u0026#34;details:[]-value:0.30685282description:\u0026#34;idf(docFreq=1, maxDocs=1)\u0026#34;details:[]-value:0.3125description:\u0026#34;fieldNorm(doc=0)\u0026#34;details:[]-value:0.0description:\u0026#34;match on required clause, product of:\u0026#34;details:-value:0.0description:\u0026#34;# clause\u0026#34;details:[]-value:3.2588913description:\u0026#34;_type:post, product of:\u0026#34;details:-value:1.0description:\u0026#34;boost\u0026#34;details:[]-value:3.2588913description:\u0026#34;queryNorm\u0026#34;details:[]-_shard:1_node:\u0026#34;aVHxi_d2TYq40ISddYZF-A\u0026#34;_index:\u0026#34;blog\u0026#34;_type:\u0026#34;post\u0026#34;_id:\u0026#34;5\u0026#34;_score:0.11506981_source:title:\u0026#34;No water no food no air\u0026#34;_explanation:value:0.11506981description:\u0026#34;sum of:\u0026#34;details:-value:0.11506981description:\u0026#34;weight(title:water in 0) [PerFieldSimilarity], result of:\u0026#34;details:-value:0.11506981description:\u0026#34;fieldWeight in 0, product of:\u0026#34;details:-value:1.0description:\u0026#34;tf(freq=1.0), with freq of:\u0026#34;details:-value:1.0description:\u0026#34;termFreq=1.0\u0026#34;details:[]-value:0.30685282description:\u0026#34;idf(docFreq=1, maxDocs=1)\u0026#34;details:[]-value:0.375description:\u0026#34;fieldNorm(doc=0)\u0026#34;details:[]-value:0.0description:\u0026#34;match on required clause, product of:\u0026#34;details:-value:0.0description:\u0026#34;# clause\u0026#34;details:[]-value:3.2588913description:\u0026#34;_type:post, product of:\u0026#34;details:-value:1.0description:\u0026#34;boost\u0026#34;details:[]-value:3.2588913description:\u0026#34;queryNorm\u0026#34;details:[]-_shard:4_node:\u0026#34;aVHxi_d2TYq40ISddYZF-A\u0026#34;_index:\u0026#34;blog\u0026#34;_type:\u0026#34;post\u0026#34;_id:\u0026#34;3\u0026#34;_score:0.095891505_source:title:\u0026#34;Did Vitamin B6 alone work for you? Water?\u0026#34;_explanation:value:0.095891505description:\u0026#34;sum of:\u0026#34;details:-value:0.095891505description:\u0026#34;weight(title:water in 0) [PerFieldSimilarity], result of:\u0026#34;details:-value:0.095891505description:\u0026#34;fieldWeight in 0, product of:\u0026#34;details:-value:1.0description:\u0026#34;tf(freq=1.0), with freq of:\u0026#34;details:-value:1.0description:\u0026#34;termFreq=1.0\u0026#34;details:[]-value:0.30685282description:\u0026#34;idf(docFreq=1, maxDocs=1)\u0026#34;details:[]-value:0.3125description:\u0026#34;fieldNorm(doc=0)\u0026#34;details:[]-value:0.0description:\u0026#34;match on required clause, product of:\u0026#34;details:-value:0.0description:\u0026#34;# clause\u0026#34;details:[]-value:3.2588913description:\u0026#34;_type:post, product of:\u0026#34;details:-value:1.0description:\u0026#34;boost\u0026#34;details:[]-value:3.2588913description:\u0026#34;queryNorm\u0026#34;details:[]  ","permalink":"https://xguox.me/how-elasticsearch-scoring-document.html/","tags":["Elasticsearch"],"title":"Elasticsearch 的分数(_score)是怎么计算得出"},{"categories":["Ruby"],"contents":" 原 Google Docs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  2.3.1 056 \u0026gt; str = \u0026#34;foo\u0026#34; =\u0026gt; \u0026#34;foo\u0026#34; str.upcase =\u0026gt; \u0026#34;FOO\u0026#34; str =\u0026gt; \u0026#34;foo\u0026#34; str.upcase! =\u0026gt; \u0026#34;FOO\u0026#34; str =\u0026gt; \u0026#34;FOO\u0026#34; str.upcase! # 这返回值惊呆了 =\u0026gt; nil str =\u0026gt; \u0026#34;FOO\u0026#34;   Constant 1 2 3 4 5 6 7 8  FOO = 5 =\u0026gt; 5 2.3.1 002 \u0026gt; FOO = 7 (irb):2: warning: already initialized constant FOO (irb):1: warning: previous definition of FOO was here =\u0026gt; 7 FOO =\u0026gt; 7   即使 freeze 了\n1 2 3 4 5 6 7 8 9 10  BAR = 7 =\u0026gt; 7 BAR.freeze =\u0026gt; 7 BAR = 9 (irb):6: warning: already initialized constant BAR (irb):4: warning: previous definition of BAR was here =\u0026gt; 9 BAR =\u0026gt; 9   Comparator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  1 == 1.0 =\u0026gt; true 1.eql? 1.0 =\u0026gt; false a = \u0026#34;foo\u0026#34; =\u0026gt; \u0026#34;foo\u0026#34; b = \u0026#34;foo\u0026#34; =\u0026gt; \u0026#34;foo\u0026#34; a == b =\u0026gt; true a.eql? b =\u0026gt; true a.equal? b =\u0026gt; false a.equal? a =\u0026gt; true   == 比较的是值, eql? 比较的是值和类, equal? 比较的是对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  1 === 1 =\u0026gt; true Fixnum === 1 =\u0026gt; true 1 === Fixnum =\u0026gt; false Class === Class =\u0026gt; true Object === Object =\u0026gt; true Class === Object =\u0026gt; true Object === Class =\u0026gt; true Fixnum === Fixnum =\u0026gt; false (1..3) === 2 =\u0026gt; true 2 === (1..3) =\u0026gt; false  1 2 3 4 5 6 7 8  x = true \u0026amp;\u0026amp; false =\u0026gt; false x =\u0026gt; false x = true and false =\u0026gt; false x =\u0026gt; true   Whitespace 1 2 3 4 5 6 7 8 9 10 11 12 13  def method; 42; end =\u0026gt; :method num = 21 =\u0026gt; 21 method/num =\u0026gt; 2 method / num =\u0026gt; 2 method/ num =\u0026gt; 2 method /num # SyntaxError: # unterminated regexp  1 2 3 4 5 6 7 8 9 10 11 12 13  def one; 1; end =\u0026gt; :one one - 1 =\u0026gt; 0 one-1 =\u0026gt; 0 one- 1 =\u0026gt; 0 one -1 ArgumentError: wrong number of arguments (given 1, expected 0) from (irb):13:in `one\u0026#39; from (irb):17 from /Users/xguox/.rvm/rubies/ruby-2.3.1/bin/irb:11:in `\u0026lt;main\u0026gt;\u0026#39;   Class Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  class Parent @@value = 6 def self.value @@value end def self.inc_value @@value += 1 end end =\u0026gt; :inc_value class Child \u0026lt; Parent @@value = 42 end =\u0026gt; 42 Parent.value # 呵呵哒 =\u0026gt; 42 Parent.inc_value =\u0026gt; 43 Child.value =\u0026gt; 43  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  3.times do |loop_num| sum ||= 0 sum += 1 puts sum end 1 1 1 =\u0026gt; 3 for loop_num in 1..3 sum ||= 0 sum += 1 puts sum end 1 2 3 =\u0026gt; 1..3   Freeze Array 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  arr = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] =\u0026gt; [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] arr.freeze =\u0026gt; [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] arr \u0026lt;\u0026lt; \u0026#34;four\u0026#34; RuntimeError: can\u0026#39;t modify frozen Array from (irb):43 from /Users/xguox/.rvm/rubies/ruby-2.3.1/bin/irb:11:in `\u0026lt;main\u0026gt;\u0026#39; arr[0] = \u0026#34;eno\u0026#34; RuntimeError: can\u0026#39;t modify frozen Array from (irb):44 from /Users/xguox/.rvm/rubies/ruby-2.3.1/bin/irb:11:in `\u0026lt;main\u0026gt;\u0026#39; arr[0].object_id =\u0026gt; 70307108991340 arr[0].reverse! =\u0026gt; \u0026#34;eno\u0026#34; arr =\u0026gt; [\u0026#34;eno\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] arr =\u0026gt; [\u0026#34;eno\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] arr[0].object_id =\u0026gt; 70307108991340  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  arr = [1, 2, 3, 4] =\u0026gt; [1, 2, 3, 4] arr.freeze =\u0026gt; [1, 2, 3, 4] arr \u0026lt;\u0026lt; 5 RuntimeError: can\u0026#39;t modify frozen Array from (irb):52 from /Users/xguox/.rvm/rubies/ruby-2.3.1/bin/irb:11:in `\u0026lt;main\u0026gt;\u0026#39; arr[0] += 2 RuntimeError: can\u0026#39;t modify frozen Array from (irb):53 from /Users/xguox/.rvm/rubies/ruby-2.3.1/bin/irb:11:in `\u0026lt;main\u0026gt;\u0026#39; 1.object_id =\u0026gt; 3 3.object_id =\u0026gt; 7  ","permalink":"https://xguox.me/ruby-old-driver-may-not-gotcha.html/","tags":["Ruby"],"title":"Ruby 老司机也未必 Gotcha "},{"categories":["Ruby"],"contents":"1 2 3 4 5  User.new.method(:order_commissions).source_location # =\u0026gt; [\u0026#34;....../vendor/bundle/gems/activerecord-4.2.6/lib/active_record/associations/builder/association.rb\u0026#34;, 114] User.method(:generate_token).source_location # [\u0026#34;....../app/models/user.rb\u0026#34;, 50]   这真的没有语法错误 = . =\n1 2 3 4  v1, = [[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;] v2, = v1 [33] pry(main)\u0026gt; v2 =\u0026gt; \u0026#34;a\u0026#34;  ","permalink":"https://xguox.me/ruby-method-source_location-and-comma.html/","tags":["Ruby"],"title":"语法补习: Ruby 方法 source_location 和奇葩的逗号(,)"},{"categories":[],"contents":"真是矫情啊, 发个告别邮件都琢磨老久, 结果也就那么几个字而已.\n遗憾的是还差那么两个月多一些就呆够两年了. 朝十上班, 晚五下班, 中休一小时, 周末双休的公司, 也是很任性的, 然而, 还是选择了离开. 多谢给我机会来到这.\n尤其多谢 Hooopo, 跟着炮哥是涨芝士的.\n","permalink":"https://xguox.me/farewell.html/","tags":[],"title":"Farewell"},{"categories":["Elasticsearch"],"contents":" Vagrant 在 Ruby/Rails 社区老早之前就听的挺多的, 但今天才开始尝试折腾.\n当前下载的版本是 1.8.5, 虚拟机用的是 VirtualBox 5.1.\n一开始尝试用 \u0026lsquo;hashicorp/precise32\u0026rsquo; 这个 box, 但是, 在设置 private_network 的时候貌似有 bug,\n1 2 3 4 5  Vagrant attempted to execute the capability \u0026#39;configure_networks\u0026#39; on the detect guest OS \u0026#39;linux\u0026#39;, but the guest doesn\u0026#39;t support that capability. This capability is required for your configuration of Vagrant. Please either reconfigure Vagrant to avoid this capability or fix the issue by creating the capability.   只好换一个 box, 选的是 \u0026lsquo;ubuntu/xenial64\u0026rsquo;, 但, vagrant up 装了半天, 那速度简直了. 最后只好自己单独去下载 box,\nhttps://atlas.hashicorp.com/ubuntu/boxes/xenial64/versions/20170331.0.0/providers/virtualbox.box\n自己在家开下载软件, 几 M 每秒杠杠的, 要是靠慢慢 up 的话我就可以洗洗睡了.\n1 2  vagrant box add xenial64 ~/Downloads/virtualbox.box vagrant init xenial64   完事以后, 如果只是试玩一下 Vagrant 的话可以直接 vagrant up.\n这里因为准备要建三个虚拟机.\n所以, 改了一下 Vagrantfile, 前面就是因为设置 es.vm.network :private_network, ip: '192.168.10.111' 所以换了这个 box.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  Vagrant.configure(\u0026#39;2\u0026#39;) do |config| config.vm.define \u0026#39;es1\u0026#39; do |es| es.vm.box = \u0026#39;xenial64\u0026#39; es.vm.network :private_network, ip: \u0026#39;192.168.10.111\u0026#39; es.vm.provider :virtualbox do |v| v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--natdnshostresolver1\u0026#39;, \u0026#39;on\u0026#39;] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--memory\u0026#39;, 1024] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--name\u0026#39;, \u0026#39;es1\u0026#39;] end end config.vm.define \u0026#39;es2\u0026#39; do |es| es.vm.box = \u0026#39;xenial64\u0026#39; es.vm.network :private_network, ip: \u0026#39;192.168.10.112\u0026#39; es.vm.provider :virtualbox do |v| v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--natdnshostresolver1\u0026#39;, \u0026#39;on\u0026#39;] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--memory\u0026#39;, 1024] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--name\u0026#39;, \u0026#39;es2\u0026#39;] end end config.vm.define \u0026#39;es3\u0026#39; do |es3| es3.vm.box = \u0026#39;xenial64\u0026#39; es3.vm.network :private_network, ip: \u0026#39;192.168.10.113\u0026#39; es3.vm.provider :virtualbox do |v| v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--natdnshostresolver1\u0026#39;, \u0026#39;on\u0026#39;] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--memory\u0026#39;, 1024] v.customize [\u0026#39;modifyvm\u0026#39;, :id, \u0026#39;--name\u0026#39;, \u0026#39;es32\u0026#39;] end end end  1  vagrant up   这里三台虚拟机分别是, \u0026lsquo;es1\u0026rsquo;, \u0026lsquo;es2\u0026rsquo;, \u0026lsquo;es3\u0026rsquo;, 每个虚拟机分的内存是 1G. 后面要在虚拟机上安装的流程理论上是可以写进一个 shell 脚本, 让 Vagrant 在 up 的时候执行的, 不过我还是自己一台一台跑了. 主要是, 网络问题, 安装 Java 的时候特别蛋疼吧. up 了以后, vagrant ssh es1(2,3) 分别执行下面这堆. (嫌麻烦, 网络好的可以写成 shell 脚本, 不用一直重复)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  sudo add-apt-repository ppa:webupd8team/java sudo apt-get update # 这里一会快一会慢. 也花了不少时间, 网络问题 sudo apt-get install oracle-java8-installer # java -version # java version \u0026#34;1.8.0_101\u0026#34; # Java(TM) SE Runtime Environment (build 1.8.0_101-b13) # Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode) wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.3.5.deb sudo dpkg -i elasticsearch-2.3.5.deb sudo update-rc.d elasticsearch defaults 95 10   然之后是配置文件, 一开始时候, 三个虚拟机分别是三个 nodes, 一个 client node, 一个 master node, 一个 data node, 后来索性把自己的 host 机也搞上了. 我自己本机的 ip 用的是 192.168.10.1\n先把自己这个 host机旧的 elasticsearch.yml 备份, 再开始配置集群.\n1  cp elasticsearch.yml elasticsearch.yml.bak  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  ################################### Cluster ###################################cluster.name:my-application#################################### Node #####################################node.name:es-clientnode.client:truenode.data:false# You can exploit these settings to design advanced cluster topologies.## 1. You want this node to never become a master node, only to hold data.# This will be the \u0026#34;workhorse\u0026#34; of your cluster.##node.master: false#node.data: true## 2. You want this node to only serve as a master: to not store any data and# to have free resources. This will be the \u0026#34;coordinator\u0026#34; of your cluster.##node.master: true#node.data: false## 3. You want this node to be neither master nor data node, but# to act as a \u0026#34;search load balancer\u0026#34; (fetching data from nodes,# aggregating results, etc.)##node.master: false#node.data: false#################################### Paths ####################################path.data:/usr/local/var/elasticsearch/path.logs:/usr/local/var/log/elasticsearch/path.plugins:/usr/local/var/lib/elasticsearch/plugins############################## Network And HTTP ###############################network.host:[\u0026#34;192.168.10.1\u0026#34;,\u0026#34;_local_\u0026#34;]################################## Discovery ##################################discovery.zen.ping.multicast.enabled:falsediscovery.zen.ping.unicast.hosts:[\u0026#34;192.168.10.1\u0026#34;,\u0026#34;192.168.10.111\u0026#34;,\u0026#34;192.168.10.112\u0026#34;,\u0026#34;192.168.10.113\u0026#34;]   host 机作为 client node, 接下来的话是 master node 和 data node,\n By default a node is both a master-eligible node and a data node. This is very convenient for small clusters but, as the cluster grows, it becomes important to consider separating dedicated master-eligible nodes from dedicated data nodes.\n \u0026lsquo;es1\u0026rsquo; 的 elasticsearch.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ################################### Cluster ###################################cluster.name:my-application#################################### Node #####################################node.name:es-mster#node.master: truenode.data:true############################## Network And HTTP ###############################network.host:[\u0026#34;192.168.10.111\u0026#34;]################################## Discovery ##################################discovery.zen.ping.multicast.enabled:falsediscovery.zen.ping.unicast.hosts:[\u0026#34;192.168.10.1\u0026#34;,\u0026#34;192.168.10.111\u0026#34;,\u0026#34;192.168.10.112\u0026#34;,\u0026#34;192.168.10.113\u0026#34;]   \u0026lsquo;es2\u0026rsquo; 的 elasticsearch.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ################################### Cluster ###################################cluster.name:my-application#################################### Node #####################################node.name:es-data-1node.data:true############################## Network And HTTP ###############################network.host:[\u0026#34;192.168.10.112\u0026#34;]################################## Discovery ##################################discovery.zen.ping.multicast.enabled:falsediscovery.zen.ping.unicast.hosts:[\u0026#34;192.168.10.1\u0026#34;,\u0026#34;192.168.10.111\u0026#34;,\u0026#34;192.168.10.112\u0026#34;,\u0026#34;192.168.10.113\u0026#34;]   \u0026lsquo;es3\u0026rsquo; 的 elasticsearch.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ################################### Cluster ###################################cluster.name:my-application#################################### Node #####################################node.name:es-data-2node.data:true############################## Network And HTTP ###############################network.host:[\u0026#34;192.168.10.113\u0026#34;]################################## Discovery ##################################discovery.zen.ping.multicast.enabled:falsediscovery.zen.ping.unicast.hosts:[\u0026#34;192.168.10.1\u0026#34;,\u0026#34;192.168.10.111\u0026#34;,\u0026#34;192.168.10.112\u0026#34;,\u0026#34;192.168.10.113\u0026#34;]   全部 Elasticsearch 重启, 因为自己的 host 机原本就装了 kopf 插件, 所以, 直接浏览器打开.\n实心星星是 mater node, 空心星星是 Master-eligible node.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\nMaster-eligible node\n A node that has node.master set to true (default), which makes it eligible to be elected as the master node, which controls the cluster.\n Data node\n A node that has node.data set to true (default). Data nodes hold data and perform data related operations such as CRUD, search, and aggregations.\n Client node\n A client node has both node.master and node.data set to false. It can neither hold data nor become the master node. It behaves as a “smart router” and is used to forward cluster-level requests to the master node and data-related requests (such as search) to the appropriate data nodes.\n 随意找点东西索引,\n0, 1, 2, 3, 4 那些是分片, 其中, 亮的是 Primary Shards, 暗的是 Replica Shards. 默认的每个 index 都是 5 个 Primary Shards, 对应的 5 个 Replica Shards. 得益于 Replica Shards 任意一台机器挂了, 数据也还是完整的.\nBTW, 除了监控插件以外, 比如一些分词插件, 得每个 node 也即是每台机器都要安装一遍.\n插曲 一开始设置的集群只有三个虚拟机, 分别是 client node, master node(node.data: false), data node. 添加索引以后集群状态一直是黄的. 一堆的 unassigned_shards 导致的 \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;. 因为只有一个 data node, 只能用来存 Primary Shards, 没地方存 Replica Shards. 最简单地, 把 master node 也作为 data node 用就绿了. 虽然官方说 master 和 data 最好分开.\n关于 Elasticsearch 的集群 当只有单台机器, 也就是我们一般在本地跑的时候, 通常只有是一个集群下面仅有一个 node. master node data node 都是这个 node, 集群状态一直就是黄的, 因为, 默认的配置是, 每个索引的每个 shard 都有一个 replica.\n1 2 3 4 5 6 7 8  green All primary and replica shards are active. yellow All primary shards are active, but not all replica shards are active. red Not all primary shards are active.   单台机器跑的时候, 单个 node 也就只能存 Primary Shards, 没地方存 Replica Shards, 所以就出现了 Unassigned 的 Shards, 也就黄了. 要想变回绿的话, 配置 index.number_of_replicas 为 0 就好了.\n1 2 3  # Set the number of replicas (additional copies) of an index (1 by default):#index.number_of_replicas:0   容错 假设设定了,\n1 2  \u0026#34;number_of_shards\u0026#34; : 3, \u0026#34;number_of_replicas\u0026#34; : 2   集群如图:\n如果任意一台 down 了.\n这里 down 的是 master node, 索引在丢失主分片时不能正常工作. 此时检查集群 health，看到状态会是 red, 但 Elasticsearch 很快就会从剩下的 master-eligible nodes 里面选出一个新的 master node, 然后集群状态变黄.\n这里因为指定了每个主分片要有两个副本分片, 但是, 因为现在每个主分片只剩下一个副本分片了, 所以, 还是不能绿. 不过, 假设这个时候剩余这两个 node 又不幸 down 掉某个的话, 我们的集群状态还是黄的, 并且可以正常跑起来, 没有任何数据丢失. 等到重启 Node 1 以后, 如果 Node 1 的数据并没有在 down 掉之后丢失旧数据，就可以尝试再利用起来，并只会从主分片上复制在故障期间有数据变更的那一部分数据.\n","permalink":"https://xguox.me/elasticsearch-build-development-cluster-with-vagrant.html/","tags":["Elasticsearch"],"title":"本地用 Vagrant 搭建 Elasticsearch 集群"},{"categories":["Ruby"],"contents":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  class Team \u0026lt; ApplicationRecord has_many :projects end class Project \u0026lt; ApplicationRecord belongs_to :team end [1] pry(main)\u0026gt; Project.create(name: \u0026#39;project_1\u0026#39;) (0.2ms) BEGIN (0.2ms) ROLLBACK =\u0026gt; #\u0026lt;Project:0x007fb8462ddf10 id: nil, team_id: nil, name: \u0026#34;project_1\u0026#34;, created_at: nil, updated_at: nil\u0026gt; [2] pry(main)\u0026gt; _.errors =\u0026gt; #\u0026lt;ActiveModel::Errors:0x007fb84634c500 @base=#\u0026lt;Project:0x007fb8462ddf10 id: nil, team_id: nil, name: \u0026#34;project_1\u0026#34;, created_at: nil, updated_at: nil\u0026gt;, @details={:team=\u0026gt;[{:error=\u0026gt;:blank}]}, @messages={:team=\u0026gt;[\u0026#34;must exist\u0026#34;]}\u0026gt;   在 Rails 5 以前是可以成功 Save 的, 但是, 从 Rails 5 开始, belongs_to 的关联都默认必须有值, 除非这么写:\n1 2 3 4 5 6 7 8 9  class Project \u0026lt; ApplicationRecord belongs_to :team, optional: true end [3] pry(main)\u0026gt; Project.create(name: \u0026#39;project_1\u0026#39;) (0.2ms) BEGIN SQL (5.4ms) INSERT INTO \u0026#34;projects\u0026#34; (\u0026#34;name\u0026#34;, \u0026#34;created_at\u0026#34;, \u0026#34;updated_at\u0026#34;) VALUES ($1, $2, $3) RETURNING \u0026#34;id\u0026#34; [[\u0026#34;name\u0026#34;, \u0026#34;project_1\u0026#34;], [\u0026#34;created_at\u0026#34;, 2016-09-28 04:05:41 UTC], [\u0026#34;updated_at\u0026#34;, 2016-09-28 04:05:41 UTC]] (1.0ms) COMMIT =\u0026gt; #\u0026lt;Project:0x007fff3b2bc388 id: \u0026#34;05051810-fea1-4ae9-b138-05a92e0bb3e9\u0026#34;, team_id: nil, name: \u0026#34;project_1\u0026#34;, created_at: Wed, 28 Sep 2016 04:05:41 UTC +00:00, updated_at: Wed, 28 Sep 2016 04:05:41 UTC +00:00\u0026gt;   要使用像原本那样默认可选的话, 在 config/initializers/new_framework_defaults.rb 改一下\n1  Rails.application.config.active_record.belongs_to_required_by_default = false  ","permalink":"https://xguox.me/rails-5-belongs_to-association-required-by-default.html/","tags":["Ruby"],"title":"Rails 5 belongs_to 默认 required"},{"categories":["Elasticsearch"],"contents":"            添加新的 fields 可以不需要重新索引 reindex 的背后 真正的 0 downtime         最近使用 searchkick 这个 Gem 才发现, 之前用的 elasticsearch-rails 要更灵(jian)活(lou)的多.\n 为了让数据可以被查询到, Elasticsearch 需要知道每个 field 存着的是什么样的数据, 以及这些数据是如何被索引的. 而我们也只能从 Elasticsearch 查找到已经存储了的索引.\n然而, 每次更新一下某个 index, 某个 type 的 mapping 时候, 都得 reindex, 数据量不大, 文档较少的情况还可以接受, 一个操作瞬间搞定. 但是, 上到一定数量级时候, 或者 mapping 本身比较复杂的话, 每次要 reindex 可就不是几十秒, 三五分钟的事情了. 服务器总不能那么傻傻地瞎等着直到索引重新跑完, 而且是, 每一次, 每一个小改动都得这样.\n比如, 仅仅想把某个 field 从 string 改成 date 类型, 那么, 所有已经存入的数据就会变得毫无意义. 无论如何, 最后我们还是得重新索引(reindex).\n其实, 并不是只是 Elasticsearch 是这样, 所有的数据库索引都一样的道理.\nElasticsearch (and Lucene) 把所有的 indices 存在不可变的 segments 中(每个 segment 就是一个迷你的倒排索引), 也就是说这些 segments 永远不会被更新. 所谓更新某个文档, 其实只是新创建一个文档然后把旧的那个文档删掉而已. 越多的文档被更新或者创建, 也就会有越多的新的 segments 被创建. 后台则在跑着一个合并进程把几个小的 segments 合成一个大的直到所有旧的 segments 被删除.\n在 Elasticsearch 中, 一个 index 可以包含有多个不同的 types. 每一个 _type 都有着自己的 mapping. 而每个 segment 中则可能包含有属于不同的 types 的文档, 这样的话, 即使仅仅是更改某个 type 的某个 field, 那么, 这个 index 的所有文档都得重新索引.\n= . = 所以, 提倡一个 index 一个 type\n官方给出了几种方式可以让在不宕机情况下完成 Mapping 的修改.\n添加新的 fields 可以不需要重新索引 因为 segment 只是包含着该 segment 已存有的文档数据, 这也就意味着, 只要用 put_mapping API 无需 reindex 即可添加新的 fields, 包括 multi-field. (不过只生效于新的文档, 旧有文档对应的 fields 还是为空值)\nreindex 的背后 数据重新索引的过程其实很简单. 首先, 新建一个新的 index(新的 mapping 和 settings):\n1 2 3 4 5 6 7 8  curl -XPUT localhost:9200/new_index -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;my_type\u0026#34;: { ... new mapping definition ...} } } \u0026#39;    使用 scrolled search(search_type=scan) 从旧的 index 中获取所有的文档, 然后再使用 bulk API添加索引到新的 index, 完成以后删除旧的 index.\n很多集成插件通常都会提供一个 reindex() 的方法来完成所有的这些事情.\n那么问题来了, 新的 index 和旧的 index 名字是不一样的啊, 那程序也要跟着改变才行了.\n真正的 0 downtime 使用索引别名(Index aliases)让我们可以更灵活地在后台完成 reindex 的工作, 所有的一切我们的程序都可以不用操心.\nAlias 就像 *nix 的软链接一样可以指向任意真实存在的 indices.\n所以, 其实真正的工作流应该是酱紫的. 首先, 创建一个新的 index, 命名结尾加上版本号或者时间戳:\n1 2 3  curl -XPUT localhost:9200/my_index_v1 -d \u0026#39; { ... mappings ... } \u0026#39;   创建一个别名指向这个 index:\n1 2 3 4 5 6 7 8 9 10  curl -XPOST localhost:9200/_aliases -d \u0026#39; { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;my_index_v1\u0026#34; }} ] } \u0026#39;   此时, my_index 就可以作为 my_index_v1 的一个别称来与程序进行交互.\n然后, 如果需要 reindex, 则可以再新建一个 index, 同样的, 在索引名的最后加上一个新的版本号, 或者时间戳:\n1 2 3  curl -XPUT localhost:9200/my_index_v2 -d \u0026#39;{ ... mappings ... }\u0026#39;   接着, 把 my_index_v1 的文档 reindex 到 my_index_v2, 完成以后把 my_index 这个别名指向新的 index(比如这里的my_index_v2):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  curl -XPOST localhost:9200/_aliases -d \u0026#39; { \u0026#34;actions\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;my_index_v1\u0026#34; }}, { \u0026#34;add\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;my_index_v2\u0026#34; }} ] } \u0026#39;   最后, 删除旧的 index,\n1  curl -XDELETE localhost:9200/my_index_v1   从 head 监控中可以看到, 旧的正常服役同时, 新的索引也正在进行中\n相关阅读:\nChanging Mapping with Zero Downtime\n","permalink":"https://xguox.me/elasticsearch-changing-mapping-with-zero-downtime.html/","tags":["Elasticsearch"],"title":"Elasticsearch 如何不用停机情况下完成 mapping 的修改"},{"categories":["Elasticsearch"],"contents":"当一个 document 被索引时, 通常是对应每个 field 都生成一个倒排索引(Inverted Index)用于作为存储的数据结构, 关于倒排索引, 推荐炮哥之前写的一篇文章可以结合参考理解. 每个 field 的倒排索引是由「对应」于这个 field 的那些词语(term)所组成. 从而, 搜索的时候, 就可查到某个 document 是否含有(或者说命中)某些 terms, 进而返回所有命中查找词的 documents.\n这里强调的「对应」其实就是 Analyzers 在支持着.\nElasticsearch 的官方文档:\n Analyzers are composed of a single Tokenizer and zero or more TokenFilters. The tokenizer may be preceded by one or more CharFilters.\n Analyzers 是由一个 Tokenizer 和任意个数的 TokenFilters 组成. 而在把需要处理的字符串传给 Tokenizer 之前需要经过一个或多个的 CharFilters(Character filters) 做预处理.\nElasticsearch(2.3) 默认给了我们八种 Analyzers(standard, simple, whitespace, stop, keyword, pattern, language, snowball) 开箱即用. 此外, 还提供给了我们 3 种 CharFilters, 12 种 Tokenizer, 以及一大堆 TokenFilters 用于自定义 Analyzers.\n说半天, 都是很抽象的东西. 下面用一些栗子说明.\n新建索引, 并设置好 mappings\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  curl -XPUT \u0026#39;http://localhost:9200/blog/\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;post\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;yes\u0026#34; } } } } }\u0026#39;    存入数据如下,\n1 2 3 4 5  curl -XPUT \u0026#39;http://localhost:9200/blog/post/1?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;the Good Cats \u0026amp; the Good Dogs!\u0026#34; } \u0026#39;   使用 termvector 来查看 title 这个 field 的数据是如何存储的(倒排索引)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  curl -XGET \u0026#39;http://localhost:9200/blog/post/1/_termvector?fields=title\u0026amp;pretty=true\u0026#39; { \u0026#34;_index\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;took\u0026#34; : 1, \u0026#34;term_vectors\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;field_statistics\u0026#34; : { \u0026#34;sum_doc_freq\u0026#34; : 4, \u0026#34;doc_count\u0026#34; : 1, \u0026#34;sum_ttf\u0026#34; : 6 }, \u0026#34;terms\u0026#34; : { \u0026#34;cats\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;dogs\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;good\u0026#34; : { \u0026#34;term_freq\u0026#34; : 2 }, \u0026#34;the\u0026#34; : { \u0026#34;term_freq\u0026#34; : 2 } } } } }    因为在定义 mappings 的时候, title 用的 analyzer 是 standard, 官方给的 standard 是由以下几个部分组成的\n An analyzer of type standard is built using the Standard Tokenizer with the Standard Token Filter, Lower Case Token Filter, and Stop Token Filter.\n Standard Tokenizer 是基于语法(英语)做分词的, 并且把标点符号比如上面的 \u0026amp; 和 ! 去掉, Lower Case Token Filter 则是把所有的单词都统一成小写, 除此以外, standard analyzer 还用到了 Stop Token Filter, 没说设置了哪些停词, 所以, 这里看不到其作用, 不过可以肯定的是, 单词 the 不是停词, 其实, 我们可以设置单词 the 为 stopwords, 因为其实对于索引而言, 这个词的并没有什么意义, 可以去掉. 最后, term_freq 表示每个词出现的次数.\n基于如上的结果来做一些搜索, 先试试这个:\n1 2 3 4 5 6 7 8 9  curl -XGET \u0026#39;http://localhost:9200/blog/post/_search?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;dog\u0026#34; } } }\u0026#39;    输出的结果, 居然是\u0026hellip;\u0026hellip;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  { \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 5, \u0026#34;successful\u0026#34; : 5, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 0, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] } }    什么鬼, 居然搜不到??? 其实, 仔细看就会发现其实也不奇怪, 我们搜索的关键词是 dog, 但是 title 的倒排索引索引中的词只有 [cats, dogs, good, the] 这几个词, 注意是 dogs 不是我们要搜索的 dog\n好吧, 瞬间觉得好不智能是伐 = . =\n下面换一个 snowball analyzer 再试试看, 不过要先删掉当前索引, 再重新跑一次(mappings 每次更改都要修改 reindex 才能生效)\n1  curl -XDELETE \u0026#39;http://localhost:9200/blog\u0026#39;  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  curl -XPUT \u0026#39;http://localhost:9200/blog/\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { \u0026#34;post\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;snowball\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;yes\u0026#34; } } } } }\u0026#39;    还是存入同样的数据,\n1 2 3 4 5 6  curl -XPUT \u0026#39;http://localhost:9200/blog/post/1?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;the Good Cats \u0026amp; the Good Dogs!\u0026#34; } \u0026#39;    再次, 使用 termvector 来查看 title 这个 field 的数据是如何存储的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  curl -XGET \u0026#39;http://localhost:9200/blog/post/1/_termvector?fields=title\u0026amp;pretty=true\u0026#39; { \u0026#34;_index\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;took\u0026#34; : 1, \u0026#34;term_vectors\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;field_statistics\u0026#34; : { \u0026#34;sum_doc_freq\u0026#34; : 3, \u0026#34;doc_count\u0026#34; : 1, \u0026#34;sum_ttf\u0026#34; : 4 }, \u0026#34;terms\u0026#34; : { \u0026#34;cat\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;dog\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;good\u0026#34; : { \u0026#34;term_freq\u0026#34; : 2 } } } } }    可以看到, 这一次, 存入的词语只有 [cat, dog, good], 官方描述 Snowball Analyzer\n An analyzer of type snowball that uses the standard tokenizer, with standard filter, lowercase filter, stop filter, and snowball filter.\n 大部分跟 standard analyzer 差不多, 从结果看出区别, snowball 把 the 设置为停词了, 然后还多了个 snowball filter. 没有详细深挖这个 filter, 不过大意应该是提取词干, 比如 computing 和 computed 的词干 comput, 或者上面的 dogs, cats 变为 dog, cat.\n现在再去搜索 dog, 肯定是可以命中的.\n那么问题来了, 如果我搜索的是 dogs 呢? 会不会不中? 答案是, 能够命中的, 因为这里搜索 dogs, 会先把搜索的词同样的处理为 dog 再去匹配.\n1 2 3 4 5 6 7 8 9  curl -XGET \u0026#39;http://localhost:9200/blog/post/_search?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;dogs\u0026#34; } } }\u0026#39;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  { \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 5, \u0026#34;successful\u0026#34; : 5, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;max_score\u0026#34; : 0.2169777, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 0.2169777, \u0026#34;_source\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;the Good Cats \u0026amp; the Good Dogs!\u0026#34; } } ] } }    以上, 大概说了一些默认的 Analyzers, 接下来, 看下怎么自定义 Analyzers 并运用到 mappings 中.\n官方给出了一个自定义 analyzer 语法例子如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  index:analysis:analyzer:myAnalyzer2:type:customtokenizer:myTokenizer1filter:[myTokenFilter1,myTokenFilter2]char_filter:[my_html]position_increment_gap:256tokenizer:myTokenizer1:type:standardmax_token_length:900filter:myTokenFilter1:type:stopstopwords:[stop1,stop2,stop3,stop4]myTokenFilter2:type:lengthmin:0max:2000char_filter:my_html:type:html_stripescaped_tags:[xxx,yyy]read_ahead:1024   接下来, 试下写一个自己的.\n上面的第一个栗子中, 可以看到, 当存入的是 dogs, 搜索 dog 都不能命中, 更不用说搜索 do go 什么的.\n很多时候我们一些电商网站, 输入一两个字符, 就会给出一些提示选项给我们, 如图:\n这种 Autocomplete 的功能需要把字符串分的粒度很细, 这时候我们就可以用到 Ngrams for Partial Matching\n写一个用于 Autocomplete 的 Analyzer.\n1  curl -XDELETE \u0026#39;http://localhost:9200/blog\u0026#39;  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  curl -XPUT \u0026#39;http://localhost:9200/blog/\u0026#39; -d \u0026#39; { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;autocomplete_analyzer\u0026#34;: { \u0026#34;type\u0026#34; : \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34; : [\u0026#34;replace_ampersands\u0026#34;], \u0026#34;tokenizer\u0026#34; : \u0026#34;my_edgeNGram_tokenizer\u0026#34;, \u0026#34;filter\u0026#34; : [\u0026#34;lowercase\u0026#34;, \u0026#34;my_stop\u0026#34;] } }, \u0026#34;char_filter\u0026#34; : { \u0026#34;replace_ampersands\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34; : [\u0026#34;\u0026amp;=\u0026gt;and\u0026#34;] } }, \u0026#34;tokenizer\u0026#34; : { \u0026#34;my_edgeNGram_tokenizer\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;edgeNGram\u0026#34;, \u0026#34;min_gram\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;max_gram\u0026#34; : \u0026#34;8\u0026#34;, \u0026#34;token_chars\u0026#34;: [ \u0026#34;letter\u0026#34;, \u0026#34;digit\u0026#34; ] } }, \u0026#34;filter\u0026#34; : { \u0026#34;my_stop\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34; : [\u0026#34;the\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;post\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;autocomplete_analyzer\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;yes\u0026#34; } } } } } \u0026#39;    其中, my_stop 这个 filter 和 replace_ampersands 这个 char_filter 其实可以不用, 只是拿来示范一下, my_stop 是把单词 the 去掉, 这里细分以后, 就会很奇怪的有 th, 但是没有 the, 因为 the 被停词去掉了, 好吧, 在实际中不会这么去用 nGram,\nreplace_ampersands 这个 char_filter 是在分词前预处理字符串, 把 \u0026amp; 变成 and.\n值得注意的是, 这里用的是 edgeNGram, 而不是 nGram, 从名字可以猜出来, 结果看后面\n存入数据,\n1 2 3 4 5 6  curl -XPUT \u0026#39;http://localhost:9200/blog/post/1?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;the Cats \u0026amp; the Dogs!\u0026#34; } \u0026#39;    Again, 使用 termvector 来查看此时 title 是如何存储的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  curl -XGET \u0026#39;http://localhost:9200/blog/post/1/_termvector?fields=title\u0026amp;pretty=true\u0026#39; { \u0026#34;_index\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;took\u0026#34; : 1, \u0026#34;term_vectors\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;field_statistics\u0026#34; : { \u0026#34;sum_doc_freq\u0026#34; : 9, \u0026#34;doc_count\u0026#34; : 1, \u0026#34;sum_ttf\u0026#34; : 10 }, \u0026#34;terms\u0026#34; : { \u0026#34;an\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;and\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;ca\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;cat\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;cats\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;do\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;dog\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;dogs\u0026#34; : { \u0026#34;term_freq\u0026#34; : 1 }, \u0026#34;th\u0026#34; : { \u0026#34;term_freq\u0026#34; : 2 } } } } }    倒排索引的粒度变细了, 这样, 在输入前面两个字符 th 就可以命中匹配. edgeNGram 是仅从词头开始分词, 这里如果用的是 nGram 的话, 那么粒度会更细, 会多出一些在 edgeNGram 中没的, 比如, ats, ogs 等等. min_gram 和 max_gram 都还比较好理解, token_chars 是指分割点, 比如这里加了 letter 和 digit, 因为空格不属于这两样, 那么空格就会被当成分割点, 倒排索引里也就看不到有空格. 如果要保留空格, 也就是留下 the c 和 the ca \u0026hellip;这些分词的话, 就得再加上 whitespace 了.\n1 2 3 4 5 6 7 8 9  curl -XGET \u0026#39;http://localhost:9200/blog/post/_search?pretty=true\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;th\u0026#34; } } }\u0026#39;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  { \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 5, \u0026#34;successful\u0026#34; : 5, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;max_score\u0026#34; : 0.13561106, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 0.13561106, \u0026#34;_source\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;the Cats \u0026amp; the Dogs!\u0026#34; } } ] } }    最后, 如果只是想精确的存储值而不被分析的话, 可以用 \u0026ldquo;index\u0026rdquo;: \u0026ldquo;not_analyzed\u0026rdquo;\n比如 United Kingdom 就是 United Kingdom, 不想被拆成 United, Kingdom什么的, 这种时候就可以用上了, not_analyzed 的效率会高一些.\nlong, double, date 这些类型的数据是永远不会被分析的. 要么是 \u0026quot;index\u0026quot; : \u0026quot;no\u0026quot;, 或者 \u0026quot;index\u0026quot; : \u0026quot;not_analyzed\u0026quot;\nLast but not least, 如果想知道具体怎么计算出来的匹配得分, 还可以看看 Explain API\n相关阅读: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html\nHow search and index works (Ruby 语言描述)\nAn Introduction to Ngrams in Elasticsearch\n","permalink":"https://xguox.me/elasticsearch-custom-analyzer.html/","tags":["Elasticsearch"],"title":"Elasticsearch analysis \u0026 自定义 analyzers"},{"categories":["Ruby"],"contents":" autoload_paths 假设 Rails 项目根目录下有如下目录以及 .rb 文件,\n(root/)extras/foo.rb\n如果啥也不干, 直接打开 rails console:\n1 2 3 4 5 6  [1] pry(main)\u0026gt; defined?(Foo) =\u0026gt; nil [2] pry(main)\u0026gt; Foo NameError: uninitialized constant Foo from (pry):2:in `\u0026lt;main\u0026gt;\u0026#39;   在 application.rb 中加入一行:\nconfig.autoload_paths += %W(#{config.root}/extras)\n再重启 rails console:\n1 2 3 4 5 6 7 8  [1] pry(main)\u0026gt; defined?(Foo) =\u0026gt; nil [2] pry(main)\u0026gt; Foo =\u0026gt; Foo [3] pry(main)\u0026gt; defined?(Foo) =\u0026gt; \u0026#34;constant\u0026#34;   这是 autoload_paths 延迟加载的作用, 只有当用到的时候再去查找这个常量. 而开发环境 app 目录下的所有文件都是这样延迟加载的.\n但是, 在生产环境中:\n1 2 3 4 5 6 7  # app/models/segment.rb 已经预加载 [1] pry(main)\u0026gt; defined?(Segment) =\u0026gt; \u0026#34;constant\u0026#34; # foo.rb 还是延迟加载 [2] pry(main)\u0026gt; defined?(Foo) =\u0026gt; nil   eager_load_paths 这个时候需要的是 eager_load_paths\n在 application.rb 中加上\nconfig.eager_load_paths += %W( #{config.root}/extras )\n1 2 3  # extras/foo.rb 已经预加载 [1] pry(main)\u0026gt; defined?(Foo) =\u0026gt; \u0026#34;constant\u0026#34;  ","permalink":"https://xguox.me/eager_load_paths.html/","tags":["Ruby"],"title":"Rails autoload_paths \u0026 eager_load_paths"},{"categories":["Ruby"],"contents":"Ruby 可以随时打开一个类进行 Monkey Patch, 但是这是一个比较危险的动作, 很容易引发一些意外, 而 2.0 开始, 加入的 refinements 则是为了让这一动作变得相对安全一些.\n举个只为理解毫无实际意义的栗子:\n1 2 3 4 5 6 7 8 9 10 11 12  class String def simple_camelize split(/_/).map(\u0026amp;:capitalize).join end end module Whatever CONST_A = \u0026#34;active_record\u0026#34;.simple_camelize end Whatever::CONST_A =\u0026gt; \u0026#34;ActiveRecord\u0026#34;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  class String def simple_camelize \u0026#34;just simple_camelize\u0026#34; end end module StringRefinements refine String do def simple_camelize split(/_/).map(\u0026amp;:capitalize).join end end end module Whatever using StringRefinements CONST_A = \u0026#34;active_record\u0026#34;.simple_camelize end Whatever::CONST_A =\u0026gt; \u0026#34;ActiveRecord\u0026#34; \u0026#34;active_support\u0026#34;.simple_camelize =\u0026gt; \u0026#34;just simple_camelize\u0026#34; # 咳~~咳, 再次打开 Whatever module Whatever CONST_B = \u0026#34;active_job\u0026#34;.simple_camelize end Whatever::CONST_B =\u0026gt; \u0026#34;just simple_camelize\u0026#34; class A include Whatever def test puts \u0026#34;action_pack\u0026#34;.simple_camelize # 输出 just simple_camelize end end class B using StringRefinements def test puts \u0026#34;action_pack\u0026#34;.simple_camelize # 输出 ActionPack end end  ","permalink":"https://xguox.me/ruby-2-refinements.html/","tags":["Ruby"],"title":"语法补习: Refinements in Ruby 2.1"},{"categories":["Elasticsearch"],"contents":" #懒人系列 Elasticsearch 的所有监控管理查询等等操作几乎都可以在命令行里边用 curl 之类的工具完成, 只要记住一大堆形态各异的 url 就可以了. 如果经常用倒还好, 要是不常用, 过个一会就忘记了, 尤其对我这种上了年纪记性不大好的.\n比如查集群的健康状态, 有多少 indices, 某个 index 的 mappings, 还有当前的一些配置文件. 一大车.\n主要还是为了方便管控, 于是, 也就有了各种监控工具的出现. 如:\nElasticHQ, elasticsearch-head, elasticsearch-paramedic, elasticsearch-kopf , Bigdesk\n这几个是比较出名的吧, 目测还有一大波相对没那么多人使用的不为人知的工具存在.\nBigdesk Github 上看貌似很久没更新了, 所以, 直接就跳过没安装.\n用于对比, 直接就装了三个, 反正也不怎么碍着硬盘. ElasticHQ, elasticsearch-head 还有 elasticsearch-kopf. 安装方式都一样(Mac).\nElasticsearch 是用 Homebrew 装的,\n1  brew info elasticsearch   看下 Elasticsearch 的 plugin 相关信息,\n1 2 3 4 5  cd /usr/local/Cellar/elasticsearch/2.3.1/libexec/bin ./plugin install lmenezes/elasticsearch-kopf/2.0/v2.1.1 ./plugin install mobz/elasticsearch-head ./plugin install royrusso/elasticsearch-HQ   使用方式都是开箱即用的直接在浏览器打开链接地址:\nhttp://localhost:9200/_plugin/hq\nhttp://localhost:9200/_plugin/head\nhttp://localhost:9200/_plugin/kopf\n没有深入的使用, 看上去基本功能貌似都差不多.\n截图如下,\nHead\nKopf\nElasticHQ\nUPDATE:\nElasticHQ 的 REST Editor 居然不支持自定义的 url? 不是吧, 那要来何用? = . =\nelasticsearch-head 和 elasticsearch-kopf 都不错,\n综合下来感觉, head 的功能更强大些, 自带中文 i18n, 不过没有 kopf 那样给个暗色的主题 (つд⊂)\nUPDATE:\nElasticsearch 官方出品的 Marvel, 安装稍微麻烦点, 不过都有阐明, 性能监控比较好, 作为管理工具的话还是上面几个好点吧. Basic License 免费, 更多的功能貌似是需要付费的.\n","permalink":"https://xguox.me/elasticsearch-monitor.html/","tags":["Elasticsearch"],"title":"Elasticsearch Monitor"},{"categories":["Photo"],"contents":"2016.5.27 Day8 箱根, 芦之湖, 雾\nDay9 东京, 新宿\nDay10 Airline\n","permalink":"https://xguox.me/trip-japan-3-hakone-tokyo.html/","tags":["Photo"],"title":"Trip to Japan (Hakone, Tokyo)"},{"categories":["Photo"],"contents":"2016.5.23 Day4 新干线, 伏见稻荷\nDay5 京都\nDay6 大阪, 心斋桥, 道顿崛, 梅田\nDay7 大阪城, 新干线, 箱根\n","permalink":"https://xguox.me/trip-japan-2-kyoto-osaka.html/","tags":["Photo"],"title":"Trip to Japan (Kyoto, Osaka)"},{"categories":["Photo"],"contents":"2016.5.20 Day1 Airline\nDay2 Everything Hello Kitty\nDay3 浅草寺, 秋叶原, 台场, 东京铁塔\n","permalink":"https://xguox.me/trip-japan-1-tokyo.html/","tags":["Photo"],"title":"Trip to Japan (Tokyo)"},{"categories":["Elasticsearch"],"contents":"Elasticsearch(2.3.1) Upstart Script\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # Elasticsearch Upstart Script description \u0026#34;Elasticsearch upstart script\u0026#34; start on (net-device-up and local-filesystems and runlevel [2345] and startup) stop on runlevel [016] respawn respawn limit 10 30 # NB: Upstart scripts do not respect # /etc/security/limits.conf, so the open-file limits # settings need to be applied here. limit nofile 32000 32000 pre-start script NAME=elasticsearch # Elasticsearch PID file directory PID_DIR=\u0026#34;/var/run/elasticsearch\u0026#34; PID_FILE=\u0026#34;$PID_DIR/$NAME.pid\u0026#34; # Ensure that the PID_DIR exists (it is cleaned at OS startup time) if [ -n \u0026#34;$PID_DIR\u0026#34; ] \u0026amp;\u0026amp; [ ! -e \u0026#34;$PID_DIR\u0026#34; ]; then mkdir -p \u0026#34;$PID_DIR\u0026#34; \u0026amp;\u0026amp; chown \u0026#34;$NAME\u0026#34;:\u0026#34;$NAME\u0026#34; \u0026#34;$PID_DIR\u0026#34; fi if [ -n \u0026#34;$PID_FILE\u0026#34; ] \u0026amp;\u0026amp; [ ! -e \u0026#34;$PID_FILE\u0026#34; ]; then touch \u0026#34;$PID_FILE\u0026#34; \u0026amp;\u0026amp; chown \u0026#34;$NAME\u0026#34;:\u0026#34;$NAME\u0026#34; \u0026#34;$PID_FILE\u0026#34; fi end script script exec start-stop-daemon --start --chuid elasticsearch --exec /usr/bin/java -- -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.1.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.conf=/etc/elasticsearch end script  ","permalink":"https://xguox.me/elasticsearch-upstart.html/","tags":["Elasticsearch"],"title":"Elasticsearch Upstart"},{"categories":["Elasticsearch","Ruby"],"contents":" Elasticsearch 通过 from 和 size 参数来实现分页. size 表示返回的结果数量, 默认为 10, from 则表示从起始结果算起要跳过的结果数量, 默认为 0. 所以, 默认情况下如果返回结果数是 10 个以上, 我们得到的只有前十个结果.\n1.X 的时候不过请求的页数多深, 结果数多大都会給你返回结果, 只是越到后面的页数越大或者结果数越多, 执行的效率会逐渐变慢而已.\n页数越大之所以返回效率越差, 是因为 Elasticsearch 分页的工作方式是从所有主分片的索引中返回排在最前面的 size 数量的结果, 然后再把各个分片结果合并以后再排序, 然后再返回前 size 个结果.\n假设还是默认值, 如果想返回第 1000 页的话, 也就是排在 10001 - 10010 的结果. 而并不是直接一拿就拿到排在第 10001 之后的结果集, 而是返回 10010个结果然而排序, 然后丢弃掉前面的 10000 个结果.\n这是只有一个主分片的情况是这么工作的, 而如果假设是 5 个主分片的话. 还是请求第 1000页, 那么 Elasticsearch 要在这 5 个主分片下搜索结果, 然后每个分片得到 10010 个结果, 合并以后排序这 50050 个结果, 最后丢弃前面的 50040 个.\n所以, 不管是 Google 还是百度, 都会限定返回的结果页数. 用 Google 搜索 Elasticsearch 返回了 466,000 条结果, 然而只给我们前 15 页的.\n然后, 升到 2.X 以后, 如果请求产生的结果数超过一万个的话, 会抛出这样的错误信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  {:error=\u0026gt; {:root_cause=\u0026gt; [{:type=\u0026gt;\u0026#34;query_phase_execution_exception\u0026#34;, :reason=\u0026gt; \u0026#34;Result window is too large, from + size must be less than or equal to: [10000] but was [12000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.\u0026#34;}], :type=\u0026gt;\u0026#34;search_phase_execution_exception\u0026#34;, :reason=\u0026gt;\u0026#34;all shards failed\u0026#34;, :phase=\u0026gt;\u0026#34;query_fetch\u0026#34;, :grouped=\u0026gt;true, :failed_shards=\u0026gt; [{:shard=\u0026gt;0, :index=\u0026gt;\u0026#34;customers\u0026#34;, :node=\u0026gt;\u0026#34;RZ4Rj4QZQ7G7EIv5Y-CnEw\u0026#34;, :reason=\u0026gt; {:type=\u0026gt;\u0026#34;query_phase_execution_exception\u0026#34;, :reason=\u0026gt; \u0026#34;Result window is too large, from + size must be less than or equal to: [10000] but was [12000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.\u0026#34;}}]}, :status=\u0026gt;500}    Elasticsearch 也很 nice 的建议我们使用 Scroll API.\n1 2 3 4 5 6 7 8 9 10 11 12 13  client = Elasticsearch::Model.client client.indices.delete index: \u0026#39;test\u0026#39; 1_000.times do |i| client.index index: \u0026#39;test\u0026#39;, type: \u0026#39;test\u0026#39;, id: i+1, body: {title: \u0026#34;Test #{i}\u0026#34;} end client.indices.refresh index: \u0026#39;test\u0026#39; result = client.search index: \u0026#39;test\u0026#39;, scroll: \u0026#39;5m\u0026#39;, body: { query: { match: { title: \u0026#39;test\u0026#39; } }, sort: \u0026#39;_id\u0026#39; }  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  //result  {\u0026#34;_scroll_id\u0026#34;=\u0026gt; \u0026#34;cXVlcnlUaGVuRmV0Y2g7NTs3OlJaNFJqNFFaUTdHN0VJdjVZLUNuRXc7OTpSWjRSajRRWlE3RzdFSXY1WS1DbkV3Ozg6Ulo0Umo0UVpRN0c3RUl2NVktQ25FdzsxMDpSWjRSajRRWlE3RzdFSXY1WS1DbkV3OzExOlJaNFJqNFFaUTdHN0VJdjVZLUNuRXc7MDs=\u0026#34;, \u0026#34;took\u0026#34;=\u0026gt;18, \u0026#34;timed_out\u0026#34;=\u0026gt;false, \u0026#34;_shards\u0026#34;=\u0026gt;{\u0026#34;total\u0026#34;=\u0026gt;5, \u0026#34;successful\u0026#34;=\u0026gt;5, \u0026#34;failed\u0026#34;=\u0026gt;0}, \u0026#34;hits\u0026#34;=\u0026gt; {\u0026#34;total\u0026#34;=\u0026gt;1000, \u0026#34;max_score\u0026#34;=\u0026gt;nil, \u0026#34;hits\u0026#34;=\u0026gt; [{\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;14\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 13\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;19\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 18\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;22\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 21\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;24\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 23\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;25\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 24\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;26\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 25\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;29\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 28\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;40\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 39\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;41\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 40\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;44\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 43\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}]}}    可以看到每一次 scroll 返回的结果都带有 _scroll_id, 然后后续接着利用这个 _scroll_id 来滚动搜索.\n1  result = client.scroll scroll: \u0026#39;5m\u0026#39;, scroll_id: result[\u0026#39;_scroll_id\u0026#39;]  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  =\u0026gt; {\u0026#34;_scroll_id\u0026#34;=\u0026gt; \u0026#34;cXVlcnlUaGVuRmV0Y2g7NTs3OlJaNFJqNFFaUTdHN0VJdjVZLUNuRXc7OTpSWjRSajRRWlE3RzdFSXY1WS1DbkV3Ozg6Ulo0Umo0UVpRN0c3RUl2NVktQ25FdzsxMDpSWjRSajRRWlE3RzdFSXY1WS1DbkV3OzExOlJaNFJqNFFaUTdHN0VJdjVZLUNuRXc7MDs=\u0026#34;, \u0026#34;took\u0026#34;=\u0026gt;5, \u0026#34;timed_out\u0026#34;=\u0026gt;false, \u0026#34;_shards\u0026#34;=\u0026gt;{\u0026#34;total\u0026#34;=\u0026gt;5, \u0026#34;successful\u0026#34;=\u0026gt;5, \u0026#34;failed\u0026#34;=\u0026gt;0}, \u0026#34;hits\u0026#34;=\u0026gt; {\u0026#34;total\u0026#34;=\u0026gt;1000, \u0026#34;max_score\u0026#34;=\u0026gt;nil, \u0026#34;hits\u0026#34;=\u0026gt; [{\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;48\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 47\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;52\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 51\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;60\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 59\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;73\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 72\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;79\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 78\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;84\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 83\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;89\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 88\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;92\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 91\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;98\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 97\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}, {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;test\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;99\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;nil, \u0026#34;_source\u0026#34;=\u0026gt;{\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Test 98\u0026#34;}, \u0026#34;sort\u0026#34;=\u0026gt;[nil]}]}}    scroll: '5m' 是指这个 _scroll_id 的有效时长为 5分钟, 除了分钟以外, 年月日时分秒都可以. btw, 官方说每次 scroll 以后产生新的 _scroll_id, 不知道是不是理解错误, 除非开启一个新的 scroll 会生成新的 _scroll_id, 不然, 如果在有效时间内继续滚动的话返回的 _scroll_id 是一样的.\n如果只对查询的结果总体感兴趣而不需要对总体排序的话, 可以使用更为高效的 scan 模式,\n1 2 3 4 5 6 7 8  r = client.search index: \u0026#39;test\u0026#39;, search_type: \u0026#39;scan\u0026#39;, scroll: \u0026#39;5m\u0026#39;, size: 10 # Call the `scroll` API until empty results are returned while r = client.scroll(scroll_id: r[\u0026#39;_scroll_id\u0026#39;], scroll: \u0026#39;5m\u0026#39;) and not r[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;].empty? do puts \u0026#34;--- BATCH #{defined?($i) ? $i += 1 : $i = 1}-------------------------------------------------\u0026#34; puts r[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;].map { |d| d[\u0026#39;_source\u0026#39;][\u0026#39;title\u0026#39;] }.inspect puts end   Related: Elasticsearch 开箱笔记\nElasticsearch on Rails\nElasticsearch More Like This 搜索\nElasticsearch Aggregations 聚合分析\nUpgrade Elasticsearch to 2.3\nhttps://github.com/elastic/elasticsearch-ruby/blob/master/elasticsearch-api/lib/elasticsearch/api/actions/scroll.rb\nElasticsearch analysis \u0026amp; 自定义 analyzers\nElasticsearch 如何不用停机情况下完成 mapping 的修改\n","permalink":"https://xguox.me/elasticsearch-scroll.html/","tags":["Elasticsearch","Ruby"],"title":"Elasticsearch Scroll (Ruby)"},{"categories":["Elasticsearch"],"contents":"正式环境和本地一直跑的 1.X, 最近在用 Elasticsearch 的 Aggregations 功能, 发现有一些个语法在 2.X 版本变了, 想来也要跟上潮流了. 于是着手各种升级事宜.\n官方有专门的 Rolling upgrades 链接讲如何升级的, 不过, 对于目前在用的情况来说, 貌似也用不上那么麻烦. 好吧, 其实有点谈不上升级, 感觉像是直接卸载了重装差不多 = . =\n{% highlight ruby %}\nUbuntu sudo service elasticsearch stop wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.3.1.deb sudo dpkg -P elasticsearch sudo dpkg -i elasticsearch-2.3.1.deb sudo service elasticsearch start {% endhighlight %}\n理论上应该无痛可以跑起来的, 不过, 有一台服务器上, 安装好了以后, Elasticsearch 一直跑不起来 sudo service elasticsearch start 一直 fail~fail~fail\nelasticsearch.log 又啥也没输出. 折腾了半天才发现原来是 Java 的版本没跟上. 本地的 Java 8 和另一台 Java 7 都没事, 唯独这台, java -version\n1 2 3  java version \u0026#34;1.6.0_65\u0026#34; Java(TM) SE Runtime Environment (build 1.6.0_65-b14-468-11M4833) Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-468, mixed mode)   Elasticsearch 的官方 requirements 说最低是要 Java 7, 像之前一直跑的这个 Java 6 很可能会有各种已知的 bugs, 虽然, 1.X 还是能在这种情况下跑起来, 不过升级到 2.X 就歇菜了.\n升级完以后查看了一下状态\ncurl -XGET http://localhost:9200/_cluster/health\\?pretty\\=true\n发现一直以来 Elasticsearch 的这个 status 都是黄的, 印象中好像没绿过\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ➜ curl -XGET http://localhost:9200/_cluster/health\\?pretty\\=true { \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch_xguox\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;yellow\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 6, \u0026#34;active_shards\u0026#34; : 6, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 1, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 85.71428571428571 }   elasticsearch.yml 配置文件中有一段注释: 修改 index.number_of_replicas: 0 然后 reindex 就绿了.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # Note, that for development on a local machine, with small indices, it usually # makes sense to \u0026#34;disable\u0026#34; the distributed features: # #index.number_of_shards: 1 #index.number_of_replicas: 0 # These settings directly affect the performance of index and search operations # in your cluster. Assuming you have enough machines to hold shards and # replicas, the rule of thumb is: # # 1. Having more *shards* enhances the _indexing_ performance and allows to # _distribute_ a big index across machines. # 2. Having more *replicas* enhances the _search_ performance and improves the # cluster _availability_. # # The \u0026#34;number_of_shards\u0026#34; is a one-time setting for an index. # # The \u0026#34;number_of_replicas\u0026#34; can be increased or decreased anytime, # by using the Index Update Settings API. #  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ➜ curl -XGET http://localhost:9200/_cluster/health\\?pretty\\=true { \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch_xguox\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 6, \u0026#34;active_shards\u0026#34; : 6, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 0, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 100.0 }   还有就是, 查看 elasticsearch.log 的时候经常看到类似这样的信息,\n1  high disk watermark [90%] exceeded on [VopDVS8uRPurOowYR78xkQ][Marvel Boy][/usr/local/var/elasticsearch/elasticsearch_xguox/nodes/0] free: 23gb[9.9%], shards will be relocated away from this node   也是在 配置文件 elasticsearch.yml 添加下面几句就搞定了:\n1 2 3 4 5  cluster.routing.allocation.disk.threshold_enabled: true cluster.routing.allocation.disk.watermark.low: .97 cluster.routing.allocation.disk.watermark.high: .99   BTW, 现在都流行飙版本号了?\nReact 直接跳到 15.X, 这\u0026hellip;\n Update: 2016.04.23 参加完 Elasticsearch 的线下沙龙涨了点点知识回来, 唉, 发现人家那才真的叫玩大数据啊, 各种 Spark, Hadoop, 几十几百台的机器, 上 TB 级别的数据量.\n所以, 难怪我这种说升级说的那么轻松的.\nElasticsearch 升级 2.X 前用官方插件检测一下是否兼容 elasticsearch-migration\n还有其他一些建议, 顺带贴下图纪念一下手机摄像头已坏 = . =\nRelated: Elasticsearch 开箱笔记\nElasticsearch on Rails\nElasticsearch More Like This 搜索\nElasticsearch Aggregations 聚合分析\nElasticsearch Scroll (Ruby)\n","permalink":"https://xguox.me/upgrade-elasticsearch-2-3.html/","tags":["Elasticsearch"],"title":"Upgrade Elasticsearch to 2.3"},{"categories":["Ruby"],"contents":" 处理大文件是一项非常耗内存的操作, 有时候甚至会跑光服务器上的物理内存和虚拟内存. 下面来看看使用 Ruby 来处理大型 CSV 文件的几种方式, 同时测试一下这几种方式的内存消耗以及性能.\n准备测试用的 CSV 数据文件. 在开始之前, 先来准备一个拥有一百万行的 CSV 文件 data.csv(大约 75mb)用于测试.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # generate_csv.rb require \u0026#39;csv\u0026#39; require_relative \u0026#39;./helpers\u0026#39; headers = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;street\u0026#39;, \u0026#39;country\u0026#39;] name = \u0026#34;Pink Panther\u0026#34; email = \u0026#34;pink.panther@example.com\u0026#34; city = \u0026#34;Pink City\u0026#34; street = \u0026#34;Pink Road\u0026#34; country = \u0026#34;Pink Country\u0026#34; print_memory_usage do print_time_spent do CSV.open(\u0026#39;data.csv\u0026#39;, \u0026#39;w\u0026#39;, write_headers: true, headers: headers) do |csv| 1_000_000.times do |i| csv \u0026lt;\u0026lt; [i, name, email, city, street, country] end end end end   内存及时间的消耗 上面这个脚本需要引用到 helpers.rb 脚本, helpers.rb 定义了两个 helper 方法来测量并打印出内存以及时间的消耗情况.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # helpers.rb require \u0026#39;benchmark\u0026#39; def print_memory_usage memory_before = `ps -o rss= -p #{Process.pid}`.to_i yield memory_after = `ps -o rss= -p #{Process.pid}`.to_i puts \u0026#34;Memory: #{((memory_after - memory_before) / 1024.0).round(2)} MB\u0026#34; end def print_time_spent time = Benchmark.realtime do yield end puts \u0026#34;Time: #{time.round(2)}\u0026#34; end   执行并生成 CSV 文件:\n1 2 3  $ ruby generate_csv.rb Time: 7.14 Memory: 4.79 MB   不同的机器输出结果也不尽相同, 不过, 幸运的是, 得益于 garbage collector (GC) 回收已使用过的内存, 这个 Ruby 进程耗掉的内存(4.79MB)并不是很夸张. 同时生成的数据文件如图为 74MB.\nCSV.read 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # parse1.rb require_relative \u0026#39;./helpers\u0026#39; require \u0026#39;csv\u0026#39; print_memory_usage do print_time_spent do csv = CSV.read(\u0026#39;data.csv\u0026#39;, headers: true) sum = 0 csv.each do |row| sum += row[\u0026#39;id\u0026#39;].to_i end puts \u0026#34;Sum: #{sum}\u0026#34; end end   执行结果:\n1 2 3 4 5  $ ruby parse1.rb Sum: 499999500000 Time: 21.3 Memory: 1277.07 MB   惊人的超过 1GB 内存消耗啊.\nCSV.read 源码\n1 2 3 4 5 6 7 8 9  # File csv.rb, line 1750 def read rows = to_a if @use_headers Table.new(rows) else rows end end   CSV.parse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # parse2.rb require_relative \u0026#39;./helpers\u0026#39; require \u0026#39;csv\u0026#39; print_memory_usage do print_time_spent do content = File.read(\u0026#39;data.csv\u0026#39;) csv = CSV.parse(content, headers: true) sum = 0 csv.each do |row| sum += row[\u0026#39;id\u0026#39;].to_i end puts \u0026#34;Sum: #{sum}\u0026#34; end end   执行结果:\n1 2 3 4  $ ruby parse2.rb Sum: 499999500000 Time: 21.88 Memory: 1362.89 MB   可以看到, 内存消耗的比刚刚第一个脚本还略多, 差距大概就是刚好 CSV 文件大小.\nCSV.parse 源码\n1 2 3 4 5 6 7 8 9 10 11 12 13  # File csv.rb, line 1293 def self.parse(*args, \u0026amp;block) csv = new(*args) if block.nil? # slurp contents, if no block is given begin csv.read ensure csv.close end else # or pass each row to a provided block csv.each(\u0026amp;block) end end   CSV.new 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # parse3.rb require_relative \u0026#39;./helpers\u0026#39; require \u0026#39;csv\u0026#39; print_memory_usage do print_time_spent do content = File.read(\u0026#39;data.csv\u0026#39;) csv = CSV.new(content, headers: true) sum = 0 while row = csv.shift sum += row[\u0026#39;id\u0026#39;].to_i end puts \u0026#34;Sum: #{sum}\u0026#34; end end   执行结果:\n1 2 3 4  $ ruby parse3.rb Sum: 499999500000 Time: 16.89 Memory: 76.72 MB   这次结果可以看到, 因为只是在内存中加载了整个文件内容, 所以内存的消耗大概就是文件的大小(74MB), 而处理时间也快了不少. 当我们只是想逐行逐行的操作而不是读取要一次过读取一整个文件时候, 这种方法非常奏效.\n通过 IO 对象一行一行解析 虽然有了很大的进步, 但是, 使用 IO 文件对象还可以做得更好.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # parse4.rb require_relative \u0026#39;./helpers\u0026#39; require \u0026#39;csv\u0026#39; print_memory_usage do print_time_spent do File.open(\u0026#39;data.csv\u0026#39;, \u0026#39;r\u0026#39;) do |file| csv = CSV.new(file, headers: true) sum = 0 while row = csv.shift sum += row[\u0026#39;id\u0026#39;].to_i end puts \u0026#34;Sum: #{sum}\u0026#34; end end end   执行结果:\n1 2 3 4  $ ruby parse4.rb Sum: 499999500000 Time: 13.78 Memory: 2.64 MB   仅仅用了 2.64MB, 速度也稍稍又快了一些.\nCSV.foreach 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # parse5.rb require_relative \u0026#39;./helpers\u0026#39; require \u0026#39;csv\u0026#39; print_memory_usage do print_time_spent do sum = 0 CSV.foreach(\u0026#39;data.csv\u0026#39;, headers: true) do |row| sum += row[\u0026#39;id\u0026#39;].to_i end puts \u0026#34;Sum: #{sum}\u0026#34; end end   执行结果跟上一个差不多:\n1 2 3 4  $ ruby parse5.rb Sum: 499999500000 Time: 15.54 Memory: 2.62 MB   CSV.foreach 源码\n1 2 3 4 5 6 7  # File csv.rb, line 1118 def self.foreach(path, options = Hash.new, \u0026amp;block) return to_enum(__method__, path, options) unless block open(path, options) do |csv| csv.each(\u0026amp;block) end end   Source Link 来自: http://dalibornasevic.com/posts/68-processing-large-csv-files-with-ruby\n","permalink":"https://xguox.me/processing-large-csv-with-ruby.html/","tags":["Ruby"],"title":"使用 Ruby 处理大型 CSV 文件"},{"categories":["Elasticsearch"],"contents":" Elasticsearch 除了全文搜索以外还有一个主要功能, 就是数据的聚合分析, Aggregations. 有点类似于 SQL 中的 GROUP BY.\nElasticsearch 的 Aggregations API 给出了一大堆的用法.\n1.X 主要分两类: Bucket Aggregations 和 Metrics Aggregations, 2.X 多了一类 Pipeline aggregations. Pipeline aggregations 比较新, 官方的说法是在未来的改动会较大, 甚至会移除, 所以, 暂时不讨论先.\nMetrics Aggregations 顾名思义, 主要是用于计算特定的度量字段, 其实也不一定是文档的某个特定字段值, 可以是文档通过 script 生成的值. Metrics Aggregations 是不能有子聚合的(sub-aggregations)\nBucket Aggregations 英语的 Bucket 有\u0026rsquo;桶\u0026rsquo;的意思, 按照官方的说法, Bucket Aggregations 定义了一些特定的条件, 比如 \u0026lsquo;country\u0026rsquo; 字段值为 \u0026lsquo;Canada\u0026rsquo;, 或者 \u0026lsquo;gender\u0026rsquo; 字段值为 \u0026lsquo;Male\u0026rsquo;, \u0026lsquo;comments_count\u0026rsquo; 在区间 [{ to: 50 },{ from: 50, to: 150 }, { from: 150, to: 500 }] 之中等等, 只要文档满足这些条件就丢进这个桶(Bucket)里面. Bucketing aggregations 可以有子聚合(sub-aggregations), sub-aggregations 可以是 Bucketing aggregations 也可以是 Metrics Aggregations. 子聚合是在父聚合\u0026rdquo;桶里面\u0026rdquo;的文档集合上继续做聚合分析的. 大部分情况下, 配合这两种聚合类型一起用才能发挥更多功效.\nAggregations 的语法结构: 1 2 3 4 5 6 7 8 9 10 11  \u0026#34;aggs\u0026#34; : { \u0026#34;\u0026lt;aggregation_name\u0026gt;\u0026#34; : { \u0026#34;\u0026lt;aggregation_type\u0026gt;\u0026#34; : { \u0026lt;aggregation_body\u0026gt; } [,\u0026#34;meta\u0026#34; : { [\u0026lt;meta_data_body\u0026gt;] } ] [,\u0026#34;aggs\u0026#34; : { [\u0026lt;sub_aggregation\u0026gt;]+ } ] } [,\u0026#34;\u0026lt;aggregation_name_2\u0026gt;\u0026#34; : { ... } ] }    如果喜欢敲多几个字母的话, aggs 可以用 aggregations来代替.\n举个栗子:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  ➜ curl -XPOST \u0026#34;http://localhost:9200/plus-customers/plus-customer/_search?pretty\u0026#34; -d \u0026#39; { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;filtered\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [{ \u0026#34;term\u0026#34;: { \u0026#34;gender\u0026#34;: 1 } }] } } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;country\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_metric\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;orders_count\u0026#34; } } } } } } \u0026#39;    返回结果:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  { \u0026#34;took\u0026#34; : 183, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 9930000, \u0026#34;max_score\u0026#34; : 0.0, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;aggregations\u0026#34; : { \u0026#34;group_by\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;Canada\u0026#34;, \u0026#34;doc_count\u0026#34; : 1299880, \u0026#34;avg_metric\u0026#34; : { \u0026#34;value\u0026#34; : 0.628643724696356 } }, { \u0026#34;key\u0026#34; : \u0026#34;United States\u0026#34;, \u0026#34;doc_count\u0026#34; : 8630120, \u0026#34;avg_metric\u0026#34; : { \u0026#34;value\u0026#34; : 3.4285714285714286 } } ] } } }    size: 0 是指只返回聚合结果, 不加这个条件的话, [\u0026ldquo;hits\u0026rdquo;][\u0026ldquo;hits\u0026rdquo;] 对应的值会是一个包含所有匹配文档的数组.\n当使用 Filter Aggregation 时候 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  ➜ curl -XPOST \u0026#34;http://localhost:9200/plus-customers/plus-customer/_search?pretty\u0026#34; -d \u0026#39; { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;agg1\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;Canada\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Quebec\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;agg2\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gender\u0026#34; } } } } } } \u0026#39; // Elasticsearch 1.X 这样还能正常运行, 到了 2.X 就会报错了 // [term] query does not support different field names, use [bool] query instead // 要用下面这样来替代了  ➜ curl -XPOST \u0026#34;http://localhost:9200/plus-customers/plus-customer/_search?pretty\u0026#34; -d \u0026#39; { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;agg1\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;Canada\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;Quebec\u0026#34; } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;agg2\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gender\u0026#34; } } } } } } \u0026#39;    Elasticsearch Aggregation 不支持分页 默认结果是只返回 doc_count 前十的的 keys, 如果要使结果返回所有的 keys 的话, 需要加上 \u0026quot;size\u0026quot;: 0, 比如:\n1 2 3 4 5 6 7 8 9 10 11 12  ➜ crm git:(master) curl -XPOST \u0026#34;http://localhost:9200/plus-customers/plus-customer/_search?pretty\u0026#34; -d \u0026#39;{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;agg1\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;state\u0026#34;, \u0026#34;size\u0026#34;: 0 } } } }\u0026#39;    如果不加 \u0026quot;size\u0026quot;: 0, 返回是这样子的:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  { \u0026#34;took\u0026#34; : 35, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : 448140, \u0026#34;max_score\u0026#34; : 0.0, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;aggregations\u0026#34; : { \u0026#34;agg1\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 297971, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;doc_count\u0026#34; : 33440 }, { \u0026#34;key\u0026#34; : \u0026#34;Delaware\u0026#34;, \u0026#34;doc_count\u0026#34; : 27357 }, { \u0026#34;key\u0026#34; : \u0026#34;New jersey\u0026#34;, \u0026#34;doc_count\u0026#34; : 23021 }, { \u0026#34;key\u0026#34; : \u0026#34;South carolina\u0026#34;, \u0026#34;doc_count\u0026#34; : 14211 }, { \u0026#34;key\u0026#34; : \u0026#34;Utah\u0026#34;, \u0026#34;doc_count\u0026#34; : 10169 }, { \u0026#34;key\u0026#34; : \u0026#34;Virginia\u0026#34;, \u0026#34;doc_count\u0026#34; : 9920 }, { \u0026#34;key\u0026#34; : \u0026#34;Pennsylvania\u0026#34;, \u0026#34;doc_count\u0026#34; : 9879 }, { \u0026#34;key\u0026#34; : \u0026#34;Connecticut\u0026#34;, \u0026#34;doc_count\u0026#34; : 8978 }, { \u0026#34;key\u0026#34; : \u0026#34;Arizona\u0026#34;, \u0026#34;doc_count\u0026#34; : 7231 }, { \u0026#34;key\u0026#34; : \u0026#34;California\u0026#34;, \u0026#34;doc_count\u0026#34; : 5963 } ] } } }    一直以来都有很多人抱怨为嘛不给 Aggregation 加上分页, 这里有讨论https://github.com/elastic/elasticsearch/issues/4915 目前数据量不算太巨大的时候, 还是可以搞假分页的(仅仅视觉上), 不过每次还是把所有的结果都取出来了.\nRelated: Elasticsearch 开箱笔记\nElasticsearch on Rails\nElasticsearch More Like This 搜索\nUpgrade Elasticsearch to 2.3\nElasticsearch Scroll (Ruby)\nElasticsearch analysis \u0026amp; 自定义 analyzers\nElasticsearch 如何不用停机情况下完成 mapping 的修改\n","permalink":"https://xguox.me/elasticsearch-aggregations.html/","tags":["Elasticsearch"],"title":"Elasticsearch Aggregations 聚合分析"},{"categories":["Elasticsearch"],"contents":" Elasticsearch 封装好了More Like This 搜索功能, 用来基于给定的某个或某组文档, 返回与之相似的文档. 语法如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  { \u0026#34;more_like_this\u0026#34; : { \u0026#34;fields\u0026#34; : [\u0026#34;title\u0026#34;, \u0026#34;description\u0026#34;], \u0026#34;docs\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;imdb\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;movies\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34; }, { \u0026#34;_index\u0026#34; : \u0026#34;imdb\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;movies\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34; }], \u0026#34;min_term_freq\u0026#34; : 1, \u0026#34;max_query_terms\u0026#34; : 12 } }    结合 Rails 例子:\n1 2 3  @post = Post.last query = {:query=\u0026gt;{:more_like_this=\u0026gt;{docs:[{_index: \u0026#39;posts\u0026#39;,_type: \u0026#39;post\u0026#39;, _id: @post.id}], :min_term_freq=\u0026gt;1, :min_doc_freq=\u0026gt;1}}} records = Post.search(query).records   这个查询的参数有三个,分别是 Document Input Parameters, Term Selection Parametersedit, Query Formation Parameters\nDocument Input Parameters 是必须有的查询参数, 可以是 docs, ids 或者 like_text之一.\nTerm Selection Parametersedit 可以是 max_query_terms, min_term_freq, min_doc_freq, max_doc_freq, min_word_length, max_word_length, stop_words, analyzer\nQuery Formation Parameters 可以是 minimum_should_match, boost_terms, include, boost\n相关释义文档写的还是挺清楚的.\nRelated: Elasticsearch 开箱笔记\nElasticsearch on Rails\nElasticsearch Aggregations 聚合分析\nUpgrade Elasticsearch to 2.3\nElasticsearch Scroll (Ruby)\nElasticsearch analysis \u0026amp; 自定义 analyzers\nElasticsearch 如何不用停机情况下完成 mapping 的修改\n","permalink":"https://xguox.me/elasticsearch-more-like-this.html/","tags":["Elasticsearch"],"title":"Elasticsearch More Like This 搜索"},{"categories":["Photo"],"contents":" 先评再放片, 光圈几乎都是在 F1.4 ~ F2.0 总得来说, 分辨率杠杠的, 色彩的话, 还是跟以前一样, 玩后期的几乎不在意直出色彩. 即便是 F1.4 也别指望虚化能力有多强, 因为这是 24mm, 好吧, 如果我是求虚化的话估计现在还停留在之前的 85mm 吧.\n家里的阳台一些小盆栽. 室内人像. 环境灯光本就有点昏黄, 试了下把色温拉低, 感觉还是原色好. 室外人像. 做了一些裁剪\n夜景 update 用了差不多两个周末了, 才发现, 原来插上了这个接环以后, 相机会把 APS-C 画幅拍摄设置到自动 = . = 卸下接环以后是变回关闭状态. 所以, 原来我拍了两周的 35mm.对自己表示, 呵!呵! (´･_･`) 所以, 其实, 我的归宿是 索蔡 Distagon T* FE 35mm f/1.4 ZA???\nupdate ","permalink":"https://xguox.me/sigma-24mm-f14-art-a7r-sample.html/","tags":["Photo"],"title":"Sony A7R on 适马 Sigma 24mm F1.4 Art, Sample"},{"categories":["Photo"],"contents":"索蔡 Sonnar T* FE 55mm F1.8 ZA** 其实用的还是挺不错的. 只是, 用来挂机的话 55mm 还是稍稍长了点, 最后又是出了, 用了才2个月不到, 对自己也是有点服了. 好吧, 其实这个头出的还是很不舍的. 毕竟, 总体这个头各方面都挺喜欢的, 唯独是经常和内人一起外出的话, 最近 55mm 加上那最近对焦实在吃紧. 最后, 又是出了. 先后考虑过纯蔡 Zeiss Batis 25mm f/2 和 索蔡 Distagon T* FE 35mm f/1.4 ZA, 前者觉得光圈不够, 后者 35mm 觉得还是不够广, 再者价格吃不消, 而且, 综合比较看了网上的一些评测以后, 最终, 我居然选择了唯卓(VILTROX) EF-NEX 的二代环和适马 Sigma 24mm F1.4 Art**. 不知道这头能撑多久呢? (◎-◎；)\n开箱照: 糊了也放上来\nA7R 本身的自动对焦性能就一般, 虽说这个二代环支持 EF 口的一些镜头给自动对焦, 但是, 目前也不期望速度能有多快. 所以, 其实, 本来就预了玩手动的. 不过, 惊讶的是, 自动对焦时候, 连眼(脸)部识别都支持. 当然, 还是很慢的, 一般都得要个 2~3s 才对上. 拍风景的话这速度也没啥关系, 摆拍的话其实也还好(好吧, 其实经常都是对好焦了才开始摆), 抓拍直接歇菜可以洗洗睡了. 最后, 其实自动对焦和手动峰值放大对焦使用频率各占一半吧.\nBTW, 这货加上接环的话比索蔡 Distagon T* FE 35mm f/1.4 ZA 还要重呢, 所以, 慎入.\n试镜照见下回分解.\n","permalink":"https://xguox.me/a7r-sigma-24mm-f14-art.html/","tags":["Photo"],"title":"Sony A7R on 适马 Sigma 24mm F1.4 Art"},{"categories":["Photo"],"contents":"如传闻一样, 真的很锐. 至于焦外, 没什么特别吧, 中规中矩, 木眼感觉不出什么. 洋葱圈什么的也没多在乎. 目前见识过最养眼的散景应该是 135mm F2.8[T4.5] STF . 好吧, 扯远了. 色彩什么的, 数码产品, 色彩都是主观的说法, 反正这一辑全都调过色了.\n几乎全程光圈都在 1.8 ~ 2.2 左右.\nAND THEN,\n其实这是第二次来红专厂了. 上一次来还是没毕业的差不多三年前了吧, 而且那会是跟好机油一起来的. 巧的是, 那会的机器还是 Sony SLT-A77 + DT 50mm F1.8 SAM, 也是新入一枚定焦跑去试镜了 XD\n忘记了以前是怎么样调色的了 ╮(╯_╰)╭\n","permalink":"https://xguox.me/fe55-f18-redtory.html/","tags":["Photo"],"title":"红专厂 FE 55mm F1.8 ZA 试镜"},{"categories":["Photo"],"contents":"先是出掉了 FE 28-70mm f/3.5-5.6 OSS 和 Nikon AF-S NIKKOR 85mm f/1.8G 以后, 就剩下一台光杆 A7R 了. 然后是大法门下的 Sonnar T* FE 55mm F1.8 ZA 广受好评, 再然后, 就是收到镜头啦~\nヾ(=^▽^=)ノ 高高兴兴的 open-box 咯.\n拿出来那会觉得沉甸甸的感觉, 主要是跟体型不太服合, 体积看上去是没 28-70mm 大的, 但是, 却反而还略重了一些. 主要是各种金属, 杠杠的. 抚摸党表示手感很赞.\nbtw, 不喜欢索蔡 E 口的镜头后盖. 颜色看上去很突兀, 好吧, 也不常用到 ╮(╯_╰)╭\n没用过多少 FE 的镜头, 但是, 仅从网上看一些图片的话, 个人感觉 FE 是目前搭 A7/R 系列最协调的镜头, 没有之一. 长度粗细都很适中. FE 35 2.8 略短了些, 其他的像 28-70mm 或者 Batis 25 F2 这些镜头屁股会突然瘦一大截, 于是也就不怎么好看了\n样片见下一篇吧 (●\u0026rsquo;◡\u0026rsquo;●)ﾉ♥\n","permalink":"https://xguox.me/fe55-f18-open-box.html/","tags":["Photo"],"title":"Sonnar T* FE 55mm F1.8 ZA 开箱"},{"categories":["Tools"],"contents":"很多时候, 为了方便别人帮忙 review 代码, 习惯喜欢把某个 todo 任务的提交合并作为一个.\n如果想直接合到前一个 commit 的话, 一般都是用\ngit commit --amend\n很多时候 typo 都这么干, 省的多一个 commit.\n而当不是简单的 typo, 并且这个 todo 分支在开发过程中提交了比较多的 commit 时候, 用到的方法有两个, 这种情况还是比较多的, 尤其某个 todo 较为复杂.(当然, 使用的前提是这些分支提交的 author 都是你自己)\n第一种: 极其简单粗暴, 直接 git reset 到过去的某个 SHA, 再重新 commit. = . = 目前没发现什么副作用, 因为 reset 只是把 commit 变为 unstage 状态, 并不会直接删除你在这些提交中的更改, 好吧, 如果你想 discard changes 的话再来一发 git checkout, 那就白干了 (￣(エ)￣)ゞ\n第二种: 使用的是 git rebase 这里借 ruby-china 的项目来说明吧. 最近几次的提交是这样的, 假设最后的三个提交都是某个 todo 任务的相关提交(好吧, 其实这里不是相关的, 只是假设), 如果要把它们合并为一个的话\ngit rebase -i HEAD~3\n返回的结果是酱紫的, 然后把后两个的 pick 改为 s, 保存退出, 会显示提交信息的编辑,\n自己在手工输入提交信息或者直接用默认的都可以. 除了 pick, s(quash)以外还有几个用法, 不过用的比较少 = . =\n最终结果, 可见, 最后三个 commit 被合并为一个了.\n","permalink":"https://xguox.me/git-squashing-commits-with-rebase.html/","tags":["Tools"],"title":"Git 合并 commits"},{"categories":["Linux"],"contents":"Mark 一下一个有用的命令, 运维比较渣没有系统研究过, 唯有见一个标一个 = . =\n1  netstat -lpn list current enable port   输出一般如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:9300 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN - udp 0 0 0.0.0.0:54328 0.0.0.0:* - udp 0 0 127.0.0.1:123 0.0.0.0:* - udp 0 0 0.0.0.0:123 0.0.0.0:* - udp6 0 0 :::123 :::* - Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ACC ] SEQPACKET LISTENING 7466 - /run/udev/control unix 2 [ ACC ] STREAM LISTENING 9525 - /var/run/nscd/socket unix 2 [ ACC ] STREAM LISTENING 7126 - @/com/ubuntu/upstart unix 2 [ ACC ] STREAM LISTENING 11561 -  ","permalink":"https://xguox.me/check-opened-closed-port.html/","tags":["Linux"],"title":"查看 VPS 上打开/关闭的端口号"},{"categories":["Linux"],"contents":"Shell 命令缓慢的学习中\u0026hellip;\nShell 脚本的 \u0026gt; 表示输出重定向, 而 0 1 2 则分别代表标准输入(stdin), 标准输出(stdout)和标准错误(stderr),\n{% highlight shell %} ls 2\u0026gt;1 {% endhighlight %}\n会在当前目录执行 ls, 因为没有标准错误信息, 所以, 会产生一个空白的文件 1.\n{% highlight shell %} ls xguox 2\u0026gt;1 {% endhighlight %}\n会生成一个名为 1 的文件, 里面的内容是 ls: xguox: No such file or directory\n1  ls xguox 2\u0026gt;\u0026amp;1   不会生成一个名为 1 的文件, 但是会把错误直接作为标准输出(即1) ls: xguox: No such file or directory\n所以, 也就可以解读 crontab 里面的命令了\n1  0 8-18/5 * * * cd ~/apps/current \u0026amp;\u0026amp; /bin/bash -l -c \u0026#34;RAILS_ENV=production bundle exec rake post:auto_post\u0026#34; 2\u0026gt;\u0026amp;1 \u0026gt; log/init.log   最后, 在命令最后加 \u0026amp; , 是让命令在后台执行\n","permalink":"https://xguox.me/linux-shell-2-1.html/","tags":["Linux"],"title":"Linux 的 2\u003e\u00261"},{"categories":["Photo"],"contents":"一直以来主力机都用的是 A7R + FE 28-70, 因为某些缘故, 前个月入了 Nikon AF-S NIKKOR 85mm f/1.8G, (不是拿来配 A7R 的, 虽然有想过玩转接)\n这次去台湾粗略的浏览了一遍, 转了小半圈, 跟朋友借了一台 D610, 从头到尾扛双机. 拍拍照片, 风景是不指望 85mm 的, 所以, 85mm 大部分时候是拿来拍人像(自拍或者妻子).\n回来没多久就把 85定和 FE 28-70 都出了.\n 85mm 不适合我, 或者说自己用不惯也用不好这个焦段吧. 也不适合旅游拍到此一游 = . = FE 28-70 能满足日常使用, 不过, 光圈实在捉急  诚品的灯光本就带着一些昏黄, 没去刻意调白.\n美丽岛 - 地铁站\n客串小品照.\n总体而言, 对于这一次出游台湾拍的照片自己不是很满意, 但无碍台湾是个好地方, 一头半个月也不足以完全领略的, 更何况我们也就才呆了十天不到, 应该还会再去的.\n九份的蓝天白云, 还有海和山, 让人感觉非常舒服. 尤其天的干净, 这是在广州不敢奢望的.\n对于器材, 依旧处于探索和玩的状态中.\n","permalink":"https://xguox.me/taiwan-tour.html/","tags":["Photo"],"title":"拍照在台湾"},{"categories":["Ruby"],"contents":"在小项目没怎么遇见过的 model 之间的 has_many 关联选项 :inverse_of, 用来指定相对应的 belongs_to 关联的名字, (反之对于 belongs_to 也一样,文邹邹的 (・_・ヾ ). 不能与 :through , :as, :polymorphic 一起使用.\n1 2 3 4 5 6 7 8 9 10 11  class Product \u0026lt; ActiveRecord::Base has_many :request_items, :inverse_of =\u0026gt; :product, :class_name =\u0026gt; \u0026#39;PurchaseRequestItem\u0026#39; end class PurchaseRequestItem \u0026lt; ActiveRecord::Base belongs_to :purchase_request, :inverse_of =\u0026gt; :request_items, :class_name =\u0026gt; \u0026#34;PurchaseRequest\u0026#34; belongs_to :product, :inverse_of =\u0026gt; :request_items, :class_name =\u0026gt; \u0026#34;Product\u0026#34; end  1 2 3 4 5 6 7 8  [1] pry(main)\u0026gt; purchase_request_item = PurchaseRequestItem.last PurchaseRequestItem Load (0.2ms) SELECT `purchase_request_items`.* FROM `purchase_request_items` ORDER BY `purchase_request_items`.`id` DESC LIMIT 1 =\u0026gt; #\u0026lt;PurchaseRequestItem id: 56387, purchase_request_id: 2979, product_id: 311795\u0026gt; [2] pry(main)\u0026gt; product = Product.find 311795 Product Load (0.5ms) SELECT `products`.* FROM `products` WHERE `products`.`id` = 311795 AND (`products`.deleted_at IS NULL) LIMIT 1 =\u0026gt; #\u0026lt;Product id: 311795, status: 1, is_popular: false\u0026gt;  1 2 3 4 5 6 7 8 9 10  # 没有加上 :inverse_of 选项的话是酱紫的 [3] pry(main)\u0026gt; purchase_order_item.product == product Product Load (0.4ms) SELECT `products`.* FROM `products` WHERE `products`.`id` = 311795 AND (`products`.deleted_at IS NULL) LIMIT 1 =\u0026gt; true [4] pry(main)\u0026gt; product.status=3 =\u0026gt; 3 [5] pry(main)\u0026gt; purchase_order_item.product.status==product.status =\u0026gt; false  1 2 3 4 5 6 7 8  # 加上 :inverse_of 是酱紫的 [3] pry(main)\u0026gt; purchase_order_item.product == product =\u0026gt; true [4] pry(main)\u0026gt; product.status=3 =\u0026gt; 3 [5] pry(main)\u0026gt; pri.product.status==product.status =\u0026gt; true   如果没有 :inverse_of 的话, 会在数据库中查找多一次, 而如果有 :inverse_of 的话, 是直接从内存里面取的.\n好吧, Rails 4.1 开始默认是加上了 :inverse_of 的. 只是公司有项目还跑在 Rails 3, 所以, 还是得手工加上这个 选项.\n","permalink":"https://xguox.me/rails-model-association-inverse_of.html/","tags":["Ruby"],"title":"[Rails] Model 关联选项之 :inverse_of"},{"categories":["Ruby"],"contents":" 除了页面(Page), 动作(Action), 片段(Fragment)等这三种 Rails 所支持的缓存以外, Rails 还提供了更底层的, 对于特定值或者查询结果缓存的支持.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [3] pry(main)\u0026gt; Rails.cache.read(\u0026#39;first_post_cache\u0026#39;) =\u0026gt; nil [5] pry(main)\u0026gt; Rails.cache.write(\u0026#39;first_post_cache\u0026#39;, Post.first, expires_in: 2.minute) Post Load (0.3ms) SELECT `posts`.* FROM `posts` ORDER BY `posts`.`id` ASC LIMIT 1 =\u0026gt; true [6] pry(main)\u0026gt; Rails.cache.read(\u0026#39;first_post_cache\u0026#39;) =\u0026gt; #\u0026lt;Post:0x007fcde2587d40 id: 1, title: \u0026#34;Rails.cache.fetch scope 的\u0026#39;坑\u0026#39;\u0026#34;, body: \u0026#39;\u0026#39;, created_at: Fri, 03 Jan 2015 23:55:44 CST +08:00, updated_at: Thu, 21 Jul 2015 08:00:20 CST +08:00\u0026gt;   每次都要先 read 再 write 比较繁琐, 所以, 就有了一个方便些的方法 Rails.cache.fetch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [7] pry(main)\u0026gt; Rails.cache.fetch(\u0026#39;cache_first_post\u0026#39;, expires_in: 2.minute) { Post.first } Post Load (0.4ms) SELECT `posts`.* FROM `posts` ORDER BY `posts`.`id` ASC LIMIT 1 =\u0026gt; #\u0026lt;Post:0x007fcde2474fe8 id: 1, title: \u0026#34;Rails.cache.fetch scope 的\u0026#39;坑\u0026#39;\u0026#34;, body: \u0026#39;\u0026#39;\u0026gt; [9] pry(main)\u0026gt; Rails.cache.fetch(\u0026#39;cache_first_post\u0026#39;, expires_in: 2.minute) { Post.first } =\u0026gt; #\u0026lt;Post:0x007fcde228afc0 id: 1, title: \u0026#34;Rails.cache.fetch scope 的\u0026#39;坑\u0026#39;\u0026#34;, body: \u0026#39;\u0026#39;, created_at: Fri, 03 Jan 2015 23:55:44 CST +08:00, updated_at: Thu, 21 Jul 2015 08:00:20 CST +08:00\u0026gt;   咋看上去都还正常运作的, Rails.cache.fetch 在对应的 key 如果能找到值的时候就不会再去查数据库了.\n再来看下面,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  [10] pry(main)\u0026gt; Rails.cache.fetch(\u0026#39;cache_all_posts\u0026#39;, expires_in: 2.minute) { Post.all } Post Load (4.8ms) SELECT `posts`.* FROM `posts` =\u0026gt; [#\u0026lt;Post:0x007fcddc2dfc38 id: 1, title: \u0026#34;Rails.cache.fetch scope 的\u0026#39;坑\u0026#39;\u0026#34;, body: \u0026#39;\u0026#39;, created_at: Fri, 03 Jan 2015 23:55:44 CST +08:00, updated_at: Thu, 21 Jul 2015 08:00:20 CST +08:00\u0026gt;, #\u0026lt;Post:0x007fcddfab0e28 id: 2...]\u0026gt; [11] pry(main)\u0026gt; Rails.cache.fetch(\u0026#39;cache_all_posts\u0026#39;, expires_in: 2.minute) { Post.all } Post Load (4.1ms) SELECT `posts`.* FROM `posts` =\u0026gt; [#\u0026lt;Post:0x007fb96b6a3c50 id: 1, title: \u0026#34;Rails.cache.fetch scope 的\u0026#39;坑\u0026#39;\u0026#34;, body: \u0026#39;\u0026#39;, created_at: Fri, 03 Jan 2015 23:55:44 CST +08:00, updated_at: Thu, 21 Jul 2015 08:00:20 CST +08:00\u0026gt;, #\u0026lt;Post:0x007f9a7fd787e8 id: 2,   即便第一次看起来做了缓存(其实也的确有缓存文件在 tmp/cache/ 下面), 但是, 当再次执行 Rails.cache.fetch 的时候, 还是去数据库查了. 实际使用中可能不会这么干把所有 posts 都缓存(Redis/Memcached)到内存里面, 那样的话供给缓存用的内存估计瞬间就被榨干了. 但是, 这里数据库又再去查了一次感觉不科学啊.\n搜了下 Stack Overflow, 说是, User.where('status = 1').limit(1000) Post.all 这些, 返回的实际上是 scope, 并非真正的查询. cache 的是 scope 不是查询结果.\n换成 Rails.cache.fetch('cache_all_posts', expires_in: 2.minute) { Post.all.load } 以后, 就跟预想的一样工作了. 但是, 查看(ActiveSupport::Cache::FileStore)了一下缓存文件, 当缓存 scope 时候, 文件大小为 680 bytes, 而当缓存查询结果的时候, 文件的大小是 MB 级别的.\nActiveRecord::Relation 究竟是什么鬼? 1 2 3  [7] pry(main)\u0026gt; User.where(id: 1) User Load (6.9ms) SELECT `users`.* FROM `users` WHERE `users`.`id` = 1 =\u0026gt; [\u0026lt;User:0x007fb96ee98b88 id: 1, email: \u0026#34;test@test.com\u0026#34;\u0026gt;]   看上去返回的结果很像是数组, 实际上, 却不是数组.\n1 2 3  [8] pry(main)\u0026gt; _.class =\u0026gt; User::ActiveRecord_Relation(Rails 4) =\u0026gt; ActiveRecord::Relation(Rails 3)   btw, Rails 3 中执行 all 是返回 Array的, 而 Rails 4 则是 Relation\nActiveRecord::Relation 只有当真正需要知道并使用到里面所包含的对象时候才会被执行. 比如在 controller 中:\n1 2 3 4 5 6 7  class PostsController \u0026lt; ApplicationController def index @posts = Post.all @channels = Channel.all @comments = Comment.all end end   假设在 index.html.erb 中, 没有任何地方用到 @comments 的话, 其实是不会执行数据库查询去把所有 comments 抓出来的. 而如果先使用到了 @channels, 比如 @channels.first.name 的话, 也是一样, 先执行\n1  Channel Load (27.8ms) SELECT `channels`.* FROM `channels`   再去执行(假设 view 里面有使用到 @posts)\n1  Post Load (47.2ms) SELECT `posts`.* FROM `posts`   所以, Rails.cache.fetch('cache_all_posts', expires_in: 2.minute) { Post.all } 缓存的只是 Relation 对象而不是查询结果. 其实分开来, 使用 Rails.cache.read 和 Rails.cache.write 也是这样的. ╮(╯_╰)╭ 说是坑, 好吧, 其实还是没透彻地弄清楚 Rails\n","permalink":"https://xguox.me/rails-cache-fetch-scope.html/","tags":["Ruby"],"title":"Rails.cache.fetch Relation 的'坑'"},{"categories":["Ruby","Elasticsearch"],"contents":" 上一篇扯了一大通都只是 Elasticsearch 的安装配置, 现在扯点集成到 Rails 上的东西.\nelasticsearch-rails 这个 Repository 是由三个 Gem 组成,\n1 2 3  gem \u0026#39;elasticsearch-model\u0026#39; gem \u0026#39;elasticsearch-rails\u0026#39; gem \u0026#39;elasticsearch-persistence\u0026#39;   总觉得这里起名略蛋疼 = . = 一般用到前两个较多.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class Post \u0026lt; ActiveRecord::Base include Elasticsearch::Model include Elasticsearch::Model::Callbacks # settings do # mappings dynamic: \u0026#39;false\u0026#39; do # indexes :title, type: \u0026#39;string\u0026#39;, analyzer: \u0026#39;ik_smart\u0026#39; # indexes :keywords, type: \u0026#39;string\u0026#39;, analyzer: \u0026#39;ik_smart\u0026#39; # indexes :body, type: \u0026#39;string\u0026#39;, analyzer: \u0026#39;ik_smart\u0026#39; # indexes :user_name, type: \u0026#39;string\u0026#39;, analyzer: \u0026#39;ik_smart\u0026#39; # end # end # def as_indexed_json(options={}) # as_json( # only: [\u0026#39;title\u0026#39;, \u0026#39;body\u0026#39;, \u0026#39;keywords\u0026#39;], # methods: [:user_name] # ) # end end   Elasticsearch::Model::Callbacks 这个模块主要是当 model 更新以后回调更新索引. 对于厂里的 CRM 来说, 因为是数据仓库, 更多是做一些查询, 分析之类的应用, 而不是主要应用于增删改查, 所以, 其实我们没有 include 这个模块. 手工执行一下 Post.import 基本工作就完成了.\n也可以只 import 特定 scope 或者查询下面的记录.\n1 2 3  Post.import scope: \u0026#39;published\u0026#39; # Post.import query: -\u0026gt; { where(user_id: user_id) }   title, keywords, body 是 posts 表的字段, user_name 是 Post 类的一个方法.\n上面用了 settings 设置映射以后还要定义 as_indexed_json, 否则该方法来自 Elasticsearch::Model::Serializing 默认实现(只)会序列化所有原有字段(比如包括 created_at, updated_at 等). settings 设置也不会起作用.\nmappings 方法的参数 dynamic: 'false' 是对文档新增 field 的处理, 默认为 true, 也就是会动态判断该 field 的类型, 并添加这个 field, 而设置为 false 的话, 如果有新的 field 被传进来, 则会被无视之. 不会改变文档的 _source. _source 仍然是只包含已经索引的整个 JSON 文档, 任何新来的 field 都不会被添加到映射, 也不会被搜索到.\n1 2 3 4 5 6 7 8 9 10 11  [5] pry(main)\u0026gt; Post.search(\u0026#39;blahblahblah\u0026#39;).records.all Post Search (11.9ms) {index: \u0026#34;posts\u0026#34;, type: \u0026#34;post\u0026#34;, q: \u0026#34;blahblahblah\u0026#34;} Post Load (0.2ms) SELECT `posts`.* FROM `posts` WHERE 1=0 =\u0026gt; [] [6] pry(main)\u0026gt; Post.search(\u0026#39;cool\u0026#39;).records.all Post Search (12.8ms) {index: \u0026#34;posts\u0026#34;, type: \u0026#34;post\u0026#34;, q: \u0026#34;cool\u0026#34;} Post Load (0.4ms) SELECT `posts`.* FROM `posts` WHERE `posts`.`id` IN (13, 108) =\u0026gt; [#\u0026lt;Post:0x007f8819c2d690 id: 13, title: \u0026#34;Peanut and Peach nut\u0026#34;, body:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  [10] pry(main)\u0026gt; resp = Post.search(\u0026#39;一张照片\u0026#39;) =\u0026gt; #\u0026lt;Elasticsearch::Model::Response::Response:0x007fcc13200008 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @search= #\u0026lt;Elasticsearch::Model::Searching::SearchRequest:0x007fcc13200238 @definition={:index=\u0026gt;\u0026#34;posts\u0026#34;, :type=\u0026gt;\u0026#34;post\u0026#34;, :q=\u0026gt;\u0026#34;一张照片\u0026#34;}, @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @options={}\u0026gt;\u0026gt; [11] pry(main)\u0026gt; resp.records =\u0026gt; #\u0026lt;Elasticsearch::Model::Response::Records:0x007fcc11719ca8 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @response= #\u0026lt;Elasticsearch::Model::Response::Response:0x007fcc117c93b0 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @records=#\u0026lt;Elasticsearch::Model::Response::Records:0x007fcc11719ca8 ...\u0026gt;, @search= #\u0026lt;Elasticsearch::Model::Searching::SearchRequest:0x007fcc117c8578 @definition={:index=\u0026gt;\u0026#34;posts\u0026#34;, :type=\u0026gt;\u0026#34;post\u0026#34;, :q=\u0026gt;\u0026#34;一张照片\u0026#34;}, @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @options={}\u0026gt;\u0026gt;\u0026gt;  1 2 3 4 5 6 7 8 9 10 11 12 13  [9] pry(main)\u0026gt; resp.records.first Post Load (0.8ms) SELECT `posts`.* FROM `posts` WHERE `posts`.`id` IN (61, 4, 82, 15, 90, 72, 114, 112, 28, 17) =\u0026gt; #\u0026lt;Post:0x007fcc18c23238 id: 61, title: \u0026#34;让布列松看不顺眼的摄影师\u0026#34;, body: \u0026#34;BlahBlahBlahBlah...........\u0026#34;, user_id: 1, slug: \u0026#34;bdgh2Lyf4z3jF\u0026#34;, created_at: Mon, 10 Feb 2015 12:51:04 CST +08:00, updated_at: Tue, 09 Aug 2015 08:00:07 CST +08:00, comments_count: 0, keywords: \u0026#34;Photo 摄影 布列松\u0026#34;, popular: false\u0026gt;  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  [12] pry(main)\u0026gt; resp.results =\u0026gt; #\u0026lt;Elasticsearch::Model::Response::Results:0x007fcc11693568 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @response= #\u0026lt;Elasticsearch::Model::Response::Response:0x007fcc117c93b0 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @records= #\u0026lt;Elasticsearch::Model::Response::Records:0x007fcc11719ca8 @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @response=#\u0026lt;Elasticsearch::Model::Response::Response:0x007fcc117c93b0 ...\u0026gt;\u0026gt;, @results=#\u0026lt;Elasticsearch::Model::Response::Results:0x007fcc11693568 ...\u0026gt;, @search= #\u0026lt;Elasticsearch::Model::Searching::SearchRequest:0x007fcc117c8578 @definition={:index=\u0026gt;\u0026#34;posts\u0026#34;, :type=\u0026gt;\u0026#34;post\u0026#34;, :q=\u0026gt;\u0026#34;一张照片\u0026#34;}, @klass=[PROXY] Post (call \u0026#39;Post.connection\u0026#39; to establish a connection), @options={}\u0026gt;\u0026gt;\u0026gt; [8] pry(main)\u0026gt; resp.results.first Post Search (39.4ms) {index: \u0026#34;posts\u0026#34;, type: \u0026#34;post\u0026#34;, q: \u0026#34;一张照片\u0026#34;} =\u0026gt; #\u0026lt;Elasticsearch::Model::Response::Result:0x007fcc18d44270 @result= {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;posts\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;61\u0026#34;, \u0026#34;_score\u0026#34;=\u0026gt;0.21295425, \u0026#34;_source\u0026#34;=\u0026gt; {\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;让布列松看不顺眼的摄影师\u0026#34;, \u0026#34;body\u0026#34;=\u0026gt; \u0026#34;BlahBlahBlahBlah...........\u0026#34;, \u0026#34;keywords\u0026#34;=\u0026gt;\u0026#34;Photo 摄影 布列松\u0026#34;, \u0026#34;user_name\u0026#34;=\u0026gt;\u0026#34;XguoX\u0026#34;}}\u0026gt;   用 results 返回的是对应的 JSON, 而用 records 返回的则是经过数据库查询的记录. 毫无疑问, 使用 results 的话性能会好一些.\n查看某个 model, 比如 Post 的映射配置 curl 'http://localhost:9200/posts?pretty'\n返回结果 对比一下, 同样的关键词使用 ik, ik_smart 以及使用标准 anlayser 之间分词区别. 索引文档之间的关联关系, 假设 Post has_many Comments. 可以像下面这样进行序列化:\n1 2 3 4 5 6 7  def as_indexed_json(options={}) as_json( only: [\u0026#39;title\u0026#39;, \u0026#39;body\u0026#39;, \u0026#39;keywords\u0026#39;], methods: [:user_name], include: {comments: {only: :content}} ) end  1 2 3 4 5 6 7 8 9  [19] pry(main)\u0026gt; Post.last.__elasticsearch__.as_indexed_json Post Load (0.3ms) SELECT `posts`.* FROM `posts` ORDER BY `posts`.`id` DESC LIMIT 1 User Load (0.2ms) SELECT `users`.* FROM `users` WHERE `users`.`id` = 1 LIMIT 1 Comment Load (0.2ms) SELECT `comments`.* FROM `comments` WHERE `comments`.`post_id` = 61 =\u0026gt; {\u0026#34;title\u0026#34;=\u0026gt;\u0026#34;让布列松看不顺眼的摄影师\u0026#34;, \u0026#34;body\u0026#34;=\u0026gt;\u0026#34;blahblahblah.......\u0026#34;, \u0026#34;keywords\u0026#34;=\u0026gt;\u0026#34;Photo 摄影 布列松\u0026#34;, \u0026#34;user_name\u0026#34;=\u0026gt;\u0026#34;XguoX\u0026#34;, \u0026#34;comments\u0026#34;=\u0026gt;[{\u0026#34;content\u0026#34;=\u0026gt;\u0026#34;Here is 评论...\u0026#34;}]}   更改了映射设置以后要更新索引,\n1 2  Post.__elasticsearch__.create_index! force: true Post.__elasticsearch__.refresh_index!   手动更新单个文档\n1 2 3 4  [8] pry(main)\u0026gt; Post.first.__elasticsearch__.index_document Post Load (0.4ms) SELECT `posts`.* FROM `posts` ORDER BY `posts`.`id` ASC LIMIT 1 User Load (0.4ms) SELECT `users`.* FROM `users` WHERE `users`.`id` = 2 LIMIT 1 =\u0026gt; {\u0026#34;_index\u0026#34;=\u0026gt;\u0026#34;posts\u0026#34;, \u0026#34;_type\u0026#34;=\u0026gt;\u0026#34;post\u0026#34;, \u0026#34;_id\u0026#34;=\u0026gt;\u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;=\u0026gt;2, \u0026#34;created\u0026#34;=\u0026gt;false}   Related: Elasticsearch 开箱笔记\nElasticsearch More Like This 搜索\nElasticsearch Aggregations 聚合分析\nUpgrade Elasticsearch to 2.3\nElasticsearch Scroll (Ruby)\nElasticsearch analysis \u0026amp; 自定义 analyzers\nElasticsearch 如何不用停机情况下完成 mapping 的修改\n","permalink":"https://xguox.me/elasticsearch-rails.html/","tags":["Ruby","Elasticsearch"],"title":"Elasticsearch on Rails"},{"categories":["MySQL"],"contents":"1  TRUNCATE [TABLE] tbl_name   TRUNCATE 语句可以用来清空一个表格里面的数据.\n跟使用 DELETE 语句来删除所有行相似, 又或者也跟 DROP TABLE + CREATE TABLE 连用的效果相似.\n为了达到更高的性能, 它绕过了那些删除数据的 DML(数据操纵语言). 尽管跟 DELETE 相似, 不过, 它是被归类为 DDL(数据定义语言), 在 MySQL 5.6 中, 和 DELETE 语句的区别如下:\n Truncate 操作是直接 drop 了以后重新 create 的, 比一条一条的删除快得多, 尤其数据表非常大的时候更明显. Truncate 操作是不能被回滚的. 被锁着表不能执行 Truncate 操作 Truncate 操作不会返回一个数值表示有多少行被删除了. 通常返回的结果都是 0 rows affected 所有的 AUTO_INCREMENT 的值都会被重置为起始值. Truncate 操作不会调用 ON DELETE 触发器 (对触发器不熟悉 = . =) 如果数据表有外键约束是不能执行 Truncate 操作的.  ","permalink":"https://xguox.me/mysql-truncate-vs-delete.html/","tags":["MySQL"],"title":"MySQL 的 TRUNCATE"},{"categories":["Elasticsearch"],"contents":" 和 Solr 一样, Elasticsearch 是一个基于全文搜索引擎 Apache Lucene\u0026trade; 基础上的搜索引擎. 没怎么用 Solr, 所以不好做各方面比较. 而厂里的 CRM 系统用的就是 Elasticsearch. Elasticsearch 不仅可以支持全文搜索, 同时也支持类似关系数据库的结构化查询. 它可以让你搜索并利用那些在数据库中难以查询的数据. Elasticsearch 还可以做聚合(aggregations), 可以在数据上进行复杂的分析统计.\nElasticsearch 的 RESTful API是基于 HTTP 协议, 以 JSON 作为数据格式的. 所以, 经常写出, 语法上经常写出跟 JavaScript 那样一大串开闭括号.\n形如:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  { \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch_xguox\u0026#34;, \u0026#34;nodes\u0026#34; : { \u0026#34;8fM1_dseTUWL74TiDTCPiA\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;Thinker\u0026#34;, \u0026#34;transport_address\u0026#34; : \u0026#34;inet[/127.0.0.1:9300]\u0026#34;, \u0026#34;host\u0026#34; : \u0026#34;XguoXs-MacBook-Pro.local\u0026#34;, \u0026#34;version\u0026#34; : \u0026#34;1.6.0\u0026#34;, \u0026#34;build\u0026#34; : \u0026#34;cdd3ac4\u0026#34;, \u0026#34;http_address\u0026#34; : \u0026#34;inet[/127.0.0.1:9200]\u0026#34;, \u0026#34;settings\u0026#34; : { \u0026#34;path\u0026#34; : { \u0026#34;data\u0026#34; : \u0026#34;/usr/local/var/elasticsearch/\u0026#34;, \u0026#34;logs\u0026#34; : \u0026#34;/usr/local/var/log/elasticsearch\u0026#34;, \u0026#34;plugins\u0026#34; : \u0026#34;/usr/local/var/lib/elasticsearch/plugins\u0026#34;, \u0026#34;home\u0026#34; : \u0026#34;/usr/local/Cellar/elasticsearch/1.6.0\u0026#34; }, \u0026#34;cluster\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;elasticsearch_xguox\u0026#34; }, \u0026#34;name\u0026#34; : \u0026#34;Thinker\u0026#34;, \u0026#34;index\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;ik_smart\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;ik\u0026#34;, \u0026#34;use_smart\u0026#34; : \u0026#34;true\u0026#34; }, \u0026#34;ik\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;org.elasticsearch.index.analysis.IkAnalyzerProvider\u0026#34;, \u0026#34;alias\u0026#34; : [ \u0026#34;ik_analyzer\u0026#34; ] }, \u0026#34;ik_max_word\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;ik\u0026#34;, \u0026#34;use_smart\u0026#34; : \u0026#34;false\u0026#34; } } } }, \u0026#34;client\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;node\u0026#34; }, \u0026#34;foreground\u0026#34; : \u0026#34;yes\u0026#34;, \u0026#34;config.ignore_system_properties\u0026#34; : \u0026#34;true\u0026#34;, \u0026#34;config\u0026#34; : \u0026#34;/usr/local/Cellar/elasticsearch/1.6.0/config/elasticsearch.yml\u0026#34;, \u0026#34;script\u0026#34; : { \u0026#34;inline\u0026#34; : \u0026#34;on\u0026#34;, \u0026#34;indexed\u0026#34; : \u0026#34;on\u0026#34; }, \u0026#34;network\u0026#34; : { \u0026#34;host\u0026#34; : \u0026#34;127.0.0.1\u0026#34; } } } } }    而且, 经常是嵌套的很深, 不是漏了逗号就是冒号或者开闭大括号 ヾ(´･ ･｀｡)ノ\u0026rdquo;\n好吧, 其实上面那段不是正规的数据格式, 只是执行\ncurl \u0026quot;localhost:9200/_nodes/settings?pretty=true\u0026quot;\n得到的结果. 主要是安装完 Elasticsearch 后的一些相关配置信息等等.\n安装 Mac OS X 上直接用 homebrew 就可以 install 了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  Distributed search \u0026amp; analytics engine https://www.elastic.co/products/elasticsearch /usr/local/Cellar/elasticsearch/1.6.0 (34 files, 29.6M) * Built from source From: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/elasticsearch.rb ==\u0026gt; Caveats Data: /usr/local/var/elasticsearch/elasticsearch_xguox/ Logs: /usr/local/var/log/elasticsearch/elasticsearch_xguox.log Plugins: /usr/local/Cellar/elasticsearch/1.6.0/libexec/plugins/ Config: /usr/local/etc/elasticsearch/ plugin script: /usr/local/Cellar/elasticsearch/1.6.0/libexec/bin/plugin To reload elasticsearch after an upgrade: launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist launchctl load ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist Or, if you don\u0026#39;t want/need launchctl, you can just run: elasticsearch   VPS(Ubuntu 14.04) 上可以参考这里\nBTW,\nelasticsearch.yml 里面的 network.host: 127.0.0.1\n默认的 Elasticsearch 是绑定了只允许本地 127.0.0.1 访问的, 在本地或者一般的生产环境足够了, 但是, 如果是一个集群跑在多个服务器的话就要在这里设置.\n添加插件 中文分词 Elasticsearch 默认的分词对英语支持的已经挺好的, 但是对中文的分词支持却很渣. 貌似几乎没分词可言. 默认的standard analyser 直接拆分成单个字了. 有个 smartcn 的 analyser 评价也一般.\n所以, 用的比较多的插件是 ik 和 mmseg. 对应的 Readme 都介绍的挺详细的.\n注意拷贝词库就是了, 之前就是搞了个乌龙折腾半天. 直接把 mmseg 或者 ik 整个拷到跟 elasticsearch.yml 同一层目录下就行 = . =\nUpdate, 新版不用配置直接解压把文件拷好就可以用了. 然后配置一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  index:analysis:analyzer:ik:alias:[ik_analyzer]type:org.elasticsearch.index.analysis.IkAnalyzerProviderik_max_word:type:ikuse_smart:falseik_smart:type:ikuse_smart:true# index:# analysis:# analyzer:# mmseg:# alias: [news_analyzer, mmseg_analyzer]# type: org.elasticsearch.index.analysis.MMsegAnalyzerProvider# index.analysis.analyzer.default.type : \u0026#34;mmseg\u0026#34;# index:# analysis:# tokenizer:# mmseg_maxword:# type: mmseg# seg_type: \u0026#34;max_word\u0026#34;# mmseg_complex:# type: mmseg# seg_type: \u0026#34;complex\u0026#34;# mmseg_simple:# type: mmseg# seg_type: \u0026#34;simple\u0026#34;   几乎全都是开箱即用的节奏 Σ(￣。￣ノ)ノ\nRelated: Elasticsearch on Rails\nElasticsearch More Like This 搜索\nElasticsearch Aggregations 聚合分析\nUpgrade Elasticsearch to 2.3\nElasticsearch Scroll (Ruby)\nElasticsearch analysis \u0026amp; 自定义 analyzers\nElasticsearch 如何不用停机情况下完成 mapping 的修改\n","permalink":"https://xguox.me/elasticsearch-ik-mmseg-homebrew-ubuntu.html/","tags":["Elasticsearch"],"title":"Elasticsearch 开箱笔记"},{"categories":["Jabber"],"contents":"最经常被炮哥说的一句就是, \u0026ldquo;看错误提示啊\u0026rdquo;\n今天又是一个奇葩问题, 同样的代码, 居然正式环境上正常的跑, 但是本地一直报错.\n原因原来是 cells 相应的 views 里面的 helper 要自己在 cell class 里面 include.\n但是, 至今仍然奇葩, 为什么没有任何改动, 在正式环境毫无压力没有报错的跑, 但是我自己本地却各种崩.\n原本在 better_errors 里面如果自己仔细找问题应该能发现的, 只不过, 因为看到正式环境各种正常, 深感奇葩, 遂没怎么查错就提问了.\n印象中好像类似不看错误提示信息的情况还有发生过.\n= . =\n","permalink":"https://xguox.me/ask-intelligently.html/","tags":["Jabber"],"title":"提问的智慧"},{"categories":["Ruby"],"contents":"推荐 Rubyist 看 \u0026lt;理解Unix进程\u0026gt;,\n至少, 像我这种不是科班出身的, 可以好好补补一些计算机系统的那些基础知识.\n虽然还没看完, 虽然书中的例子都是用 Ruby 去写的, 有一些章节理解起来还是有些费力.\n前些天看到个好玩的, 在 irb 里面输入\n1 2 3 4 5  if fork puts \u0026#34;entered the if block\u0026#34; else puts \u0026#34;entered the else block\u0026#34; end   输出结果: if 和 else 都执行了.\n通过\nps -ef | grep irb\n可以看到有两个 irb 进程.\n更直接一点, 退出 irb 重新执行:\n1 2 3 4 5 6 7  puts \u0026#34;parent process pid is #{Process.pid}\u0026#34; if fork puts \u0026#34;entered the if block from ##{Process.pid}\u0026#34; else puts \u0026#34;entered the else block from ##{Process.pid}\u0026#34; end    if 语句块中的代码由父进程执行的, 而 else 语句块中的代码是子进程执行的. 子进程执行完后退出, 父进程则继续执行.\n 不过, 这里就纳闷了, 子进程没有退出吧, ps 还是出来两个 irb, 而且, 如果想继续输入代码是得隔着一个按键敲的, 比如要退出的话是 敲 eexxiitt.\n= . =\nAnyway, 说到底其实是 Ruby 里的 fork 用法.\n","permalink":"https://xguox.me/ruby-unix-fork.html/","tags":["Ruby"],"title":"理解 Unix 进程"},{"categories":["Jabber"],"contents":"荒废了许久, 不过这儿没有杂草丛生. 老早前还说总结 2014 的. 还有早之前参加 2014 RubyConf China.\n从上一篇的北上回来, 就把这里给落了. 即便至今也没有信心能坚持再拾起这里.\n连流水账都没有写的欲望了? 这半年的过程, 堪比患病的那些日子, 有过之无不及.\n自己也好奇, 会写些什么在这里, 什么东西一定不会写这里呢?\n想再出去远门散步.\n","permalink":"https://xguox.me/2015-first-post.html/","tags":["Jabber"],"title":"2015 first post"},{"categories":["Photo"],"contents":" 喜欢散步这个词, 是从 Aokiji 那儿学来的, 懒散的正义! 该逗比的时候毫无大将之风, 该流弊的时候却流弊的轰轰.\n这一次是散到了南京. 定南京是因为这里交通方便, 飞机能进, 高铁动车各种出. 也觉得旅游景点较多, 还有一个理由是, 我即将要去北京(不是散步). 又南又北的, 听着都顺口些.\n两年了, 我应该把这当成一个纪念日吗? 听起来, 两年没出过大广东也没啥大不了, 感伤个什么呢? 这不好好的吗!\n这一次的飞机很神奇的异常准点, 下机时, 同乘的人儿各种称赞, 据说还提前到了. 只是, 大广州还穿着短袖短裤的飞过来, 这边是下着阴冷小毛雨的节奏. 赶紧的裹上了包里的外套才稍微暖和了些. 虽说做好准备这边要冷不少, 但也落差太大了些.一定是没预到的雨的缘故. 即便计划得好好的, 却也还是忽略了不少东西. (比如没查好各种路线地铁公交. 这都没查好也叫准备好了?)\n大南京给我的第一感觉, 第一印象还是不错的.\n从禄口机场直接奔到莫愁湖这边入住下来. 莫愁, 莫愁, 我会说我故意选这个作为起始点的吗?\n洗洗睡睡, 一觉醒来才开始一整天的漫步行程.\n起先还担心着要下雨, 结果吃早餐时候发现, 天气还行, 虽然阴阴的. 结果吃完早餐, 就不用担心了, 因为真的下雨了. 好吧, 买把伞走起呗.\n围绕着莫愁湖散一圈. 刚开始, 步伐或许是因为雨而有些急凑, 走过凉亭时候看到一些老年人在那悠哉的晨练着, 才意识到问题.我这哪门子是散步了. 于是, 缓了下来. 整个莫愁湖边, 平均年龄应该有个50+吧. 瞬间觉得自己格格不入, 但从意识到步伐问题后心情就改了不少.\n因为雨, 我的右鞋子几乎成了水鞋了. 但是, 非常神奇的是, 我的左鞋子居然没湿半分. 这科学吗?\n进食与心情有关, 行走也跟心情有关. 换作以往, 我这一定要郁闷自己了. 这大老远的跑来散步你居然下雨, 还把鞋子整湿的整个人都不舒服了. 要真那么想我这一趟步可以不用散了. 这不选莫愁湖开始吗, 就这么点儿事就愁了那没戏了. 想想, 也就真心无怨言的愉快的转悠完.\n经过一群老人家的时候听着他们放着那些我也知道的老歌曲是会心的笑了, 我其实真不知道自己为什么会笑. 比如那首「千年等一回\u0026hellip;\u0026hellip;..」傻傻的路过也跟着哼, 还有好一些叫不出名儿的歌曲.\n这样的天气还能那么多人那么积极的在起舞晨练, 厉害! 话说, 这只逗比的小黄鸭是哪来的. 之前好像火过一会来着是吧. 转转悠完理些事儿(其实整了我老久, 因为不认识路)刚好饭点. 在莫愁湖边的内家叫「老滋老味」的店吃了碗素什锦面和6个小笼汤包, 妥妥的把我撑饱了, 不管味道怎么着也给个好评吧!XD 下一站, 中山陵 到了中山陵, 我第一时间怀念的是早上的雨. 不为啥, 因为人多了. 我就奇怪了, 这也还是工作日啊, 就这么多人, 那周末是啥情况? 放大假的时候又是啥情况啊, 不敢想象. 想拍些照, 发现怎么都躲不开人. 最后想到一个不是办法的办法. 既然无法当主题摄, 那就好好的在焦外衬托吧. 上去一转下来, 买了张旁边的「音乐台」的门票进去了, 放的都是一些熟悉的叫不上名儿的纯音乐, 水柱随着节奏有规律的起伏. 少许一些人儿拿着鸽食在逗着一大群的鸽子.\n而我坐着在正中间的台阶(早上下完雨的, 你PP不湿吗?), 应该不是困了想午睡, 估计是zhuangbility属性又犯了. 就那样看着前方的水柱, 或者那些飞舞起来的鸽子, 发呆.\n伴着音乐又读了一遍信.\n上下中山陵应该也就半个钟左右, 但是花在音乐台发呆的时间估计是两倍.\n离开时, 在总统府下车, 原本是计划在内的一个点, 但是忽然又不想去了. 于是直接略过, 转车到了夫子庙.\n更多人的, 夫子庙 像乌镇的古, 像上下九的商. 还有一座夫子庙.综述完毕 XD\n吃小食的地不少, 不过我绝对不是吃货, 所以无爱.\n估计也是累了, 没太用心的去欣赏风景. 原本准备还去玄武湖的, 因为离晚上动车的南京站很近的.但是, 还是由于累了, 对于没有午觉碎的我来说, 能坚持完夫子庙已经该给自己大拇指了吧.\n身上的电各种不够用啊, 只有一根数据线, 要充电的有相机, 4G终端, 手机. 最悲剧的还是要靠我背后的Macbook 来换电!!! 反正我傻逗了.\n困了, 写到这好了.\n","permalink":"https://xguox.me/one-day-in-nanjing.html/","tags":["Photo"],"title":"One day in Nanjing"},{"categories":["Jabber"],"contents":"应该会是这么多年以来印象最深刻的一个国庆长假(这不扯淡吗? 刚刚过去还能不印象深刻?). 有些事情就是那么的奇怪, 如果我当初了解多了, 或者多问了两句, 兴许早就打了退堂鼓, 然后这一切也就没发生了.\n不准备把那薄薄的一本搬到这, 只想说些这两天回来没有做到的.\n还是坚持继续吃多了一天素食, 而且还是自己动手的, 虽然很好, 但是, 没有吃完, 有浪费! 是因为没有念餐前感恩词了吗?明天可能还要吃回肉食了!\n早起读《弟子规》就更没了, 别说读了, 连早起早睡都打回原形的样子.\n虽然和家人沟通时候有了些打心底改善的态度, 但是心中的孝好像比起在上课时消退了几分, 感觉.\n最要紧的是, 回到 coding 感觉就不成人样了?\n","permalink":"https://xguox.me/things-fail-to-do.html/","tags":["Jabber"],"title":"只说没有做到的"},{"categories":["Jabber"],"contents":"明儿要貌似要发布 iPhone 6, 不关我事, 我买不起. 不过每每这个时候看到某些关于 Apple 的言论会让我这个伪谷粉觉得蛋疼, 但自己才疏学浅, 偶见这则评论, 不能同意更多了.\n原文来自V2EX, 我是写不出的了, 点赞之余还得搬过来 repost 再赞一次\n我就知道有人要这么说： 10.10好丑啊，iOS 7就够TM丑的了，OS X还学iOS 7！无语！难看！还是10.9好用！\n之前我还听过别人这么说： 10.9好没创新啊，竟然把iOS上的通知中心搞进来了，界面也没什么突破，没劲！还叫什么Mavericks？记不住！还是10.8好！10.9免费的？不用！\n还听过别人这么说： 10.8太！难！看！了！图标怎么是单色的了！设计师是色盲吗！还是10.7的图标养眼！\n还有人这么说过： 10.7那gay gay的背景是肿！么！回！事！按钮怎么连水晶效果都没有了！我了个去！果断降级，还是10.6的界面经典啊！\n也有人这么说： 10.6完全没心意啊！最低竟然要1G的内存了，就知道骗钱！更新个iChat加上一点小改动就能继续骗钱了！我靠！我发布会前一周买的新本本升级10.6竟然还要我10刀？！坑爹！继续用10.5！\n这么说的也有： 10.5这界面太！脑！残！了！又要加内存了！BootCamp？？运行Windows？我靠low！爆！了！Time Machine？脑！残！才！用！10.4好顶赞！10.4大法好！\n当然了，最后一条是我编的，这帮人我见过的好像还没有从10.4就开始用Mac的\n每当看到这样的用户吐槽，我都会感到蛋蛋的忧伤，按照这帮人的逻辑来理解系统的好坏那就是这样：\n10.10 \u0026lt; 10.9 \u0026lt; 10.8 \u0026lt; 10.7 \u0026lt; 10.6 \u0026lt; 10.5 \u0026lt; 10.4 \u0026lt; 10.3 \u0026lt; 10.2 \u0026lt; 10.1 \u0026lt; 10.0 \u0026lt; Mac OS 9 \u0026lt; Mac OS 8 \u0026lt; Mac OS 7 \u0026lt; 6 \u0026lt; 5 \u0026lt; 4 \u0026lt; 3 \u0026lt; 2 \u0026lt; 1\n总之就是一代不如一代好用，推算下来的话，1984年设计出来的Mac OS就是极好的，可以满足一切审美需求和日常办公需求\n同理不光是Mac OS，iOS也有这种奇怪的思维，看看iOS 8刚出来的反应： 卧槽iOS 8还TM那么难看！这TM不是安卓吗！抄袭啊，图标也丑爆了！这gay gay的颜色。。我要做个Gayrident的网站羞辱iOS！真low，这样的iOS我还好意思用吗！我要用Windows Phone！槽点太多，吐不过来了，总之一个字：难看！啥？你问我什么是好看的？虽然我也没什么审美啊，这个我也不知道啊。。反正就是不！喜！欢！负分！\n记得iOS 7刚出来的时候： 惊！天！大！槽！没了乔布斯大人的苹果已经早已不是当年的Apple(这句是关于 Apple 最恶心的言论, 没有之一)，没了Forstall的Apple怎会重演当年iOS 5的辉煌？Ive你一个做硬件的来做软件，怎么可能做的好！以前iOS中那精细的木纹书架，那高级游轮上的皮质纹理，那随着手机晃动就会变换高光的音量调节按钮，Mike Matas在每个app图标上体现出来的人文情怀。。全都消失的无！影！无！踪！这是iOS设计史上最大的一个败笔，这是整个Apple史上又一次由兴变衰的转折点！这也是整个设计史上的一个大黑点子！哎？股票怎么没迭？脑残粉！都是脑残粉！虽然我不炒股也不太明白。。反正Apple的股票我是不会买！iOS 7我也不会升！iOS 5无疑是一件精美的艺术品，我要等着那帮傻冒纷纷升级iOS，等着下次WWDC某库宣布我们iOS 7的adoption已经达到99.99%，我这台装有iOS 5的iPhone 4s，哦不iPhone 4S，必然可以买到千万美元的价格！\n还记得iPhone 5刚出来的时候： 哎！呀！我！槽！iPhone 4s，哦不 iPhone 4S我又打错了，已成为了乔老爷的遗作，后来者必定要经过数十年甚至上百年才能赶超iPhone 4s，哦不 iPhone 4S我又打错了的经典设计，那玻璃的高冷材质是磨砂材质无法代替的！非一体化的背壳怎么看都是缺乏美感！屏幕竟然TMD加长了，这明显是违背乔帮主生前的意愿的！！(类似上面的那句, 依旧的恶心)虽然我就是一个普普通通的用户审美一般，也没读过什么艺术设计相关的书籍，但是！iPhone 5看起来就是那么的不！顺！眼！iPhone 4s，哦不iPhone 4S我又打错了是极好的，有审美的，有品味的，有情感的，有艺术感的，有人文情怀的，iPhone 5是难看的，冰冷的，麻木的，没有情感的，坏品味的，长度增加了但是粗度不够的，总之，我是绝对不会买iPhone 5的，我要等着那帮傻冒纷纷升级iPhone 5，等着下次WWDC某库宣布我们iPhone 5的adoption已经达到99.99%，我这台装有iOS 5的iPhone 4s，哦不iPhone 4S，必然可以买到千万美元的价格！\n哦iPhone 5s出来的时候也一样： 天！啦！噜！iPhone 5S哦不iPhone 5s我打错了，竟然这么没有心意！土豪金！卧槽公然向着中国人献媚！Touch ID？靠XXX早就有这个功能了好不？你看吧，人家用仪器只需要几秒钟就能破解到你这Touch ID！那设备才几千刀而已！没！创！意！乔老爷离开的这几年，Apple真是越来越。。哎不说了，说多了都是泪！\n总之继续按照这帮人的逻辑来想，他们眼里的系统排位感觉就是这样子了：\niOS 8 \u0026lt; iOS 7 \u0026lt; iOS 6 \u0026lt; iOS 5 \u0026lt; iOS 4 \u0026lt; iPhone OS 3 \u0026lt; iPhone OS 2 \u0026lt; iPhone OS 1\n总之也是一代不如一代好用，这么一圈看下来，也就属2007的的第一代 iPhone OS 1最好用了，你看那烂大街的iPhone 6、iOS 8，虽然我没有iPhone 原型机，没有iPhone 1、iPhone 2、iPhone 3GS，但我有乔老爷的遗作iPhone 4s哦不iPhone 4S，我有没升过级没越过狱的纯净版iOS 6，以后把它传给我儿子，绝对上千万刀上亿刀、在湾区直接买房的节奏！\n说(bian)了这么多，其实吧，我是真心希望每次出来新产品都喷的人做到以下几点：\n Apple完蛋了，Ive做UI就是极烂的，以后手机坏了我也只买二手iPhone 4s哦不iPhone4S Apple完蛋了，他们的股票我绝对不会买，这就是给资本主义送钱！ Apple完蛋了，10.10是邪恶的，我就是10.4的忠实用户，哪怕我升到乔布老爷走之前的10.7，我也不会升级到10.8、10.9、10.10！甚至将来的10.11、10.12！10.1000！这是一个人的审美问题！跟好不好用无关！ Apple完蛋了，iOS必须是用乔布斯大人走之前的iOS 5、用6的都是伪果粉，伪拟物粉！ Apple完蛋了，作为开发者，我停止更新我所有上线了的app，为了抵制iOS 7，所有app均不适配iOS 7，而且所有app全部免费！并在app描述中加上“Hi Steve, I hate iOS 7, RIP.”  其实，大部份认为丑、难看、一代不如一代的人，我觉得对以下几点的认识都不够完善：\n 审美水平不足，不是说他们欣赏不了，而是他们跟不上了时代的潮流（trending） 对的，时代的潮流，设计就是一种时尚，时尚就是会随着时间的变化而改变，随着时代的不同而不同 iOS 6难道不经典吗？经典，但符合时代潮流吗？不符合。女人为什么要换衣服？因为趋势一直在变，iOS 7为什么要“拍扁”？因为设计的趋势也在变，因为跟得上潮流的人看腻了，Apple也是很想要抓住走在潮流最前端的那拨人 Apple不做激进的改动，你们永远看不到iPhone、iPad、也看不到更薄的MacBook Pro和垃圾桶形状的Mac Pro了，你们就天天讨论Android刷什么ROM好用就好了，世界和平 不要因为跟不上潮流而感到恐惧，多了解了解艺术设计相关的文化，提高自身的审美水平，而不是拉低别人的水平到自己的标准上。用流行的话说就是iOS 7、8那么烂，你行你上啊？其实，你也不知道什么是好的  ","permalink":"https://xguox.me/bull-shit-about-jobs-and-apple.html/","tags":["Jabber"],"title":"[repost]怒赞!"},{"categories":["Jabber"],"contents":"不是个喜欢做好事的热心人, 但是今天的正能量却是杠杠的.\n午觉碎醒走去上班的路上, 忽然就飙起倾盆大雨. 走到祈福湖附近的时候, 身旁一个小盆友喊道「叔叔, 可以撑我一下吗?」. 小孩骑着小单车, 一脸都是毫无顾忌的样子. 我第一反应是, 「叔!叔!」, 好凶残地说, 我还没适应这个称呼好伐!!! 第二反应是, 「就我这把伞, 就这雨势, 即便是不撑埋你我也要湿身啊」. 第三反应是, 说「好啊, 你踩得慢些, 小心滑让我跟得上.」\n然后基本上咱是顺路的, 一路上我的左半身湿得能挤出水来, 但是, 我还是很装好人的把雨伞尽量往他身上移, 还一直若无其事的跟他聊聊天扯上几句.\n艾玛, 我都被自己感动了! 想起以前的那些小学生作文, 「记一件难忘的事」 XD\n才发现这儿的 RSS 没起作用, 原来是之前写的 Ruby 的那篇标题直接带着 \u0026amp;, 导致报错, xmlParseEntityRef\n","permalink":"https://xguox.me/something-trivial.html/","tags":["Jabber"],"title":"琐碎杂事一记"},{"categories":["CSS"],"contents":"因为项目是我中途参入进来的, 所以并没有注意到 CSS(Sass) 的组织一直都是用的 Sprockets 的方式去 require, 尽管这期间写了不少的CSS(Sass), 包括后来当添加一些个变量之后才发现不对劲, 老是报错 Sass 里头变量 undefined.\n才知道原来使用链式调用的话, variables or mixins 的作用域只在其定义的文件内.\n而使用 @import 的话则是创建了一个全局的作用域. Guides 看得不够仔细啊!\n","permalink":"https://xguox.me/import-vs-require.html/","tags":["CSS"],"title":"@import vs require"},{"categories":["Jabber"],"contents":"深感无力烧镜头, 于是, 新入了个玩具, AKG Q701 骚绿骚绿的, 他们说, 敢戴着出街是需要强大的勇气的说! 额\u0026hellip; 绝大多数时候我应该不会戴着出街挂着吧. XD\n收快递的时候被这么一大个箱子吓着了 = . = 不就是个耳机撒. 原来还有什么耳机架和耳机包一些赠品, 但是我在下单的时候选的是标配, 没要的这些的. 因为这样价格可以便宜 300+, 而这些东西自己单独去买也不过100块有找而已. 只是不知道卖家为嘛还是送了给我, 所以的确不是啥值钱的东西, 但是, 还是表示多谢啦.\n小兴奋所以, 没拍好 =. = 不是过曝就欠曝的\n线也是骚绿骚绿的!\n这根6.3mm 的接口这是要接哪去呢?\n自知是个木耳朵, 比起手机自带的耳机, 能听出的区别仅仅是那么一点, 价格上的大差距反正我是没听出来. 不知道是不是传言呀煲机几百个小时才见效. 但是, 佩戴舒适感是手机内个入耳式无法比的. 再一个, 入耳式的弄脏了很蛋疼.\n据说这货要各种流弊的前端才能推得动, 感觉我从相机坑出来, 又掉进耳机坑了, 暂且不会入各种流弊的 DAC 耳放吧. 权当是个骚爆的能发声的耳机手办好了.\nMio 酱的是 K701, 纠结了一段时间后, 因为骚绿选了Q701 = . =\n","permalink":"https://xguox.me/akg-q701.html/","tags":["Jabber"],"title":"新玩具 AKG Q701"},{"categories":["Ruby"],"contents":"才发现, \u0026amp; 还是一个按位与运算符, 好像应该用才想起, 貌似以前看 \u0026lsquo;Ruby 编程语言\u0026rsquo; 内本书的时候有看到过, 不过感觉比较少用到, 没怎么去记住.\n今个儿看到\n1  14 \u0026amp; 13   输出的是 12, 才发现又得补课了.\n1  \u0026#34;#{14.to_s(2)} \u0026amp; #{13.to_s(2)} = #{12.to_s(2)}\u0026#34;   数组取共有元素的用法是知道的, 但是, 对于FalseClass NilClass 还有 TrueClass 来说, \u0026amp; 相当于 \u0026lsquo;AND\u0026rsquo;, 这个还真不知道 = . =\n1 2 3 4 5 6 7 8  irb(main):001:0\u0026gt; false \u0026amp; true =\u0026gt; false irb(main):002:0\u0026gt; nil \u0026amp; true =\u0026gt; false irb(main):003:0\u0026gt; true \u0026amp; Object.new =\u0026gt; true irb(main):004:0\u0026gt; Object.new \u0026amp; true =\u0026gt; NoMethodError: undefined method `\u0026amp;\u0026#39; for #\u0026lt;Object:0x007f9e7ac96420\u0026gt;   转换 Block 和 Proc 的用法也是没问题的.\n if object is a block, it converts the block into a simple proc. if object is a Proc, it converts the object into a block while preserving the lambda? status of the object. if object is not a Proc, it first calls #to_proc on the object and then converts it into a block.  update:\n前几天看到这么个写法: params[resource] \u0026amp;\u0026amp;= send(method)\n赶脚好奇葩的样子, 长得像+= 这类运算, 但是如果按这个理解的话不太妥 = . =\n查之发现,\n不单是有 \u0026amp;\u0026amp;=, 还有 ||=\n比如 options[:name] ||= 'Bond' 相当于 options[:name] = 'Bond' unless options[:name]\n而 options[:name] \u0026amp;\u0026amp;= options[:name].strip 相当于 options[:name] = options[:name].strip if options[:name]\n","permalink":"https://xguox.me/ampersand-in-ruby.html/","tags":["Ruby"],"title":"Ruby 中的 \u0026"},{"categories":["Jabber"],"contents":"很久很久以前, 大概是 2012.5 , 应该是 第6次 GZRuby Meetup, 那时候还是在客村那边打杂, 本想去 那一次 的 Meetup, 结果就忽然发烧了, 结果也就是没去成.\n错过了那一次之后, 就滚去了上海, 然后落脚没多久, 就去了一次 Ruby Tuesday, 依稀记得那次恰好撞上各种状况, 比如地铁停运了好一段时间\n接着是 回来的时候下了好大的雨还没伞, 回到小区的时候, 原本走的小门因为太晚关了, 绕了好大一圈.\n之后也还去了好几次, 然后就没然后了.\n时隔差不多两年了, 又是一次 GZRuby Meetup, 只是抛个头露个脸罢了.\n原本想做个比较, 但是还是算了, 没啥可比的.\n接连的 6.2, 过完儿童节的端午节, 从大惠州老早奔回了广州, 又去参加了 #ifanr WWDC iOS 开发者聚会#, 这还真的是去打酱油了, 完全没接触过iOS开发.\n啥也不说, 上图说话(最近好像连流水账也省得写了)\n环境很赞, ifanr 专属的 cafe\n前天的 Ruby Meetup 刚见面的 Leon, 世界真小\nMeetup 结束后一群技术宅在 Bomb Squad, 不能直视.\n","permalink":"https://xguox.me/early-june-coding.html/","tags":["Jabber"],"title":"线下 Coding 六月之儿童节与端午节"},{"categories":["Photo"],"contents":"新识的小伙伴也喜欢摄影, 尤爱把玩一些老镜头, 前不久发现他有一枚康泰时 T* 50\u0026frasl;1.4(MM) Planar , 恰好, 偶的 7R 也号称有容纳一切镜头的力量 (法兰距小, 各种转接撸起). 于是果断密谋在X宝入了一个YC-E口的转接环.\n大光定的眼睛总是很迷人的, 接上 7R以后, 整体外形比原先的 28-70 要协调多了, 至少菊花接驳部位不会突然瘦了一圈. 手感金属质感对焦环也都很舒服.\n老镜头, 自然逃不了手动对焦. 幸好 7R 还有峰值对焦, 不然单靠眼力判断, 要瞎的节奏啊.\n但是, 事实上有一些个时候峰值对焦也不一定好使, 甚至有时候, 你看到了峰值对上了, 但是事实上却还是不太满意, 可能是手抖了, 又或者是光圈全开了, 的确肉了. 但是, 玩这些头, 就得预了这些结果的说.\n出片色调, 可能用 Lightroom 的预设上瘾了, 色调这东西, 各有所爱, 不好说吧. 就如小伙伴说我喜欢调得冷冷的, 而他则对此不太买账. 那就上张暖调的吧!\n外拍时间其实也不多, 所以, 即使我几乎可以随便的使用这个头, 但是也一直没找着机会, 直到前些天一枚小伙伴要毕业了, 恰逢在周末, 也就总算有机会上场亮骚一把了.\n为在正式开拍前上手, 前一晚的M记各种乱入 多谢点餐的小哥, 对着镜头也不甚介意, 让我有足够的时间转动对焦环.\n没有镜头数据, 这个应该是 F1.4 的, 总体感觉还是挺满意的.\n这张不太好, 焦点没对上眼睛, 大夜晚的 F1.4也把快门速度保证了, 但是, 可能 最大光圈还是比较肉了吧. 又或者我对焦没对精准! 总体成功率比较一般. 装x一枚\n傻x三枚\n看着感觉不知道是在做啥的样子\n大马路拍一杯可乐, 看到这杯可乐就不禁想起当时搞笑又惊险的插曲. 当时大马路上几乎没什么车了, 于是我就堂而皇之地站在正中间摆弄, 不过我还是跟小伙伴说如果身后有车来了提醒一下我. 先来了一辆, 提醒之后, 我乖乖地闪回路边, 车过了, 再回去, 摆弄一会之后, 朋友说又来一辆, 不过是摩托车. 但我还是回头望了望, 这一望, 起先我也以为是摩托车, 直到驶前导30米略多左右, 才发现那原来是一辆坏了一枚前头大灯的的士, 艾玛, 要不要这么坑! 大夜景有大光定快门速度是可以保证了, 但是许多时候光线的不协调还是得打灯, 虽然我没玩过!!!\n翌日 尽管是要手动对焦, 但是其实有那么一段时间, 给小伙伴们拍半身照的时候, 其实几乎不用怎么转动对焦环, 基本上焦平面的距离都固定的.\n所谓色彩, 数码就是这般! 看片时间!\n这时候的限制就其实不是MF, 而是定焦, 当然, 50mm的标准焦距也不至于太尴尬, 除了全身的时候走远几步.\n所谓焦外二线性啥的, 咱就不装那么专业好吧!\n这类跳跃片用手动大光定还是很好拍的. 只是, 能看着镜头伐, 亲.\n回来还是按照自己的喜好把色彩调整了, 所谓菜头德味还是没特别感觉.\n手动头, 抓拍还是不是强项, 但是还是可以一玩的.\n焦点还是勉强的没对上. 给小mm点赞.\n","permalink":"https://xguox.me/plannar-50mm-t.html/","tags":["Photo"],"title":"Plannar 1.4/50 T*   (50mm 的毕业季)"},{"categories":["Jabber"],"contents":"这些天, 祈福不太安宁, 比如, 昨天的小游行, 还有今天,\n原本周末想大睡特睡, 结果两天很快就没了.\n大早的又去了耕田大学, 一是给 Mac 贴膜, 二是拷贝让朋友帮忙下了近 100G 的 Game of Thrones. 然后而外让朋友把手机刷了一把4.4,\n果然是没习惯玩手机的主, 即使刷完以后还是各种蛋疼, 比如 Google 的服务经常报错, 倒是 VPN 又正常了.\nPro 这次换了个很赞的A面,\n我是真的把 Logo 也黏上了让它不乱晃, 但是倒了大夜晚还是比较明显 ╮(╯▽╰)╭\n坐邨巴进去的时候, 发现一幕比较少见的.\n一位大叔可能因为某些上了年纪的身体不适, 跟一位与他熟识的阿姨在聊起来, 阿姨一个劲的介绍一些方子给他, 期间扯着啥我也没太注意, 倒是后来坐在他们后面的另一位他们不认识的大妈加了进去聊了一起, 只听到之前的那位阿姨又介绍了某款按摩椅的, 说的多好多好, 什么什么牌子, 我也没在意听, 再后来, 快下车前两站, 又另一位老大娘加了进去, 谈的更火热了. 几个人好似相见恨晚一般. 然后我就下车了, 然后就没然后了.\n这在其他的地方不是那么的常见吧, 比如地铁, 外面的公交. 也只有祈福或者像祈福这样的大环境才会有这么好玩的交流发生.\n","permalink":"https://xguox.me/clifford-jabber.html/","tags":["Jabber"],"title":"祈福一个周末"},{"categories":["Jabber"],"contents":"关于\u0026rsquo;冰与火之歌\u0026rsquo;(A Song of Ice and Fire),\u0026lsquo;权力的游戏\u0026rsquo; 早就于1996年在英国出版, 然后2011被HBO改编成连续剧上映银幕. 再然后, 不管是原书还是电视剧都堪称神作, 但其实我都没赶上头啖汤去欣赏, 只是去年看了已出版的5卷, 接着才发现有\u0026rsquo;权利的游戏\u0026rsquo; 这部剧, 于是果断把三季一次过看通. 除了以前上英文课老师放的 TBBT 就没看其他的美剧了.\n今个儿发现, 第四季貌似要开播了, 然后只知道剧情都被我忘了7788, 只知道史塔克家都死的死散的散. 最虐心的是猫姨惨死的那一集, 艾玛, 虐爆了! 求HBO别把我石心夫人弄太浮夸了. 好吧, 于是, 又要苦逼的把前三季的都下载下来复习一通, 30集1080P, 加加埋埋都上100个G了.\n第四季要不要收集完了再看呢? 一季一季的等已经够忧桑了, 还要一集一集的等!!!\nGRRM 挖了好多坑, 话说还剩两卷就足够填?\n","permalink":"https://xguox.me/a-song-of-ice-and-fire.html/","tags":["Jabber"],"title":"A Song of Ice and Fire"},{"categories":["Jabber"],"contents":"Air 用了 一年略多, 又被我换成了 Pro.\n15\u0026rsquo; 的 ME293, 开箱之后端起来那一刻, 我是多么的怀念 我的Air, ME293也就2.02kg, 看上去比以前的MD231也就重个一斤半左右, 放在背包双肩扛着都一样, 没什么差别, 但是, 每当用手端起的时候我就深深的觉得, 觉得亏大了. Air 一只手端着随便甩都杠杠的, 这15\u0026rsquo;的MBP, 有压力啊.\n换15\u0026rsquo; 主要是想用个屏幕大的, 的确舒服了很多, 但是代价蛮大的. Retina 也虽好, 但是支持的程度还没跟上, 经常看到锯齿图形, 不舒服斯基. 按理说, 花个2~3K 撸个显示器绝对流弊.\n╮(╯▽╰)╭\nCPU的提升没什么感觉, SSD的翻倍, 以前的120G还多出好几十G我会乱说? 续航强了些, 足够满足晚上不带充电器回家的需求. 这也算需求?\n虽然有经常备份以前的系统. 但是等到293拿到手以后, 我又不想恢复那些备份, 在但是, 在一个愚人节的晚上, 深夜时分, 我把系统玩残了, 到最后, 还是不得不用以前231的备份还原! 于是这就真正的变成了一台旧 Mac 了.\n也不知道这台293能陪我多长时间. 看样子我换的很脑残\u0026hellip;\n话说, 7R 我竭(jie)力(gei)克(peng)制(you) 才得以幸免于难. 我这是卖上瘾了嚒?\n","permalink":"https://xguox.me/new-mac.html/","tags":["Jabber"],"title":"New Mac"},{"categories":[],"contents":"说不清为嘛要转到 Jekyll, 原本的 Octopress 也就是基于 Jekyll 的,\n这个三栏单页的主题看着就想到简书, 也几乎直接秒了IE, 只支持到 IE10\n习惯了的 Octopress 要转过来还是遇上不少麻烦, 主要是 Markdown 的解析. 默认的 maruku 比较蛋疼. 另外就是还一些特定的标签, 比如 Audio, Video, Codepen, Raw等等.\n某些代码快可能直接就崩了.\n回迁过程修整这些 bug 的时候, 发觉以前写的一些, 好像很多微博posts , tweets 那样, 想要删去. 结果又还是没删.\nYou\u0026rsquo;ll find this post in your _posts directory - edit this post and re-build (or run with the -w switch) to see your changes! To add new posts, simply add a file in the _posts directory that follows the convention: YYYY-MM-DD-name-of-post.ext.\nJekyll also offers powerful support for code snippets:\n1 2 3 4 5  def print_hi(name) puts \u0026#34;Hi, #{name}\u0026#34; end print_hi(\u0026#39;Tom\u0026#39;) #=\u0026gt; prints \u0026#39;Hi, Tom\u0026#39; to STDOUT.   Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll\u0026rsquo;s GitHub repo.\n","permalink":"https://xguox.me/welcome-to-jekyll.html/","tags":[],"title":"Welcome to Jekyll!"},{"categories":["Tools"],"contents":"近日, 微软发布了 Mac 版 OneNote. 原本没想着装来耍耍的, 只是看了下大概, 发觉 UI 相较之 Evernote 更符合我的胃口, 于是, OOXX\n话说, 以前用 Lumia 的 Windows Phone 的时候玩过一会.\n其实我也算不上 Evernote 重度用户吧. 除了随手记点东西外, 用的最多的是 Clipper 功能, 发现 OneNote 也有, 只是, 不舒服的是, Evernote 只是一个小小的 icon 在 Chrome 的右上角, 而 OneNote 这个是挂在书签栏, 先不说是否隐藏书签栏的习惯, 但是看看那个白白的连个 favicon 都没的一撮挂在书签栏那 , 反正我不是处女座! 除此外, 剪藏下来的并不是原网页, 而是只是一个截图 = . = 这个弱爆了吧!\n编辑器上 Evernote 用的貌似是个 HTML5 编辑器, OneNote 更像是 Word. 喜欢 OneNote 的类似便签那样的功能.\n不好的地方, 目前来看的话就只有同步吧! 同步感觉比 Evernote 要弱好多的样子, 不知道是不是跟大墙有关系, 总感觉慢很多! 连登录都不畅快! 包括一些新建, 删除操作也是, Mac 下彩色菊花老转起, 半卡死状态灰常讨厌! 除此外, 想从Evernote 转过去的话, 毕竟存了不少东西. 不想吐槽自己大部分 Clipper 都没阅读, 好多都是用到的时候再去搜索.\n","permalink":"https://xguox.me/onenote-or-evernote.html/","tags":["Tools"],"title":"OneNote or Evernote?"},{"categories":["Jabber"],"contents":"一切都跟那时候的上海那么像, 我还是在一家创业公司, 当着\u0026rdquo;全栈\u0026rdquo;新手. 住的地儿跟上班的地儿也是步行的节奏. 稍稍不同的是, 现在是正职罢了. 跟 2013 自己胡搞不同的是, 不能胡搞了.\n室友从90后汉纸换成了妹纸. 相近的房租, 换来的空间大了许多. 相较而言, 广州也离家近了.\n有种描述不出来的感觉, 得知我复出继续投身码农事业后, 很多亲朋好友更多是关注我都做些什么啊?公司大不大啊?待遇好不好啊 之类的. 唯独只有一个人问我, 工作开不开心! 唯独这位近80高寿却又平凡的至亲, 戳中我泪点!\n比一切都重要的是, 祝愿自己身体健康, 平安!\n感觉现在 Jabber 也扯淡不出东西来了.\n","permalink":"https://xguox.me/a-new-day-has-come.html/","tags":["Jabber"],"title":"a new day has come"},{"categories":["CSS"],"contents":"原本想要使用 CSS3 中的多栏属性实现类似报纸那样的排版, 或者 pinterest 那样的瀑布流, 但结果跟预想相去较远. 原以为用 column-count 或 column-width 来将 HTML 元素变成多栏, 再 column-gap 定义为 0 让所有元素能贴在一起, 但是才发现没有真正理解多栏的含义, 使用多栏的话是从上到下的排序, 而如果按时间顺序列出所有 posts 的话, 那结果只是\n基于这样的排序, 于是就想着将计就计, 把页面整体做成横向滚动, 为此还使用了 [jquery.mousewheel.min.js]() 来把鼠标滚动设置成横向. 想来绝大部分的网页都是上下滚动, 搞搞非主流. 最后发现, 整体这么设置的话, 会造成用户观看的时候视线变成了频繁的正弦曲线, 很糟糕. 现在(14.2.14)还是这个样子, 因为没有想到其他 nice 的方案, 暂时没做修改.\n主流浏览器对 column-*的支持各不相同, 又比如, moz-column-count can not work properly with -moz-column-fill: auto, 见 https://bugzilla.mozilla.org/show_bug.cgi?id=733808 所以像这样整体的过度使用暂时还是放弃为好.\n","permalink":"https://xguox.me/css3-column-star.html/","tags":["CSS"],"title":"css3 之 column-*"},{"categories":["Jabber"],"contents":"一件 T\u0026rsquo; 而已, 我是不是很无聊? 而且明明就没箱子 = . = 其实是因为之前忘了在哪看到的, 有一篇牙签盒什么的开箱. 更鸡碎的东西 = . = 不要吐槽没事找渣儿玩的 2货. 大过年刚才拿到 New Relic 送的 T-shirt.\n10月29发过一条微博的. \u0026gt; 最近感觉没衣服穿了,想起NewRelic那有T-shirt送,不过要成功部署一个应用,刚好最近NewRelic也开始支持Node. 于是果断把放在OpenShift的Ghost给部署一个 =.= 然后,有比我这还钓丝的么? 结果发现,这是夏装T,等寄到我手上我都要穿棉袄了. 尴尬到\u0026hellip;\nand then, 今个儿终于拿到手了. 刚好就当过年新衣服好了. 最近天气也热了不少. BLABLABLA\n看这戳 1月6 号就到了, 不过是寄到朋友家, 拖了好些天才拿回来.\n英文的地址好像很流弊的样子, 又装到了\n拆开来, 等登灯灯\u0026hellip;好一个 NERD , 说的就是咱!\n挂外面展开\n后面这个 logo 的地方不太好, thanks New Relic\n奇葩的是 made in USA 为嘛上面有好些个中文字\n上身对着镜子撸一张\n没事找乐子, that\u0026rsquo;s all.\n新年快乐, 马年吉祥!\n","permalink":"https://xguox.me/new-relic-t-shirt-unbox.html/","tags":["Jabber"],"title":"New Relic T' 开箱!"},{"categories":["Jabber"],"contents":"单曲循环从以前的陈奕迅换成了郑秀文.\n截图来自http://xguox.me/blog/2013/01/01/2013-new-year/\n身体好起来了, 但不算完完全全百分百! 文字功夫, 没怎么见提升, 翻译了不少东西, 感觉英文能力仅在 web 领域有提高. 一度有过想去当摄影师的念头. 最后, Javascript 最惭愧, 几乎没长进!\n2013, 必须是最难忘的一年, 也是看似最平淡的一年! 挨了两刀! 在家猫了整年, 走医院当吃饭般平常, 吃药比吃饭还频繁.\n1月份和7月份不谈, 2-5月基本淡定的玩儿代码, 拿了毕业证后略微不淡定, 5-10月份不务正业的几乎把 coding 荒废! 更可耻是甚至曾经想要放弃!!! 纯粹心理原因!\n11月开始重新再学习 Ruby, 12月开始 Rails , 12月14 自己启动代号 do4fun 项目, 直至今天基本功能基本页面基本实现.\n有意思的是, 我的Github 上的 Contributions 基本可以看出来了这些个规律 总结是, 我的心理能力属性为弱, 但是正在增强!\n透过这10多天搞出来的东西, 既在学习也在回顾.\n主要在回顾吧\n想来当初 @简叔和@萌波 要找的实习生其实现在的我才符合要求, 那会的我, 好吧, 越想越不可思议. 回想起来, 12年7月份的我 简书的 UI 几乎是在 简叔 的强力帮助下搞出来的! 后台起先程序小不打紧还能干的来, 但到后来程序起来了又是要萌波力挽狂澜! 尤其 JS 那块. 自己看着都蛋疼!\n抛开 JS 不谈, 就算 HTML 和 CSS , 也直到现今完成了 do4fun 之后才敢说熟悉, 原来我一直对盒模型 布局方面就模模糊糊, 甚至连 margin 和 padding 都老弄混淆! 回想起当初简叔教我关于 Bootstrap 的, 好吧, 我又是现今才熟练了! 现在虽然依赖于 Bootstrap, 但是, 也算是有能力调成了不像 Bootstrap.\n当然没有那段白板一样的经历, 也不可能有现在可以快速回血的存在. 感谢 简叔 \u0026amp; 萌波!\nAnyway, 接下来可以干的也不少, 尤其 JS 这块硬骨头! 不能再拖的玩啊! 准备尝试用 Backbone 或者 Angular 把前端加强! 希望能顺利吧. 另外测试, Rspec 写一会不写一会的! 习惯没有养成, 堆着堆着就是一堆突如其来的 Bug.\n真心希望年后可以复出! 不过真若可以, 到时候我能找到什么工作呢? 前几天, 无意间听到 老爸, 大伯他们还想把我弄去当些村官类职责. 不禁想, 不考虑意愿, 如果真那样了, 是不是我要拿着我的 Mac 在那些办公室 码代码叻!\n不管是生日愿望, 还是新年愿望, 都无比希望自己能健健康康的, 去做自己喜欢的事!\n","permalink":"https://xguox.me/2013-2014.html/","tags":["Jabber"],"title":"2014!"},{"categories":["Tools"],"contents":"没事喜欢折腾下 Sublime Text 的主题, 当然可能看习惯一种主题对编码也有帮助的, 对各种语法匹配各种颜色形成了习惯, 出现 typo 或者其他的一些错误也较容易发现吧.\n不过只是想在白天和黑夜交替更换一下 solarized 的 light 和 dark , 大夜晚的用浅底的配色不幸福啊\n 在IFTTT 上创建一个 Recipe , Sublime Forum 里边的哪位大神用的是关联天气的日出和日落(if Weather), 好吧, 我还是直接设定时间好了(if Date \u0026amp; Time), and than 设置更换主题的时间, IFTTT 居然不给同一个 Date \u0026amp; Time 设定两个时间 = 。 = 所以, 这里要用到两个 recipes(sunrise \u0026amp; sunset) , 先做一个 sunrise 的. 时间设置早上9点(可能或有几分钟的触发延迟)\n then Dropbox -\u0026gt; create a text file -\u0026gt; Dropbox folder path 不改了直接用 IFTTT/Daylight -\u0026gt; File name 也是, 就 sunrise -\u0026gt; content 则是 light\n Automator 新建一个文件夹操作, 接收文件夹选择 /Users/xguox/Dropbox/IFTTT/Daylight -\u0026gt; 文件和文件夹 的 查找 Finder 项目 , 查找 /Users/xguox/Dropbox/IFTTT/Daylight 是否有文件拓展名为 txt 的. -\u0026gt; 添加 运行 Shell 脚本\n 在 textarea 中添加这段 shell script , 存储-搞定!\n  1 2 3 4 5 6 7 8 9 10 11 12  DAYLIGHTDIR=\u0026#34;/Users/xguox/Dropbox/IFTTT/Daylight\u0026#34; SUNSETFILE=\u0026#34;sunset.txt\u0026#34; SUNRISEFILE=\u0026#34;sunrise.txt\u0026#34; if [ -f \u0026#34;$DAYLIGHTDIR\u0026#34;/\u0026#34;$SUNRISEFILE\u0026#34; ] || [[ -f \u0026#34;$DAYLIGHTDIR\u0026#34;/\u0026#34;$SUNSETFILE\u0026#34; ]] ; then CURRENTSETTING=`grep tmTheme ~/Library/Application\\ Support/Sublime\\ Text\\ 3/Packages/User/Preferences.sublime-settings | awk -F\u0026#39;[.|.]\u0026#39; \u0026#39;{print $2}\u0026#39;` DAYLIGHTFILE=`ls \u0026#34;$DAYLIGHTDIR\u0026#34;` NEXTSETTING=`cat \u0026#34;$DAYLIGHTDIR\u0026#34;/\u0026#34;$DAYLIGHTFILE\u0026#34;` sed -i \u0026#39;.bak\u0026#39; \u0026#39;s/\u0026#39;$CURRENTSETTING\u0026#39;.tmTheme/\u0026#39;$NEXTSETTING\u0026#39;.tmTheme/\u0026#39; \u0026#39;/Users/xguox/Library/Application Support/Sublime Text 3/Packages/User/Preferences.sublime-settings\u0026#39; rm -f \u0026#34;$DAYLIGHTDIR\u0026#34;/\u0026#34;$DAYLIGHTFILE\u0026#34; fi   重复一次建一个 sunset 的recipe, 调好时间把 content 的 light 改为 dark 就是了.\n关于 Shell 好吧, 尴尬我没认真学习过, 只能大概看懂然后做些修改( xguox 的配色来自 Base16 不是自带的 ). 其实我是在终端先把脚本测试运行了一通滴!比如 awk 那里就没看懂, 直接运行了那一行才把原本的[(|)]改成了[.|.], 理论上其实可以无限更换, 只是太花哨频繁了也不甚好.\nIFTTT (Dropbox)+ Automator(Shell) 这个组合赞死了! Google 之发现很早就有人这么干了, 看来还是 out 了.\n参考自Sublime Forum\n题外话扯淡, 本来说试试用简书的富文本, 发现貌似从原本的 wysihtml5 换成了 redactor. 靠!林总果然是土豪啊! 一个编辑器也要花上 $199 . 不过单看这一般功能看不出比一些开源的货有啥流弊之处。\n","permalink":"https://xguox.me/automator-ifttt-dropbox-sublime-theme.html/","tags":["Tools"],"title":"自动更换 Sublime Text 主题 by Automator IFTTT Dropbox"},{"categories":["Ruby"],"contents":" 重拾 Rails, 然后用了 devise 这个 Gem ,遂继续笔记之\nDevise 在 ruby 社区里头几乎都称之为一门重炮, 当然了, 重换来的是功能丰富, 用户注册登录相关的一个 Gem 搞定, 只是要把源码搞透彻那必须流弊的人才干得来!\n现阶段对我来说, 知道怎么用就 OK 了.\n但是, 其实也还算不上完全知道怎么用. 遂笔记 MARK 一发.\n参照官方的Getting started 添加 Gem\n1  gem \u0026#39;devise\u0026#39;   安装 Gem\nbundle\n跑 generator\n1  rails g devise:install   and then 是一坨的提示，照着撸完，之后\n1 2 3  rails g devise user rake db:migrate rails generate devise:views   至于如果需要添加 index show 等其他一些 action 的话直接再 generate 一个 users controller 不过 在routes.rb中需要加一句 (在devise_for :users 之后)\nresources :users, only: [:index, :show]\n然后是添加其他字段, add username migration 之后\n1 2 3 4 5 6 7 8 9 10  class ApplicationController \u0026lt; ActionController::Base before_action :configure_permitted_parameters, if: :devise_controller? protected def configure_permitted_parameters devise_parameter_sanitizer.for(:sign_up) \u0026lt;\u0026lt; :username end end   strong parameter 印象中 rails3 下如 Post.new(params[:post]) 是可以正常运作的,\n但是, 在 rails4 当中就报错\n1  ActiveModel::ForbiddenAttributesError in PostsController#create   在 Rails4 以后要改成\n1  Post.new(params.require(:post).permit(:title, :content))   相当于以往的 attr_accessible 把工作从 model 转移到 controller 了, 规避了以前的 mass assignment 问题, 又想起以前 github 中招 XD\n摘抄官方 guides\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  class PeopleController \u0026lt; ActionController::Base # This will raise an ActiveModel::ForbiddenAttributes exception # because it\u0026#39;s using mass assignment without an explicit permit # step. def create Person.create(params[:person]) end # This will pass with flying colors as long as there\u0026#39;s a person key # in the parameters, otherwise it\u0026#39;ll raise a # ActionController::ParameterMissing exception, which will get # caught by ActionController::Base and turned into that 400 Bad # Request reply. def update person = current_account.people.find(params[:id]) person.update_attributes!(person_params) redirect_to person end private # Using a private method to encapsulate the permissible parameters # is just a good pattern since you\u0026#39;ll be able to reuse the same # permit list between create and update. Also, you can specialize # this method with per-user checking of permissible attributes. def person_params params.require(:person).permit(:name, :age) end end   所以上边要先把 username 在 params 给 permit 了.\n暂时是这些先 MARK\n","permalink":"https://xguox.me/rails4-devise-strong-parameters.html/","tags":["Ruby"],"title":"Rails 4 Devise Strong Parameters"},{"categories":[],"contents":"尾田大神一下子, 「唔声唔声吓你一惊」\n其实德雷斯罗萨已经很激动了, 连毕古麻姆都来了!!!\n只是这激动归激动, 但都是各种不利草帽的因素!!\n整个德雷斯罗萨的剧情又是一波高潮!!!萨博的出现固然激动, 但是因为老早知道他没死,出现是迟早的问题, 所以, 其实赞点还是给了克尔拉(其实我第一眼木有认出来, 只觉得面熟), 原本以为只是个一笔带过的小龙套, 第二笔就给盘活了!!! 忍不住扒了存在移动硬盘的里的全部OP动画漫画, 特地回看了那几集!!!(顺便整理下文件和文件夹的命名. Mac下的Automator好用极了!)\nand then,\n克尔拉的帽子莫不是跟萨博小时候来情侣装?\n(≧▽≦)/ 期待下一期, 下下期,下下下期\u0026hellip;\u0026hellip;\n","permalink":"https://xguox.me/sabo-and-koala-and-one-piece.html/","tags":[],"title":"One Piece Chapter 731"},{"categories":["Photo","Jabber"],"contents":" 前奏 原本是要长叹自己从20岁病到现在22了。发觉消沉的东西太多了，还是不发神经扯淡了。 不是特地计划生日当天跑来深圳大鹏半岛这边。不过我却的确来了。 路途 在车站搭招手就上的惠深城巴。\n网上说在龙岗天虹可以坐车到南澳再转车去西冲。但是我在龙岗车站天虹那边蛋疼了好久还是没找着。最后是搭818到大鹏站再转M232到鹤薮村，略蛋疼了。这路程耗时比我从惠州到龙岗天虹还多了去。\n再次感慨这个拿了3年驾照还不会开车的老司机，要是自驾的话会很不错的。 南澳到西涌那段山路被司机刺激了一番， 一般人在那段路开的可能会略吃力， 但是当地的这些司机那是杠杠的，各种加速超车急转流弊哄哄的。\n住宿旅馆是在鹤薮村这边，「薮」字一堆文盲不知道读啥音， 其实我也不知道， 不过我在手机上猜读「sou」于是一次中！\n到了村子，还等老板开电动车来接的说。老板是河源人，于是难怪口音听着都差不多。没一会咱就扯上客家话。村里边估计90百分之都是旅馆，现在是淡季， 旅馆什么的也比较便宜些， 基本都是旺季价格折半。但是，找吃的好难。好不容易看到吃的。还贵的113，据说还难吃。\n于是自己买菜在旅馆老板这煮。 213青年欢乐多啊\n好男人就是我 - 拍照的那个\n开吃也要撸几发 吃相真™猥琐\n海边 一路奔波， 填饱肚子后都4点好几了，不等休息就开始杀向海边。\n10蚊鸡的票 人不多， 但也比预想的略多，主要是这天气，居然12月份了还没冷下来。这天气最多算凉爽（知足吧，要不是淡季， 那看人就够了） 没过一会人就越来越少了。\n我能想到最浪漫的事,就是\u0026hellip; 我总感觉我经常看到海，但是却想不起上一次看海是什么时候了。厦门的？没有那么久远。大亚湾的，也不是吧？ 在上海貌似没到过海边。广州也顶多是在江边。 额\u0026hellip;上一次看海好像还真的就是在厦门。\n脱了鞋子踩踩沙子东拍拍西拍拍\u0026hellip;其实吧，我感觉我真心太过care拍照了。单论玩，其实，不单是这一次，往前的好几次，感觉我都只是在拍照，没有enjoy进去。某些个点，想要拍张照片，架上架子，这构图不好，那曝光不对，还没对好焦\u0026hellip;折腾折腾之后，其实景物留在我脑海的印象不深刻，至少我觉得不如与我同行的两个朋友深刻。他们是先感受风景特别需要时候才会掏出手机或者卡片机， 而我是看了一眼之后就捣鼓半天对着取景器或者LCD。\n这次更不可理解的是，在没有ND/GND的情况下，明知各种过曝也还是执着着要营造丝绸流水的效果， 结果就是脑残了， 远端天空什么的过曝过的吐， 死白的地方一片又一片，尽管我把光圈缩到F22，把ISO拓展到50。 依旧抵挡不住过曝的打击。 可能遮黑卡的方法可以解决这问题， 只是当时候身上没有黑纸之类的能使的上（后来想会不会镜头盖其实也可以拿来勉强当黑卡呢？） 朋友看了照片还赞以为什么云端仙境，云雾缭绕了。 当时的确天空灰蒙蒙的，但是也不至于我拍出来这样的死白， 完全是因为过曝的原因。\n可能在他们眼中我拍的那些只要是清晰的就是好片，但是，再我看来却各种问题，过曝的死白，欠曝的死黑,色散眩光暗角什么的。 比如这个， 原本是想帮他拍张剪影， 结果iso一直调在了auto，然后飙到2000，傻逼了。 出片满意率每次都很低。 然后回到电脑前整理照片就是各种删各种修。有点为拍照而旅行的谬误。\n夜晚 到了晚上， 人就更少了，除了岸边上那些烧烤档有几波烧烤的之外，海边上的也就零零散散的几个在自己的可见范围。 走了大概1/3， 看到这么个玩意， 艾玛， 真好精力，拍个照到此一游， 这个大作就是我的了XD。e\u0026hellip;我说你能更烧饼一点吗？=。 =\n答：能。 请往下看。\n先配上经典的BGM\n[](http://m5.file.xiami.com/258/1258/6610/375918_14519_l.mp3?auth_key=5205b0c11bb45c41a17eee70ee248618-1386683095-0-null)\n开始放大招\u0026hellip; 烧饼就是这样炼成的， 最后照片是反过来的 ╮(╯▽╰)╭ 还是没修炼到家\n话说到这里已是8点多，并还木有吃饭，只好到岸边那些餐厅让人宰了去，好吓人的餐牌， 鸡： 98/半只。 随手点几个迟了滚蛋回去。 回来路上无意间抬头发现星星巨多， 于是赶紧的回到旅馆没休息几下奔上楼顶就撸了几发 考虑到下边要过曝，于是拿镜头盖挡了挡， 技术不到位， 结果月亮让我拍成了太阳 没有拍过星空星轨，没有快门线也难拍， 加之，要曝上一个小时什么的话，我没倒下估计7R的续航就要趴地了。 死要装13跑前面去又不露脸，强忍着不动但是细微的晃动还是被相机抓住了，人糊了。 原本飘着的几个孔明灯还以为可以添色几分， 结果感觉还影响了我的画面\n次日 一早， 原本是要调个5点半闹钟起床到海边看日出，结果是傻逼了日出不是在海的方向,而是在山的这一边（顺便鄙视一下赖床到6点的家伙！）。 与预想的朝霞映海的景观相去甚远， 海上依旧云雾一坨接一坨的。 而且退潮后岸边淤泥+大片大片的青苔， 不好看！ 于是 继续把剩下的那一大段海岸线走完（有接着走这样的说法？其实还是重新把整个海岸线走一边）。\n天空被我后期撸的有点假=. = 走走停停 拖鞋埋了光脚走 这一走就是从起床走到了正午2点。走辣么长的沙滩真Y累人， 一脚深一脚浅的， 最要紧的还是鸽的三脚架扛着走，那个重呀！而且裤子被海水冲湿了大半截， 裹着沙子拖沓着更是走的我蛋都碎了。要不是路途上找点乐子拍下照， 纯走的话我早就要打退堂鼓了。\n来张黑白的文艺一下 继续装 小风小浪 看这剩下的路程还有那么一大段,其实我们在担心的是,回来要怎么办 = . = 走了半天,回头一看,发觉没走的那段比走过的那段还是要长那么的多!!!\n停下来装装逼 Y的抢我的装逼位置 你又来凑什么热闹!!! 装X无底线啊 快要走到尽头了 最右，又死白了！ 这下返程有的好受了。\n因为拍了段视频，结果7R的电池到尽头那会就刚好歇菜了。移动电源撸上！\n出来了也不忘拈拈花草 能不要辣么浪漫嘛！ 回旅馆收拾滚蛋 脚都走软了 吃饭时间， 不好吃\n回家洗洗睡 比较可惜了，一个周末来了大鹏，那么多的景点只在西涌玩了。 名义上周末两天， 除去路途车程实际玩的时间可能还不到一天。原本还准备去杨梅坑。不过时间各种不允许（其实走完西涌全程来回的10多公里，体力上也也有点不允许， 年轻人，你弱爆了）。只好吃过饭就散了，各自回家。\nThe End\n","permalink":"https://xguox.me/hello-22.html/","tags":["Photo","Jabber"],"title":"Diary: 22 西涌"},{"categories":["Photo"],"contents":"接上一篇, 想着在网上搞了那么多的预设,不用放着也不碍事. 结果没些时间, Lightroom 就各种闪退各种编辑不能.\n按理说应该不是跟 Maverick 兼容问题吧,如果那样的话升级了之后应该就会有问题了, 不过升级 10.9 以后没怎么使用, 都是随手的就预设搞定. 没注意预设这点, 只是简单地用 CleanMyMac 卸载了之后重装了一次, 顺手下载了一个 5.3 RC版的来解 A7R 的 RAW, 好吧,我也没怎么用 RAW.\n安装过后还是老样子的只要一编辑图片就闪退, 蹦出一个 Error 框来唬人(可以进入 Lightroom,看到图片). 不过奇怪的是, 我用 CleanMyMac 卸载完重装以后, 居然还可以继续使用之前的预设. 打开 ~/Library/Application Support/Adobe/Lightroom/Develop Presets/ 发现, 一个也没少啊. 这个就奇葩了.\n果断把所有预设删除, 杠杠的就可以编辑!!!果然, 不过是预设太多了还是因为部分预设的版本太老或者其他什么的原因?\n把之前的预设都删光光了之后一个个预设文件夹的复制进去,发现,有些导致了不能编辑, 有些则直接就造成 Lightroom 启动不能. 更具体原因不明\u0026hellip;\n所以说,还是别贪心啊啊, 找好自己喜欢的风格喜欢的调子就好. 这下不能拖延症了啊!\n","permalink":"https://xguox.me/lightroom-flash-quit-because-of-too-many-presets.html/","tags":["Photo"],"title":"解决 Lightroom 闪退"},{"categories":["Photo"],"contents":"http://hq.update.sony.net.edgesuite.net/united/clock/assets/sound/a_clock_main_theme2012.mp3\n接上一篇, 按理说, 应该是给刚到手新鲜热辣略烫手(都几天了还烫手= . =)的 A7R 玩儿开箱, 不过以 A7/R 这一系列的火爆程度, 开箱文从15号左右开始, 一些有门路的早一步拿到机子的早就开箱18鞭了. 无忌索坛也早早开始就被屠版了(Nikon Df 没开卖就干了四五期也很流弊) 所以不掺和一脚. 再或者说, 从发布那天开始, 就已经不知道多少人高潮过多少次了. 不过不玩细节开箱也来个秒开, 两张流帅照还是有的.\n很奇葩的镜头吧, 官方木有 28-70 的配套. 更奇葩的是为嘛在 Skydrive 是正的贴过来就旋转了 -90deg 叻!\n在 Google+ 上和人交流起来才发觉好多术语描述不出来(读的话还能靠猜), 什么「拉风箱」「高感」「焦外」, 艾玛, 英文跟白学那样.\n最后自己只能憋出这些个评语\n Love the little metal body , so many custom button/wheel for good manipulation, it’s tiptop, and quite a lot of modern features, but sigh for Sony didn’t equip it with touch-screens , or it would be perfect. for disadvantages ,seems that it is hard to autofocus in dark. besides , it’s a terrible battery monster. I am trying everything to make it more durable. btw, it must take me more time to shoot and review the pic quality .\n 其实还有一车要说只是实在翻译不过去了.\ne\u0026hellip;..搞半天还没上主角啊 , 2333333333333\n考虑到 7R 的3600万像素, 所以第一时间想入的配件是脚架, 主要是手太抖了,也不知道是不是心理作用,反正手持拍的总感觉不好. 而且,貌似持越轻的东西越抖, 当初还因为这个犹豫微单.\n第一枚架子, 曼富图(MK293A4-A0RC2). 原本还想上碳素版(MK293C4-A0RC2),奈何贵了近800软妹币, 再一次应验没有什么选择困难, 只不过是钱包困难. 说多都是泪. 不然谁不想用捷信呢?我还想买上10根捷信架子放家里晾衣服叻! 不知道衣服会不会干的快一些 = . = 这款架子貌似不算新,出来有些时间了, 现在曼富图这个价位比较火的一款应该是 MKBFRA4-BHCN (看某猫官方店的销量)\n先再次吐槽X通的效率, 然后,终于收到货了.(没有大光圈来虚化不知道是不是让很多人解毒了捏?)\n躺平\n站直\n褪去最外面的厚纸皮 拆埋内盒,看到半边脸了, 那层白色的包装膜缠着各个脚挺紧的\n终于见到真面目鸟, 我蛋疼的跑到阳台来一发\n四节升起,额,一定是我打开的方式不对, 这个视角看上去顶上天花板了. 家里房子太矮了.\n升埋中轴! 吐槽下自己, 折腾了好一会才发现有一个脚的一节没有完全拉伸开来.\n搭上 A7R, 再一次说明,手真抖, 拿手机拍出来的各种不能直视.\n三维转一发, 新人用脚架载新机新入手就是心惊胆颤怕摔了. 好多余的担心, 螺丝圈了N圈还怕毛线.\n局部, oops, 反光了, 没关系,看得到 「MADE IN ITALY」就行了\n无节操自爆, 请不要吐槽我的邋遢装束. 刚开始还在担心高度问题, 结果很合适的说, 看来我高估自己的身高了. 不过 befree 系列貌似要矮上近 10 cm 那可能真的比较不合适了.\n就是不露脸 = . = 再来局部 看官网说还有两个功能, 中轴倒置和两级脚管, 不去拆了, 借官图\n挂在 NG5270 侧边, 高出了近10cm\u0026hellip; 总结, 虽然是铝合金, 不过重量也不咋滴(本人体能战五渣),拿在手上感觉挺小巧的. 高度对于170左右的刚刚好, 至于稳定性, 别人用来架大炮都没事, 我架个小微单算啥. 不过, 没有像 befree 那样自带脚架套不幸福. 其余的因为用脚架经验不多, 有待发掘.\n最后是上片 ISO 测试(不截图 100% 测试个啥?), 的确很奇葩, 现在卖相机买相机都喜欢飚高感了去, 可用都要飙到 12800 去了. 伤不起\u0026hellip;\n尽量的控制变量, 如果不特别指明则 ,都是默认的 「高ISO降噪: 低」,「长曝光降噪: 开」. 貌似发到这图都缩了感觉不明显了啊. 原图打包 请猛戳 这里\n好吧,最后下起雨了准备回去的时候才想起忘了关闭 SteadyShot, 另外虽说娱乐测试,但构图没水平线不直, 尴尬.\nISO 100 (尴尬,这张应该是长曝光降噪:开)\nISO 100\nISO 800\nISO 1600\nISO3200\nISO 6400 高ISO降噪: 标准\nISO 6400 高ISO降噪: 低\nISO 6400 高ISO降噪: 关\nISO Auto 多帧降噪 **结论, 没意义 ╮(╯_╰)╭ **\n更没意义的更高ISO\nISO 12800 ISO 25600 ISO 25600(高ISO降噪: 关) 最后上张失焦的. 还有被玩到厌的车流长曝光 最后的最后,上一张以前那台 Canon 6D 的 8000 ISO\n其实不放大百分百啥也看不出\n最后的最后的最后, 吐槽一下,在某大型商场的大门前的靠近马路边架着架子拍片的时候,一个不知道是店员还是保安过来,不让我在那拍照, 然后我问他「我在走过几步到马路边上可以吗？」. 然后 他允了. 艾玛， 中国人有那么介意照相机吗？\n","permalink":"https://xguox.me/a7r-iso-unbox-manfrotto.html/","tags":["Photo"],"title":"用 A7R 玩开箱并各档不严谨不严肃不专业无责任高感测试"},{"categories":["Photo"],"contents":"在网上搜罗了各种 Lightroom 预设一大车, 目测有两三千个了. 之前都是直接的一坨坨的 copy 到 ~/Library/Application Support/Adobe/Lightroom/Develop Presets/, 然后好奇了一下用 Sublime 随意打开来瞧瞧,额,原来长这样,好高端,不明觉厉.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  s = { id = \u0026#34;DFE17DAE-6240-4CB6-85ED-B41D5B065404\u0026#34;, internalName = \u0026#34;Toners-Light Mocha\u0026#34;, title = \u0026#34;Toners-Light Mocha\u0026#34;, type = \u0026#34;Develop\u0026#34;, value = { settings = { Blacks2012 = 0, Contrast2012 = 0, ConvertToGrayscale = true, EnableGrayscaleMix = true, EnableSplitToning = true, GrayMixerAqua = -18, GrayMixerBlue = 9, GrayMixerGreen = -27, GrayMixerMagenta = 3, GrayMixerOrange = -19, GrayMixerPurple = 15, GrayMixerRed = -10, GrayMixerYellow = -23, Highlights2012 = 0, ParametricDarks = -5, ParametricHighlightSplit = 75, ParametricHighlights = 0, ParametricLights = 5, ParametricMidtoneSplit = 50, ParametricShadowSplit = 25, ParametricShadows = 0, ProcessVersion = \u0026#34;6.7\u0026#34;, Shadows2012 = 0, SplitToningBalance = -59, SplitToningHighlightHue = 0, SplitToningHighlightSaturation = 0, SplitToningShadowHue = 37, SplitToningShadowSaturation = 25, ToneCurveName2012 = \u0026#34;Strong Contrast\u0026#34;, ToneCurvePV2012 = { 0, 0, 32, 16, 64, 50, 128, 128, 192, 202, 255, 255, }, ToneCurvePV2012Blue = { 0, 0, 255, 255, }, ToneCurvePV2012Green = { 0, 0, 255, 255, }, ToneCurvePV2012Red = { 0, 0, 255, 255, }, }, uuid = \u0026#34;1DF2363E-780A-4C26-9711-D980D505BF0D\u0026#34;, }, version = 0, }    然后话说我是不是也可以把这一坨的上千个预设 push 到 Github叻? 顺便当做个版本控制也好, 虽然我应该不大会去修改这些预设 = . = 不过传了也没坏处吧. Github 不一定只能放代码的!\n其实多也没什么好的, 倒是这搜罗了略有点多,其中一大坨感觉基本不会用到, 而且还经常让人犯选择困难,那样反而更蛋疼. 还是习惯了形成一些自己的风格好. (但是,又很贪心地想把这全部风格都尝试一遍 XD)\n不过, 一张照片如果原图就是好照片的话基本上各种调试之后都会觉得不错, 只是看符不符合自己风格罢了.So, 找个时间整理一遍还是有必要的.(不知道拖延症会把这个整理拖到什么时候 = . =)\n借了A7 的样片搞了几下, 有一张是原图,不过忘了是哪张了.\n","permalink":"https://xguox.me/lightroom-presets.html/","tags":["Photo"],"title":"Lightroom 预设"},{"categories":[],"contents":"因为 Air 的硬盘只有 120G,所以总习惯性地  - 关于本机 - 更多信息\n然后每次都看着那蛋疼的数字, 然后这次略离谱了\n有一段时间没 check, 话说之前一般都维持在 30+ G 的呀,奇怪了那个备份是什么玩意, 我明明已经设置了 Time Machine 备份到我外接的移动硬盘了并且基本上一直都连着这个盘的呀! 查询之,才发现貌似还有个叫 Local Snapshots(本地快照) 的功能(?).\n艾玛, 这么个备份法我的硬盘要蛋碎了. 但是这玩意是备份在哪了的说? 再查之, 原来是在 /Volumes\n果断删之, 虽然数据的确重要,但是 已经有个外接盘24小时一次的备份了,除非遇上移动盘跟系统同时崩了.\n另外据说可以直接 sudo tmutil disablelocal 关闭本地快照, 或者 sudo tmutil enablelocal 再启动之.\n想起前不久苹果官方召回2012款 Macbook Air 的事. 貌似那一款的东芝盘有什么重大 bug, 艾玛, 刚好就是我这款, 而且我的这个盘也是东芝的, 如果配的是三星的SSD貌似没事. 不过其实也一直没问题, 官方说装了个补丁没事就好偶也没啥好不乐意的. 就算无故崩了也还有那高频率的备份.\nPS. 影片 + 照片好久没整理了,一直滞留着, 又是一笔大开销, 倒是那个其他还是那么神奇,一会大一会小.\n","permalink":"https://xguox.me/time-machine-local-snapshot.html/","tags":[],"title":"Time Machine \u0026 Local Snapshots"},{"categories":["Photo"],"contents":" http://hq.update.sony.net.edgesuite.net/united/clock/assets/sound/a_clock_main_theme2012.mp3 先赞上一个 bgm, 来自 Sony Global - \u0026ldquo;α\u0026rdquo; CLOCK: WORLD TIME, CAPTURED BY \u0026ldquo;α\u0026rdquo; 那个对焦声音以前听 A77 听习惯了, 现在蛮有感觉的.\n 好不容易把闹腾相机的心静下来, 结果前些天又让骚尼君扯动了那片涟漪.A7 / A7R 完全把我的折腾心又搅起来了.这不20号帮牛总拍完婚礼立马把6D出了.下订, 然后, 就剩下等待了, 结果, 在我无限接近得到A7的时候, 半路杀出了个Nikon Df, 这是帅的一塌糊涂啊, 尤其银色的骚的无与伦比, 快到嘴的A7要飞了.再结果, 那让人蛋碎的定价\u0026hellip;\n 开篇之前, 先扯出我的观点：\n 不管是Leica还是1DX还是NEX, 全都能出好的片子.甚至乎, 把这几台机子拍的片混淆在一起90%的人分辨不出来 我是玩相机的, 不是玩摄影的 很明显的我不是土豪, 糕富帅之流.因为这类人可以眼睛不眨的随便玩儿这些机器, 各种徕卡哈苏随便当备机 如果你真那么流弊看出来了低端机和高端机的差距, 那是因为人家没后期. 几十块的后期书籍绝对比器材巨大的差价带来的图片差异要明显. 目测6D的一半功能还没玩熟练. 把花在器材的钱节省下来出去玩比器材压箱底封尘抚摸能多出好片.  回顾11年11月11日, 一个很蛋疼的日子, 开始陷入相机这趟浑水就折腾的一发不可收拾. 从那会开始的入门级600D+ 18-55 , 没几个月又添置了一枚55-250的中长焦.接着12年中全套出了, 貌似这也是第一次出让二手东西. 几千块的说多也不多, 说少也还要数上一会, 但是很傻b, 当时直接就在路边\u0026rdquo;洋洋洒洒\u0026rdquo;地点数几十张百元大钞, 也完全没担心会有假钞啊什么的情况, 也老老实实的把一些附件, 卡啊包啊什么的送给买的那人.想着留着也用不上╮(╯▽╰)╭ 说起来, 出的原因比较奇葩.记得是因为跟小泡他们去乌镇, 然后在东栅的不爽经历.主要是当时人真Y的多爆了.完全是跟随着人流在缓缓挪动前进, 加之天气又热, 然后我把600D套挂在身上, 完全没兴致按快门, 还觉得自己特傻逼.\n尽管后来西栅还觉得OK, 但是东栅那印象太\u0026rdquo;深刻\u0026rdquo;了\n回来第一件事就是卖了 600D 全套. 好吧, 其实这不能怪 600D, 完全是我一时脑热了.回看第一台拍出的照片, 的确也就跟用高端的DC的样子, 构图乱, 曝光不准, 后期渣.关于构图, 一直没做到画面简洁这一点, 经常贪心地想把什么都收进画面!自然纷乱是必然的了.\n后来没过两个月, 心里又痒痒的没相机玩, 然后刚好心思思索尼的单电A77(其实最开始是想着nex7或Xpro-1的, 还是因为钱, 靠.大爷的, 哪有什么选择困难症, 其实就是缺钱), 于是又入了一台A77+18-55(没入恒定F2.8那枚只因太屌丝了)\n要不是遇上毕业季, 这台机子肯定是拍的最少的吧, 结果却好像是拍的最多的一台. 后来, 拍完毕业照什么的, 又才入了个50 1.8的大光定.转悠了几天的广州, 大概一万快门略多之后7月份又卖了.\n至于为什么要卖, 其实到现在我也没想明白, 可能是想体验下传说中的全幅, 刚好 6D 和 D600 的价格还合适吧.\n本来想着耍泥坑的D600的, 因为大灯泡据说很犀利, 另外想着能玩遍佳能索尼尼康, 不过传说的快门掉渣很严重, 于是又回到了佳能, 拾起6D. …6D买的是单机, 本来配的50 F1.8, 后来用没多久, 觉得手感太渣出了买了85 F1.8 , 然后用途单一了又出了, 现在想来其实更喜欢50F1.8那个人类光学精华多一些.出了之后买了24-105 .传说中最渣的红圈, 的确有狗定胜牛变得感觉, 好吧, 24-105也还算不上牛变. 说实话, 传说中的全副也不过如此.有那样的条件aps-c也照样出片.当然, 高感确实流弊, 6D把ISO开到6400都杠杠的, 极端条件开到过万也能使.\n然后没贡献个几千次快门就出了.只因, 骚尼的黑科技\n其实期间一直都无敌想耍耍RX1的.还是钱包不堪只能独自YY.随身携带6D对我而言只有理论上的可能, 尽管我为了这个目的专门买了NG 5270, 也还是不太现实.\nA7 / A7R 的出现, 6D果断就上架了, 然后\u0026hellip;\nILCE-7 是要到碗里来了吗?\n这一做法, 牛总直接喷我是傻逼了. 前几天拿着6D+24-105出去, 好些个朋友纷纷说专业, 至少看上去专业.问之如何个专业法？答之相机够大, 越大越专业.比起看上去专业, 我还是拿个小微单好了.虽然比拉风的话被6D套甩开N条街, 看上去也业余, so what, 本来就是业余撒!不过我这是要换了A7, 牛总估计也都不好意思让我这个业余再去给他客串摄影师了.但话说回来, 以这玩机心态, 说不定哪天会抬一台a99回来的叻! 一直都有双机的计划.\n总的说来, 这其中进进出出的倒腾必然带给我差价的亏损, 朋友很多一听我说又要卖器材就说比我原价买的时候亏了多少多少, 但是比起学到一些照相知识, 比起得到一些照片, 比起满足了玩相机的乐趣, 那些损失真心算不上什么, 反倒是觉得我赚了.把机器珍藏在家里不用, 等到掉价了那才叫损失吧.(这算不算吖Q 呢？)\n帮亲朋好友拍过婚礼, 自己出玩行行摄摄, 从 Flickr 可以看出, 除去那会600D的一部分照片遗失了之外, 从头到尾大概两万次快门多些, 带给我的, 留下来的只有不到3000张照片.其中绝大部分都是以记录为名, 能称的上好片的为数不多, 稍微好一点的放到了500px, 也就那么不到百张.不过拍照不大都是要记录吗？只不过因为业余兴趣所以用上了比手机要好的机器罢了.\n再骂自己一句, 啥时候我也变成数毛党, 百分百党了, Y的看到各种样片还各种 download 之后放大到百分百.我烧坏了定.\nUPDATE 帅到掉渣的 Df 一出, 我被迷的一塌糊涂了. (千万别真的跟 D600 那样掉渣就好).\n可是帅到没朋友啊, 单机官方价16000+, 比A7贵出一截, 我这是要卖身的节奏啊!!!!!!!(A7 的性价比巨高, 减去各种附送的, 套机才10000略多点) , 但是真的喜欢那个造型.\n单看参数, 数码科技等各方可能被 A7 完爆(出片未必) 什么不能拍视频, 1\u0026frasl;4000 快门, 对我来说都是浮云!( wifi 可能有点吸引) 但是 Df 这个造型秒了 A7 九条街啊(A7 样子其实也不错)\nPS 有人说这是定位给喜欢胶片的人. 然则, 表示完全不喜欢胶片, 我是反胶片党, 更恶心提什么胶片感的.\n再UPDATE 虽然喜欢 Nikon 的 Df, 但无奈钱包不足够宽松, 本来我已经做好吃螃蟹的准备, 但是国内奸商报价24400, 虽然知道很快会降下来, 但是这很快也得等上一阵子\u0026hellip; 想过走海淘的, 日亚上卖 250000JPY, 折合CNY才15500左右. 顶多就是以后用英文菜单把逼格再拉高一档罢了. 但是, 走海淘由于某些原因, 还是要等, 于是, 折腾了一圈, 回来选了A7R, 被 金属做工(据说也看不大出)+去低通36mp 吸引了.好吧, 反正我就是那么容易被相机吸引的.\nDf 等以后降到合理价了买来玩双机好了.\n不过这个 A7R也纠结了好一会, 官方居然不给订狗头套, 加个35 F2.8的头要18000+ , 艾玛, 这又赶上 Df那个价了! 蛋疼! 后来有发现7R 搭狗头的, 于是就这么个奇葩玩法了.希望不会浪费那块 36MP 据说流弊到爆的 CMOS\n600D 用了不到一年, A77 用了大概9个月, 6D, 不到三个月, A7 略惨烈, 死在订单中, Df 更惨烈\u0026hellip;\n不折腾会死玩相机星人, 往后还是不折腾机身镜头了, 把附件搞上去再多出去玩儿才是王道 ╮(╯▽╰)╭ 有了闪灯或是脚架会比倒腾这些机身镜头来的更有意义, 不过那也是不小的开销(没人逼你买7个顶级灯来召唤神龙的!!!)\n","permalink":"https://xguox.me/camera-geek.html/","tags":["Photo"],"title":"不折腾会死玩相机星人"},{"categories":["Jabber"],"contents":"http://m1.file.xiami.com/259/58259/515557/1771031189_3321387_l.mp3\n忽然发现, 曾经买过一本Rails书, AWDWR. 然后,落了在上海. 第四版的,这一版本的中译本封面真心难看,跟英文原书完全不同了.\n其次还落了的是一个杯子.\n当时选择买这个杯子的原因是因为上面的20, 刚好那时20岁. 这没一会. 我就即将22岁了. 还记得这个杯子比一般的要高出个七八厘米,目测也是20cm左右, 也特厚特重. 但是用久了,习惯了以后就没感觉了. 等到我回来家里. 刚开始的那些天拿起家里的杯子各种轻, 那种感觉特别明显. 但是, 时间过去, 我也又熟悉了家里的这个这么轻的杯子,忘记了那个20的重量.\n也不知道还有啥落了,估计有也不是什么重要东西, 滚回来一年了.\n","permalink":"https://xguox.me/a-sudden-consciousness.html/","tags":["Jabber"],"title":"一本书和一个杯子"},{"categories":["Ruby"],"contents":" 不把这些当做笔记卸写下来总隐隐约约觉得自己没掌握, 所以还是继续做笔记.\nProc lambda 在1.9之后的写法(之前的也可以用)\n1  lamb = -\u0026gt; { }   proc 对象 和 block 对象之间的转换.\n调用方法的时候参数前加 \u0026amp; 1 2  tweets.each(\u0026amp;printer) # 把 proc 对象 转换成 block 对象 each 后面的是 block ,不是传参数   定义方法的时候参数前加 \u0026amp; 1 2  def each(\u0026amp;block) # 把 block 对象 转换成 proc 对象, 把 block 转换成 proc 才能作为参数   PS. method 对象转换成 block 对象\nsymbol tweets.map { |tweet| tweet.user }\nSame as\ntweets.map(\u0026amp;:user)\nblock_given? closure 1 2 3  def tweet_as(user) lambda { |tweet| puts \u0026#34;#{user}: #{tweet}\u0026#34; } end   当 lambda 被创建后局部变量(在这即user) 被保存起来\n1 2 3 4 5 6 7  gregg_tweet = tweet_as(\u0026#34;greggpollack\u0026#34;) # 相当于创建了 lambda { |tweet| puts \u0026#34;greggpollack: #{tweet}\u0026#34; } gregg_tweet.call(\u0026#34;Test!\u0026#34;) # greggpollack: Test!   self 1 2 3 4 5 6 7 8 9  puts \u0026#34;Outside the class : #{self}\u0026#34; # Outside the class : main class Tweet puts \u0026#34;Inside the class : #{Tweet}\u0026#34; end # Inside the class : Tweet   class_eval sets self to the given class and executes the block instance_eval sets self to the given instance and executes the block ","permalink":"https://xguox.me/relearn-ruby-iii.html/","tags":["Ruby"],"title":"重拾Ruby (III)"},{"categories":["Ruby"],"contents":" 继续复习 + 笔记, 今天要写的是 Dynamic Classes \u0026amp; Methods\nStruct 一般的, 定义一个类的做法如下：\n1 2 3 4 5 6 7 8 9  class Game attr_accessor :name, :year, :system def initialize(name, year, system) self.name = name self.year = year self.system = system end end   而鉴于以上这个例子的数据结构比较简单, 所以其实我们可以使用Struct来写之:\n1  Game = Struct.new(:name, :year, :system)   如果需要添加实例方法则可以这么干:\n1 2 3 4 5  Game = Struct.new(:name, :year, :system) do def to_s \u0026#34;#{name},#{year},#{system}\u0026#34; end end   如果需要定义的 data 比 behavior 要多的话, 推荐使用 Struct 来创建类, 反之则使用传统的方法.\nalias_method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Timeline def initialize(tweets=[]) @tweets = tweets end def tweets @tweets end def contents @tweets end end   由于 tweets 和 contents 两个方法体其实是一样的只是方法名不同, 为免重复我们可以这么干\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  class Timeline def initialize(tweets=\\[\\]) @tweets = tweets end def tweets @tweets end #实际上其实这是getter方法,所以其实这里可以这么写 # attr_reader :tweets alias_method :contents, :tweets end   更多例子, 假设如下在 Timeline 类\n1 2 3 4 5 6 7 8  class Timeline attr_accessor: :tweets def print puts tweets.join(\u0026#39;\\n\u0026#39;) end end   我们需要给 print 方法添加 authentication .如果由于某些原因我们不想改动现有的方法的话, 可以重新打开Timeline 这个类,然后使用 alias_method\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  class Timeline alias_method :old_print, :print def print authenticate! old_print end #实际上就是旧有的 print 方法改名为 old_print, 而重写了 print 给它加上了 authenticate! #然后调用旧有方法 old_print def authenticate! # do sth authentication end end   但是,需要注意的是,重新打开一个类是个危险的做法. 所以, 另一种做法是使用继承.\n1 2 3 4 5 6 7 8 9 10  class AuthenticatedTimeline def print authenticate! super end def authenticate! # do sth authentication end end   好吧, 尴尬了,貌似 alias_method 没看到什么更实际的意义了 =.=\ndefine_method 下边例子, 可以看到比较多的重复代码,\n1 2 3 4 5 6 7 8 9 10 11 12  class Tweet def draft @status = :draft end def posted @status = :posted end def deleted @status = : deleted end   使用 define_method 可以杠杠的减少这些重复. 并且,当需要添加新的 state 时候只需添加到 states 数组中即可.\n1 2 3 4 5 6 7 8 9  class Tweet states = [:draft, :posted, :deleted] states.each do |status| define_method status do @status = status end end end   send 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  class Timeline def initialize(tweets) @tweets = tweets end def contents @tweets end private def direct_messages end end tweets = %w{ \u0026#39;Compiling!\u0026#39; \u0026#39;Bundling...\u0026#39;} timeline = Timeline.new(tweets)   timeline.contents 一般的, 我们是这么调用 contents 方法.\n除此外,我们还可以使用 send\ntimeline.send(:contents)\n等同于\ntimeline.send(\u0026quot;contents\u0026quot;)\n另外,我们还可以用 send 来调用 private 或者 protected timeline.direct_message\n如果不希望调用 private 和 protected 的方法则可以用 public_send\n尴尬,更具体用途有待挖掘.\nmethod 方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  class Timeline def initialize(tweets) @tweets = tweets end def contents @tweets end def show_tweet(index) puts @tweets[index] end end tweets = [\u0026#39;Compling!\u0026#39;, \u0026#39;Bundling...\u0026#39;] timeline = Timeline.new(tweets) content_method = timeline.method(:contents) # =\u0026gt; #\u0026lt;Method: Timeline#contents\u0026gt; content_method.call # =\u0026gt; [\u0026#39;Compling!\u0026#39;, \u0026#39;Bundling...\u0026#39;] show_method = timeline.method(:show_tweet) # =\u0026gt; #\u0026lt;Method: Timeline#show_tweet\u0026gt; show_method.call(0) # =\u0026gt; Compiling! (0..1).each(\u0026amp;show_method) # =\u0026gt; # Compiling! # Bundling... # 通过 \u0026amp; 把 method 对象转换成 block # same as # show_method.call(0) # show_method.call(1)   在 Ruby 中, 任何东西都是 object, 任意的一个方法同样,也是一个 object\nPractice 重构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Library attr_accessor :games def each(\u0026amp;block) games.each(\u0026amp;block) end def map(\u0026amp;block) games.map(\u0026amp;block) end def select(\u0026amp;block) games.select(\u0026amp;block) end end   好吧,没掌握熟练,各种转晕了\n","permalink":"https://xguox.me/relearn-ruby-2.html/","tags":["Ruby"],"title":"重拾Ruby (II)"},{"categories":["Ruby"],"contents":" 真心尴尬,学习并使用了一年多的 Ruby/Rails,然后些个月不碰就忘了大半了.原本就不高端,这一回滚跟重头再来差不多了. 语法还能快速过了一遍.(主要通过 CodeSchool) 元编程,DSL那一块接下来可要费点劲了,毕竟原先也没完全掌握到.\n今天学到两个之前没注意的 tricks.\n一个是关于to_s的,另外个是关于 Module .\nto_s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  class Game attr_accessor :name, :year, :system attr_reader :created_at def initialize(name, options={}) self.name = name self.year = options[:year] self.system = options[:system] @created_at = Time.now end def to_s self.name end def description \u0026#34;#{self}was released in #{self.year}.\u0026#34; end end class ConsoleGame \u0026lt; Game def to_s \u0026#34;#{self.name}- #{self.system}\u0026#34; end end   其中的字符串插值#{self}其实相当于隐式调用了to_s,在这里也就是self.to_s 好吧, 我傻逼了,字符串插值当然是不管啥都执行to_s. 不过我想说的不是这个.而是牵扯到puts和p\nputs and p and inspect 调用一些例子:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class A def initialize(string, number) @string = string @number = number end def to_s \u0026#34;In to_s:\\n#{@string}, #{@number}\\n\u0026#34; end def to_a \u0026#34;In to_a:\\n#{@string}, #{@number}\\n\u0026#34; end end puts a = A.new(\u0026#34;hello world\u0026#34;, 5)   输出\n1 2  In to_s: hello world, 5   这里的to_s在对象被实例化之后隐式调用了\n然后是\n1 2 3 4 5 6 7 8 9  class Item def inspect \u0026#34;Result of inspect\u0026#34; end end puts Item.new puts Item.new.to_s p Item.new   输出\n1 2 3  #\u0026lt;Item:0x8f85568\u0026gt; #\u0026lt;Item:0x8f8552c\u0026gt; Result of inspect   puts通常输出的是 对象调用to_s的结果,而p则是直接输出inspect\ninspect vs to_s 另一个例子\n1 2 3 4 5 6 7 8 9 10 11  class Item def initialize(item_name, qty) @item_name = item_name @qty = qty end end item = Item.new(\u0026#34;a\u0026#34;,1) puts item p item   输出\n1 2  #\u0026lt;Item:0x8f85388\u0026gt; #\u0026lt;Item:0x8f85388 @item_name=\u0026#34;a\u0026#34;, @qty=1\u0026gt;   可以看到, puts打印出类名然后跟着一个十六进制的数,这个数字跟该对象在内存中的存储位置相关,不过我们很少会使用到.\n而p的话是打印出类名及该对象的所有的实例变量,这个在debug的时候非常有用.\n上面这个例子说明了p和puts的不同,但是,有时候你会想要自定义这些方法的输出形式. 这个可以通过重写to_s来完成.\n如下面的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Item def initialize(item_name, qty) @item_name = item_name @qty = qty end def to_s \u0026#34;#{@item_name}: #{@qty}\u0026#34; end end item = Item.new(\u0026#34;a\u0026#34;,1) puts item p item   输出\n1 2  a: 1 a: 1   此时,p和puts是相同的,因为to_s被重写,Ruby会把它当作默认的inspect结果.除非你重写inspect方法. 如前边的例子\n参考RubyMonk\nhttp://stackoverflow.com/questions/12040527/Ruby-automatically-calls-to-s-method-when-object-is-created/19751192#19751192\nModule 一般的,把 module 的方法添加到类中有两种方式(用途也不同).\n一种是使用 include 添加后作为实例方法; 另一种是使用 extend ,对应的是作为类方法使.\n当然,这是比较多用到的, 再还有一种则是扩展单个对象. 直接 copy 官方文档的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  module Mod def hello \u0026#34;Hello from Mod.\\n\u0026#34; end end class Klass def hello \u0026#34;Hello from Klass.\\n\u0026#34; end end k = Klass.new k.hello #=\u0026gt; \u0026#34;Hello from Klass.\\n\u0026#34; k.extend(Mod) #=\u0026gt; #\u0026lt;Klass:0x401b3bc8\u0026gt; k.hello #=\u0026gt; \u0026#34;Hello from Mod.\\n\u0026#34;   当需要作为类方法和实例方法添加到类当中的时候, 当然, 可以同时使用include和extend, 不过还有另一种简便一些的方法 Hooks - self.included\n比如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  module LibraryUtils def self.included(base) base.extend(ClassMethods) end def add_game(game) end def remove_game(game) end module ClassMethods def search_by_game_name(name) end end end class AtariLibrary include LibraryUtils end # 当前 module 被 base 包含 # module名 ClassMethods 可以任意取   最后, 还有可以通过调用ActiveSupport::Concern.参考 Rails 文档 典型的 module 是:\n1 2 3 4 5 6 7 8 9 10 11 12  module M def self.included(base) base.extend ClassMethods base.class_eval do scope :disabled, -\u0026gt; { where(disabled: true) } end end module ClassMethods ... end end   使用 ActiveSupport::Concern 上边的例子可以这么改写:\n1 2 3 4 5 6 7 8 9 10 11 12 13  require \u0026#39;active_support/concern\u0026#39; module M extend ActiveSupport::Concern included do scope :disabled, -\u0026gt; { where(disabled: true) } end module ClassMethods ... end end   此外, 它可以优雅地处理 module 之间的依赖. 如下例子,假定 module Bar 依赖于 module Foo. 通常的我们会这么写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  module Foo def self.included(base) base.class_eval do def self.method_injected_by_foo ... end end end end module Bar def self.included(base) base.method_injected_by_foo end end class Host include Foo # We need to include this dependency for Bar include Bar # Bar is the module that Host really needs end   但是,Host为嘛要关心Bar 的依赖 Foo呢?为啥不直接在Bar里头引入Foo呢?\n1 2 3 4 5 6 7 8 9 10  module Bar include Foo def self.included(base) base.method_injected_by_foo end end class Host include Bar end   不幸的是,这不起作用. 因为当Foo在Bar中included 的时候,Foo 中的base实际上是Bar module,而不是 Host类.这时候用ActiveSupport::Concern的话就可以杠杠地解决这个依赖问题.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  require \u0026#39;active_support/concern\u0026#39; module Foo extend ActiveSupport::Concern included do def self.method_injected_by_foo ... end end end module Bar extend ActiveSupport::Concern include Foo included do self.method_injected_by_foo end end class Host include Bar # works, Bar takes care now of its dependencies end   最后,附上included源码\n1 2 3 4 5 6 7 8  # File activesupport/lib/active_support/concern.rb, line 118 def included(base = nil, \u0026amp;block) if base.nil? @_included_block = block else super end end  ","permalink":"https://xguox.me/relearn-ruby.html/","tags":["Ruby"],"title":"重拾Ruby (I)"},{"categories":["Tools"],"contents":" 好的开发人员总是在寻找可以让自己的工作流程更快速,更自动化的方法. 这次,我们带来的是Alfred的一系列workflows,它们可以极大的提高你的开发效率,相信你会为之震惊.\nWhat is Alfred? 对于大多数了解不深的人来说,Alfred只是一个备受赞誉的Mac OS X app, 它可以快速地帮助你查找在线或者本地的文件. 最新的版本 Alfred 2更是带来了大量的改进,尤其是其中的Powerpack,可以让你创建自己的workflows(工作流程).\n下面你将会看到这些精心挑选出来的,能够改变你的工作方式的workflows.\nOpen With Sublime Text 强大的文件,目录搜索能力是Alfred所最让人喜爱的功能之一. 那如果我们想要利用它使用自己喜爱的编辑器(比如说Sublime Text 3)打开文件或者目录呢?\n Open With Sublime Text (v3), developed by @franzheidl 下载 触发: subl,subl*  想要其他编辑器的话请猛击 Extras\nXguoX: 可能更多人还是选择在终端直接用命令敲开.\nCan I Use\u0026hellip; Workflow 在HTML5时代,当使用某一个CSS属性或者JavaScript APIs之前,你需要检查知道浏览器是否支持. 当然,你可以打开浏览器,直接浏览Can I Use\u0026hellip;这个网站, 然后搜索某个关键字从而看看浏览器的支持情况.除此外,你还可以使用这个Alfred Workflow.\n Can I Use\u0026hellip; Workflow,developed by @willfarrell 下载 触发: caniuse  XguoX:搜索速度还行\nDash Workflow 没有人会知道某一门语言或者某一个框架的所有.我们时常会需要查找某个特定的method如何使用.最近,发现了一个非常惊艳的app,Dash,在本地查找各类APIs文档,完全离线的说. 这还不算啥,通过这个workflow,你可以通过过滤各个语言(框架)关键字来查找相关文档.这个流弊的app可是免费的哦,所以,在使用这个workflow前请先安装好Dash.\n Dash Workflow,developed by @willfarrell 下载 触发:dash html css gem angularjs Rails 基本上常用的语言框架库都有了  XguoX:Dash 真心很流弊!巨赞!!!\nTerminal Finder 一些操作我们可能会希望在终端完成,而另一些则希望在Finder完成.这个workflow可以流畅地在这两者之间转换.在终端(iTerm)中打开当前的Finder窗口,反之亦然.\n TerminalFinder, developed by @LeEnno 下载 触发: ft tf fi if  XguoX:又是一个巨实用的workflow\nPackage Managers Workflow 代码复用是软件开发的一个重要组成部分,现如今我们有很多的方案来构建我们的代码以及搜索使用第三方软件包. 想要使用某个Node.js module? Grunt task?通过这个workflow,你可以快速简便地在一个地方通过你想要的包管理器查找到你想要的插件或者组件.\n Package Managers Workflow, developed by @willfarrell 下载 触发: bower grunt npm composer gems pear pypi cocoa brew alcatraz rpm maven docker  ##Colors 不用再每次想要转换某个颜色格式的时候打开Photoshop了.通过这个workflow可以很轻易在HEX, RGB, HSL这些个颜色格式之间转换.\n Colors, developed by @TylerEich 下载 触发: # rgb hsl c  Jenkins Workflow 做单元测试固然是好,但是每更改一行代码就手动跑一次测试的话会让人抓狂的. 为了得到更好的代码质量,我们需要跑跟更多的测试,或者至少的自动运行那些我们已经在跑的测试. 这就是为嘛持续集成系统那么重要.通过这个workflow,你可以列出Jenkins的所有工作以及它们的状态.\n Jenkins Workflow for Alfred v2, developed by @jeroenseegers 下载 触发: jenkins status  XguoX:好吧,这玩意没接触过\nOpen in FileZilla 目前来说传输文件到Web服务器的最流行方式还是使用FTP. 而这个workflow可以帮助你快速地通过FileZilla连接到远程服务器端. FileZilla也是一个免费的应用,所以,在用这个workflow之前请记得先安装之.\n Open in FileZilla, developed by @jeffmagill 下载 触发: fz  在用其他FTP客户端吗?请猛击 Extras\nDomainr Workflow 不想错过一些帅气的域名的话,可以通过Domainr APIs快速查找.\n Domainr Workflow, developed by @dingyi 下载 触发: domainr  Encode / Decode 有时候, 我们需要把一些UTF-8字符转换成HTML编码,或是解码某个URL. 使用 Encode / Decode , 这些杂碎的事情将不再需要浪费那么多的时间了.\n Encode / Decode, developed by @willfarrell 下载 触发: encode decode  Font Awesome Workflow Font icons很好很强大, 只需简单地输入类似的\u0026lt;i class=\u0026quot;my-icon-name\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;. 但问题是,我们经常没能准确地记住我们需要的某个icon的类名,以至于老需要去翻看文档. 现在的话通过这个workflow我们可以很轻易地查找到Font Awesome的icon集.\n Font Awesome Workflow for Alfred 2, developed by @ruedap 下载 触发: fonta  XguoX:赞!\nSource Tree Workflow 有人习惯在终端使用Git命令, 也有喜欢使用GUI工具. 如果你属于后者,那么Source Tree workflow 可以帮你列出,查找,打开Git仓库. Source Tree 也是需要在使用这个workflow之前先下载安装的应用.\n Source Tree, developed by @zhaocai 下载 触发: st stbookmark  使用其他的Git客户端吗?请猛击 Extras\nGitHub Workflow 如果你最喜欢的社交网站是Github,那你一定会想要看看这个.简单快速地查找并在浏览器打开Github上的仓库.\n GitHub Workflow, developed by @gharlan 下载 触发: gh  ##StackOverflow Workflow 在StackOverflow搜寻各类编程问题的答案\n StackOverflow Workflow, developed by @xhinking 下载 触发: st  TimeZones Workflow 现如今,很多的团队的成员纷纷来自全球各地. 那么,我们不会希望在同事的下班时间去打搅人家.所以,在这之前,我们总会先查看一下对方的当地时间.这个workflow可以巨方便地列出世界各地不同城市的当前时间.\n TimeZones Workflow, developed by @CarlosNZ 下载 触发: tz  VirtualBox Control 很不幸地,跨浏览器兼容的仍然是开发人员所面临的一大问题. 测试你的网站在不同浏览器 \u0026amp; 不同操作系统 是否运行正常是件无法逃避的事. 使用虚拟机(比如VirtualBox)是当下流行的,可以完成这事的一种方式. 好吧,在用这个workflow之前老规矩,先装上VirtualBox.\n VirtualBox Control, developed by @aiyodk 下载 触发: vm  在用其他的虚拟机客户端吗?请猛击 Extras\nCreate Your Own! 所有的这些workflows都非常的赞并对于大多数人来说很有帮助. 但是,每个人的工作方式不尽相同. 所以我们需要创建真正属于自己的workflow. 其实这个也很是简单. 以下的这个例子仅需不到10秒,就可以创建一个workflow来自动搜寻Smashing Magazine.\nWant More? 这是作者收集的一些 此外,Alfred的官方论坛上也有海量的workflows.\nThe End? 一堆帮助你自动化工作流程的技巧,很赞对吧!希望能够对你有所帮助.可能这些会激发你的一些灵感,从而创造,分享你的隐藏技能.\n如果你喜欢的workflow没有在这列出,可以在下边的评论当中跟我们分享. 如果你觉得上边提到的那些赞到爆的话,也可以告诉我们哦!\n文章翻译自 Hidden Productivity Secrets With Alfred\n","permalink":"https://xguox.me/hidden-productivity-secrets-with-alfred.html/","tags":["Tools"],"title":"Hidden Productivity Secrets With Alfred"},{"categories":["Jabber"],"contents":"尽管在我的个人Tags里边经常标的无非那几个,Code,摄影,One Piece, 也无碍接下来抱怨几句最近的OP(动画).\n追着OP的看官不难发现,最近动画明显是各种拖剧情,某很多些神情,某大量的回忆篇加强插入,某动作刻画的时间,都Y的太太长了.而且还不少不断重复的画面,镜头来回转换加深刻画.\n最近几集没有深究,好吧,表示开始习惯了. 印象最深的是610话, 标题明明是 两位中将的xxxxxx 结果整集下来一半的镜头给了 M, 剩下的一半当中有四分三是在各个场景之间切换, 真正看维尔戈跟斯摩格过手不知道有没5招,目测镜头时间不超过3分钟!!!\n表示理解动画要拖拖剧情等漫画,但这拖得略不靠谱了吧.如果只是为了等漫画,那我觉得偶尔直接来个停播会好点呢?完全可以理解啊,就像漫画前不久也停刊好几回,当然那是照顾到尾田大神的身体状况,不管怎样,完全可以理解.又或者跟以往那样来些个什么番外篇打酱油,也都总比这样把一集的剧情拖沓成这样要好.\n相比之下,漫画的剧情引导显得顺多了. 以「顶上战争」作为分界的两年后的新世界篇,在漫画里头与两年前的剧情相比有过之而无不及.但在动画里头,给我的感觉却是,比之两年前的剧情,那是相去甚远,之所以产生这种心理,应该是缘由这剧情拖沓.反正1 - 570+的动画重看了N次,没有一次说是强烈地觉得剧情拖沓跳过哪一集.而今,好几集好几次我都想按快进 =.= 但还是没有按下去. 难道,是因为时间的作用使之沉淀下来,难道现在的剧情也要等过上些时间沉淀才会打消这些感觉?\n最后,我发觉并深感OP里最喜欢的不是Luffy君,说不上谁是最喜欢的角色,但,可以肯定,对 罗宾酱\u0026amp;索大 的喜欢更多一些.\n","permalink":"https://xguox.me/one-piece-recently.html/","tags":["Jabber"],"title":"由OP最近的拖剧情说起"},{"categories":["Jabber"],"contents":"好多年以前,在「街头篮球」这款游戏看到过的一句,「一天不碰球,就一天没长进」. 而今,我是好几个月没真正意义的code,所以,这状态,想不roll back都难,更何求长进.\n真心,重头来过也忽然无从下手.\n打开Octopress准备更新,发觉连Git的一些命令的想了好久才想起来.\n看到Git的提示里冒出有\ngit bisect reset\n这货功能很强大呀,昏了,之前怎么完全没印象有过着玩意的?又是学艺不精啊!\n想把Octopress更新到2.1,一直略期待着,却一直在preview状态,今儿却才发现,2.1的branch挂了,倒是多了个 3.0的branch.\n好吧,老规矩的话,那就换个新面孔好了.这不流行Flat-UI嘛.发现一个还不错的\u0026ndash;octopress-flat.加上自己做一些些个修改.\nand then 准备new_post的时候报了这玩意,奇葩了,之前zsh貌似不见报这错的啊?难道几个月不碰就变的面目全非了.\n zsh: no matches found: new_post \n搜之,在~/.zshrc 里添加\n alias rake=\u0026quot;noglob rake\u0026quot; \n前些时间落下了好多东西,包括刚着手的翻译行当还有初尝Meteor.\n在写简书那会,虽远远称不上精通全端、一锅端,但也绝不至于像现在这样什么都端不起来. 现在想再端起来,还要各种瞻前顾后的,连端啥都决定不好.\n想来,回上海或者北漂几乎没啥可能,尽管我很enjoy上海那段日子.so,目测往后也是在珠三角荡悠.要是能找到一份remote job,那就更好不过了.说起来,一直很想去佛山那家创业公司.前年还是他家boss指引我才开始知道并用上Github.现在是更想去了,因为他们支持远程工作 XD\nbut,现在的我比起11年那会还不行.这情何以堪.\n好吧.Anyway,一切都只能等痊愈再说!\n消沉够了就要回到正轨奋斗了.好好利用这几个月.\n接下来\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;..要不要,从HTML重新开始呢呢呢,不知道俺的Ruby/Rails 还剩几多斤两呢, 好吧,都不是, 从工具开始, 回归使用Vim好了\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;.\n","permalink":"https://xguox.me/back-to-code.html/","tags":["Jabber"],"title":"Back to Code"},{"categories":["Photo"],"contents":"那时候,你们在想着什么,仅仅只是摆着姿势那么简单吗? 你说我穿的太colorful了,没有你那么忧伤 要是我的最深处也这般colorful那该多好\n不知道你是望着天空还是闭着眼睛,我自己也忘记了 草地不很舒服,但给的感觉却让人享受着可以睡去\n背影总给人有沉重感,仿佛,这些年的影子都被呈现出来了.\n路,一个人走,两个人走\u0026hellip;回头,总能望见你\n路过的行人都不自觉就把我们设为焦点,或轻轻一笑,或心有所思,谢谢为我们合照的那个人. 说自己垃圾,说自己病痨 我情愿被你们这么说 也不要自己自嘲\n低头,抬头,仰望,俯视,在同一个位置\n你说,车站总给人有一种说不出的感觉. 我说,等,等待,等车,等人\u0026hellip;\n你想拥抱雨,正如我也那么想被雨穿透\n无力的文字已经不足以描述我有多爱这两张照片\n多谢路过的小师妹,一开始,因为我之前把ISO设置到了6400,还开着闪光灯,闪瞎了\u0026hellip;折腾了几下才发现问题. 小师妹问,你们是大四的师兄吧? 我可以这么回答吗?我们还在读高中!\n从未想过摘下腕带,尽管它们普通的甚至有些寒酸,也不是谁的珍贵礼物,只是我自己给自己的地摊货,但它们好像是替代你们看着我.\n","permalink":"https://xguox.me/photos-and-stories.html/","tags":["Photo"],"title":"照片的故事"},{"categories":[],"contents":" 关于D一族的真相: 路飞等人,一出场就有了与众不同的名字,即在名字当中比所有的普通人都多了一个D字,而海贼里不像现实中的美国等国,有MiddleName这个属性,事实上除了D一族的人以外,漫画中还没有出现过三个名字的人物.这真的是D一族自古流传下来的特殊起名方式吗?\n我的观点是,是流传下来的,但这个起名方式却是从800年前刚刚突然开始流传的.因为罗宾在读到海底森林的历史文本时,最后的署名是\u0026rdquo;乔伊波伊\u0026rdquo;,这个人物很明显与D一族息息相关但却没有D(该人物具体分析见后文).\n在漫画中,一个名字,两个名字的人物都数不胜数,而三个名字的人物只有D一族而已.我想D很可能是因为国家遭难后,被迫背井离乡的人们为了不忘记国家而特地加进名字当中的,而D很可能就是以前王国名字的缩写.这样一来乔伊波伊在历史文本的署名里没有D就很好理解了,因为800年前,他们的名字没有加入D,所以乔伊波伊留书给友人时不会平白加个D进去.\n这里容许我先把D的真相先放一放,分析一下古代兵器 古代兵器一共有3个,分别是冥王Pluton,海王Poseidon,以及天王Uranus. 其中的普鲁托和波塞冬的真相已经揭露,分别是一炮能轰掉一座岛的战船和能控制海王类的人鱼公主.天王乌拉努斯是什么呢?因为有了人鱼公主是波塞冬的先例,所以天王已经不局限于\u0026rdquo;兵器\u0026rdquo;的范畴里了,而且已经有了威力惊人的普鲁托,再有类似战斗力的兵器也有些多余,即便天王是所谓能在天空飞的船,用处似乎也不大(能源问题也无法解决),所以我认为天王不是真正意义上的兵器. 三个古代兵器都是D国的最高机密,但前两者都并非出自D国,如果三者都来自外界的话,D国似乎没什么理由成为独占这三个兵器秘密的国家.所以天王乌拉努斯应该来自D国,并且是人类.\n推理到了这里,就回头来看一下D国的名字吧.波塞冬这个海王之名既然来自人鱼中的尼普顿(海神)一族,那D一族自然就是天神一族了.如此一来我脑海中就浮现了一个D开头的天神的词汇Deus迪乌斯.如果是这个词,我觉得一切都能串联起来了,在奥哈拉的时候,尾田特意安排不让教授说出该国的国名,就是知道提前泄露了这个国名的话,那D的神秘就被揭开了大半吧.\n天王Uranus的真面目 我觉得该\u0026rdquo;兵器\u0026rdquo;势必要有与\u0026rdquo;天\u0026rdquo;有关惊人的能力,并且该能力必须跟人鱼公主那样,是某个独有的能力.而漫画至今为止,满足这些条件的能力只有一个了,可能有不少读者知道我在说什么了,没错,指的就是目前只显示了冰山一角的,海贼王罗杰的\u0026rdquo;聆听万物之声\u0026rdquo;的能力.所以,我的结论就是,古代兵器天王乌拉努斯,以前是海贼王罗杰,而现在指的则是路飞.\n有了这个结论之后,有很多东西都更为明朗了,比如我查阅之后发现,Uranus的含义不仅代表天空,还象征了希望与未来,这不正是路飞的特质么.\n而在顶上战争中战国元帅的异常反应,以及之前表现出的对D的异常警觉,我觉得也得到解释了.要知道,在新世界里霸气不说烂大街,但也不是稀有到极点吧,多弗拉明戈啥的都有霸气了,而G5小兵们的反应,也没有像他这样夸张.那为什么见多识广的战国会对个霸气,对个D族海贼如此惊讶呢?我想是因为他还知道一个事实,那就是,D一族的人如果拥有霸气,就有成为天王乌拉努斯的可能(就像海王波塞冬的能力局限于王族的人鱼).\n这也符合了战国对艾斯说的这一番话,这个资质,即不是指是个高手就有的霸王色霸气,更不是说能把武装色见闻色练到多强,指的其实是\u0026rdquo;聆听万物之声\u0026rdquo;这个能力.这里战国说的\u0026rdquo;海贼王\u0026rdquo;,指的就是\u0026rdquo;天王\u0026rdquo;,他不挑选四皇的任何一人(至少红发有霸气),而偏偏说艾斯会成为\u0026rdquo;海贼王\u0026rdquo;,不是说艾斯的能力或战斗潜力,而是因为他是D族,惧怕他成为\u0026rdquo;天王\u0026rdquo;.\n就像在20年前,罗杰被冠上了\u0026rdquo;海贼王\u0026rdquo;的名号,就实力来说,也并没有说过罗杰就比金狮子,白胡子等人强了.只是去过拉夫德鲁就被封海贼王,是否有点不妥呢.我认为,这其实是知道真相的ZF高层赋予的名号,五老星清楚的知道,罗杰是古代兵器天王乌拉努斯,也惧怕他的意志,他的能力,于是把跟ZF敌对的\u0026rdquo;天王\u0026rdquo;冠上了\u0026rdquo;海贼王\u0026rdquo;的头衔,让普通民众去憎恨这样一个存在.\n这种表达与真意有差别的说话方式,也是尾田这个老狐狸一直玩的把戏了.听起来就像是说艾斯实力会变强到成为海贼王啥的,知道内情的人才知道这是指天王的能力会觉醒.同时我也了解了为什么尾田一定要安排艾斯在这里死去,安排路飞非要在2年后才觉醒了这种能力.因为就像波塞冬一样,天王乌拉努斯在一个时代只会出现一人,这样一来,与路飞同样是D一族,性格相同,做法类似,同样意志的艾斯,很可能会先于路飞一步觉醒该能力,故事就变成艾斯传了.\n同样的,知道真相的白胡子在临死前说罗杰等的不是蒂奇,这也许是因为只是D族,只有霸气还不够吧,还需要继承D的意志,三个齐全了才能觉醒.\n一窥天王乌拉努斯能力的一角 综合雷利的说法,他们不认识历史文本的文字,但在空岛上,罗杰切切实实的说将文字(而非石头本身)带去了目的地,并且在边上的黄金上留下也是这种文字!\n再结合漫画中明确透露的该能力能听懂海王类的对话(无法像波塞冬对其进行召唤与命令).我想,这个\u0026rdquo;聆听万物之声\u0026rdquo;的能力应该没有任何夸张,确实它能听到\u0026rdquo;万物\u0026rdquo;的声音,万物包括记录历史文本的石头.\n这样一来,这块无法破坏的石头为什么能刻上文字也能说得通了,即便D国有在石头上刻字的技术手段,也应该不会让工匠来操作.更何况海王,冥王这种情报,我觉得应该是除了\u0026rdquo;王\u0026rdquo;以外不该泄露给任何人的信息.这样,能在石头上写上字的人乔伊波伊自然就是原初的\u0026rdquo;天王\u0026rdquo;,那该如何写字呢,石头无法破坏,罗杰更是不认识这种字.这里只能运用\u0026rdquo;听\u0026rdquo;的能力了,我认为,该能力能与万物进行\u0026rdquo;交谈与控制\u0026rdquo;,而石头黄金等物体会\u0026rdquo;听从\u0026rdquo;天王的命令,浮现他想记录的信息,也就是说,文本上的文字可以说是\u0026rdquo;石文字\u0026rdquo;,而罗杰那时就是靠这个能力,从石头那里\u0026rdquo;听到\u0026rdquo;了记录的内容,并\u0026rdquo;命令\u0026rdquo;旁边的黄金记录下该任务\u0026rdquo;已完成\u0026rdquo;的信息.\n能与石头进行对话的能力,从鱼人岛的谢罪文中也能一窥究竟.尼普顿很清楚的说了,这据说是写给人鱼公主的,但详细内容没流传下来,所以尼普顿不认识这些文字,国王都不认识了,很明显以前的王族都不会认识.但如果要写信的话,用大家都明白的文字写是常识吧.但偏偏石头上的还是这种鸟文,我想这也从侧面证明了石头上写字的人无法控制显示的字体,即无法人为的在这些石头上刻字.\n二十国攻打天神国的缘由及导火索 在800年前,D国就拥有了三个古代兵器的助力,又有\u0026rdquo;天神\u0026rdquo;的国名,很可能属于君临天下的国度.应该是世界各国的领导者.可惜的是,这个国家的施政方针让一些国家的贵族,王族产生了不满.\n就漫画中揭露的几个王族来看,可以清晰的看到两个极端,一种是像沙之岛和鱼人岛那样,王族与民众都相濡以沫,没有隔阂.一种就像天龙人,哥亚王国那样,尊卑分明,下等阶级被当做垃圾,当做奴隶.虽然各国有各国的做法,也算互不相干,但天神国作为世界各国的领导者如果这么做的话,问题就严重了,恰巧D国托付历史文本的两个国度都是没有明确的尊卑之分的国度,可以想象D国的王族也是类似这两个国家的.\n一直以来,都是号称\u0026rdquo;君权神授\u0026rdquo;,君王总是以\u0026rdquo;天子\u0026rdquo;的姿态,让民众敬仰,崇拜,服从.但若是各国领导者的\u0026rdquo;天神\u0026rdquo;一族的天王要与民众们平起平坐的话,那阶级思想根深蒂固的国王们以后该如何服众?为了保住自己的地位,\u0026ldquo;王\u0026rdquo;决定要挑战\u0026rdquo;神\u0026rdquo;,于是20国的联合军出发将神一网打尽了.\n而\u0026rdquo;神\u0026rdquo;虽然有冥王,有波塞冬相助,但如果他们以暴制暴,那就与初衷完全相悖了.于是他们选择了隐忍,将一众族人分送去世界各地,并留下了历史文本,希望D国逃难出去的人们能出现D意志的继承者,并能找到更好的方法来打破贵族与平民之间的鸿沟吧.与其说这是20国与D国的对抗,不如说是这个世界古旧的制度与新的思维之间的对抗,这就是一切的源头.\n导火索 我认为就是将鱼人岛移去地上的这个决定.\n罗宾在一开始就说了,鱼人岛的历史文本,是至关重要的.但如果仅仅是写给人鱼公主的私人信件的话,再怎么也重要不到哪里去.但是将各个线索串联起来看一下的话呢?\n奴隶制度,人鱼和鱼人被认为是鱼.在鱼人岛的偏偏是谢罪文,为何要谢罪,是因为无法完成约定.那也就是说,之前D国的乔伊波伊已经与尼普顿一族约定了要把鱼人岛移去地面上.但有波塞冬的力量,移动诺亚搬去海面不是轻而易举吗?为什么会做不到呢?\n这里我能想到的就是世界会议了,在过去的世界会议上,身为盟主的天神国决定帮助鱼人岛移去地上,在当年的世界会议上提出了该提案,遭到了对鱼人深度种族歧视的国王们的反对和憎恨,并导致了接下来的灭国.于是才有了这封谢罪文.而这个约定,也致使对D国积累了诸多不满的20个国家的国王终于下定决心要灭掉这个危险的存在.\n现在就期待着路飞,这个新时代的象征希望与未来的天王乌拉努斯,能亲自一个一个国家的进行调和,将D国的意志真正的融入到每个民众心中,一环一环的打破古老的枷锁,让世界的风向朝向自己.这样,一定能够打开崭新的局面吧.\n","permalink":"https://xguox.me/record-of-the-void-century-and-d.html/","tags":[],"title":"[Repost]关于D一族的真相以及历史文本的分析"},{"categories":[],"contents":"昨天是父亲节.\n半夜里,我突然想起来,曾经推荐过一个美国摄影师的作品.母亲去世后,他就拍他的父亲,还写下自己的感想.\n这组作品,我看过多次,昨天再看,竟然又一次被感动了.儿子对父亲的关心、对老年人衰老和孤独的感慨,都在其中得到了细腻的体现,非常感人.\n网上似乎还没有全文翻译,我就把它译了出来,再一次推荐给大家.\n=============================================================\n与父亲在一起的日子 作者:Phillip Toledano 译者:阮一峰 原文网址:http://www.dayswithmyfather.com\n1 这是一份日记,记录我的父亲.\n2 我希望留下我和他在一起的日子.\n3 2006年9月4日,母亲突然过世,将照顾父亲的重担一下子留给了我.父亲患有健忘症,经常不记得几分钟前发生的事. 我带着父亲参加了母亲的葬礼.但是回家后,每隔15分钟,父亲就问我母亲去了哪里.我不得不一遍遍地解释,她死了.父亲震惊极了,问我为什么没人告诉他?为什么不让他参加葬礼?为什么他从没去医院探望她?\u0026hellip;\u0026hellip;他全忘了. 于是,我改口说,母亲去巴黎照顾生病的舅舅了,暂时不回来.\n4 年轻时,父亲非常英俊,在好莱坞当电影演员.他不喜欢自己现在苍老的样子. 我让他看着镜子,想拍一张肖像.他看到镜中的自己,顿时变得非常不安. 你看,即使到了98岁,他还是有虚荣心.\n5 屋子里到处都是他留下的小纸条,上面写着\u0026rdquo;人都到哪里去了?\u0026ldquo;、\u0026rdquo;发生什么事了?\u0026ldquo;\u0026hellip;\u0026hellip;这显示了他内心深处的恐慌,他从不对我说这些.\n6 父亲握着我的妻子卡拉的手. 每当卡拉画眉毛或者穿新裙子,他都会注意到,他对细节很敏感.另一方面,他还赞美卡拉\u0026rdquo;身材好\u0026rdquo;,喜欢看卡拉穿短裙,我觉得很有意思.\n7 这是拉尔夫叔叔的照片.他是父亲的弟弟,去年死了,父亲还不知道.\n8 父亲经常对我说,他想死.他说是时候走了,他已经活得太长了. 很奇怪,我竟然有些赞同他的想法.他现在如此健忘,这样的生活根本不能叫做生活,而是一种折磨.但是,这个世界上,我的直系亲属只剩下他一个人了.\n9 这是我家的狗乔治.父亲不记得她的名字,就叫她\u0026rdquo;狗狗\u0026rdquo;. 晚餐时,父亲喜欢把菜扔到地上喂她(这总会吓到母亲).当乔治朝着地上的菜窜过来,父亲就会很高兴,夸她可以去拍电影.\n10 每天,父亲待在厕所的时间很长很长.因为他健忘,所以他可以在那里待上几个小时. 他常常在厕所里刚系上裤子,就说\u0026rdquo;等一等,我要去上厕所\u0026rdquo;.这让人又恼火又伤心.我对他说,你已经在厕所里一个小时了,他就会用一付难以置信的表情看着我.\n11 我喜欢他看报纸的那些时刻. 有那么几分钟,他很清醒,一切看上去又回到以前.那时,我们就不再称母亲去了巴黎,而是说她去买吃的了,一会就回来.那是一天中的甜蜜时刻.\n12 吃饭总是一个大问题.我现在明白,为什么母亲去世之前,总是烧一模一样的菜,因为父亲只吃那个. 一般情况下,他只吃鸡蛋,炒鸡蛋、鸡蛋色拉、鸡蛋汤等等.奇怪的是,他的胆固醇一点也不高.\n13 父亲是一个幽默的人.我把两块饼干放在他的胸口,他说\u0026rdquo;快来看我的奶头\u0026rdquo;.这难道不好笑吗?\n14 父亲退休后,依然保持对艺术的热爱.他用大量的时间听歌剧、画画、雕塑. 虽然现在他不再画画了,但是依然向往艺术.他对日落很着迷,说这些色彩可以画出很多画.虽然身体不行了,他的心还是年轻的.\n15 父亲是讲故事的高手.从小我就喜欢听他的故事,他的表演可是奥斯卡级别的. 如果他心情不好,我就请他讲一个故事,他会很快进入角色,把自己忘了.\n16 今天,他过98岁生日.\n17 我让他拍一张我的照片.我想知道,他能看出我有多爱他吗. 我手上的戒指,是母亲去世那天戴的.\n18 今天是母亲的生日.我总是记不住这个日子,但是今天早上我梦到了她.她在笑. 要是她还活着,今天就是81岁了. 妈,生日快乐.\n19 父亲很在意别人给予他的关心. 每次我去看他,他都把这当做一件大事,总是说他有多爱我.在他眼里,我是一个天才,而卡拉也是我们家当然的一份子. 我真高兴,这段时间我们在一起.\n20 我总是被父亲爱母亲的程度震惊.他一直在谈她,对她充满感激,无比珍惜这段感情. 母亲很爱我,就像胶水一样,把我们这个家紧紧粘在一起.从小她就事事管着我,我有点嫌她啰嗦.有一次,她甚至打电话让我不要出门,因为外面风大危险. 现在她不在了,我才意识到,我一生都在抵制她对我施加影响.如今它不在了,我却想念了.她应该很高兴听到我这样说.\n21 父亲很注意健康.早在粗粮流行之前,他就开始吃了.每天早上,他都非常投入地做健身操,弯腰用手指触碰脚趾,还做仰卧起坐. 他还喜欢往橙汁里加生鸡蛋.他总是问我要不要一起喝,我说这种饮料很恶心,他就得意地哈哈大笑.\n22 最让父亲开心的,就是我取得成功. 每当他心情不佳的时候,我就立刻讲一个我职业上的最新突破.我告诉他,我正在为某本著名杂志或者某个大型项目,拍摄重要的作品.有时候,这是真话,有时候,这是假话.但是这不重要,重要的是我要尽可能让他开心. 他一听到我的这些话,脸上就会露出幸福的表情.他总说:\u0026ldquo;我必须让所有朋友都知道,我有一个著名的儿子.\u0026rdquo;\n23 有一天,我把父亲以前的电影借回家.那是一部30年代拍摄的《陈查理探案记》,我和他一起看. 他告诉我,那时太年轻,电影里的小胡子都是胶水粘上去的.看着父亲年轻时的形象,我感觉很奇妙.那时他完全不知道,未来会有母亲和我,我们会生活在一起,生活充满了各种可能.\n24 父亲又问母亲去哪里了,我还是说在巴黎. 他又问她现在干什么(除了照顾生病的舅舅),我就说她正在管理一个杂技团,然后做出把头放在狮子嘴里、荡秋千、跳火圈的动作.这让我们两个都笑起来了.\n25 有一次,我们说着说着,他就停下来了,什么话也不说,发出叹息,然后闭上眼睛. 就是从那时起,我明白他其实都知道,关于母亲,关于所有的事情.\n26 昨天,父亲去世了. 我整晚都和他在一起,握着他的手,听着他的呼吸声,想知道哪一口气是他最后的呼吸.他死在家中自己的床上,我和卡拉在他的身旁. 过去三年来,我一直害怕,他会在我不在家的时候去世.我不想让他孤单经历这一切,不想让他周围有陌生人.我知道,这样说可能不合适,但是我非常满意这件事发生的方式. 很幸运,过去三年我与他在一起,我们没有话没说,知道彼此深爱对方,知道他深深为我骄傲,而我也发现他竟是如此有趣.这是我的一份非常非常宝贵的礼物.\n27 感谢每一位读完这份日记的人,我从没想过其他人会对这件事有兴趣.我深感荣幸我有这么多读者,我读过你们的每一条留言、每一封电子邮件. 父亲如果知道我这样做,他会很开心.他希望别人知道他的故事.他的生命从无静止,他就像一条河流,一直在奔腾向前. 上周是他99岁生日,我问他知道自己几岁吗?他笑着说:\u0026ldquo;22岁?\u0026rdquo; 现在,他去巴黎了,与母亲团聚了.\n(完)\n","permalink":"https://xguox.me/repost-days-with-my-father.html/","tags":[],"title":"[Repost]与父亲在一起的日子 (组图)"},{"categories":["Translation"],"contents":"概括起来,今年到目前为止,我所做的事情包括:\n  花了10个月的时间做世界环游,途经非洲,东南亚,澳洲,中南美洲里的17个国家和地区.这次旅行的主题就是冲浪和摄影. 出席在香港,日本,美国和伦敦举行的会议 启程时给O\u0026rsquo;Reilly出版公司写了一本书,书名叫做《JavaScript Web Applications》 另外写了一本关于CoffeeScript的书,很快就会由O\u0026rsquo;Reilly公司出版. 写了大量的开源库,例如Spine, Spine.Mobile, GFX, 和 Juggernaut. 筹划了一个创业公司的框架 出席伦敦2011FOWA会议 最后,我在Twitter公司找到了一份工作   那么,让我从一年前开始,那是2010年9月,我刚好从一个我合作创办的公司里出来,尽管这段经历是很有价值的,但无休无止的长时间苦干让我精疲力尽.我回到了英格兰,需要对未来做一些思考.我一直有一个梦想——移居美国(几年就好),所以,我在Google记事本上写了下面的话:\n 人生的选择: 去纽约哥伦比亚大学深造 坏处 - 非常昂贵,并不一定能学到什么真正有用的东西,无聊? 好处 - 那是一个纽约的大学! 写一本书,申请 01 签证 坏处 - 需要大量的时间,有风险 好处 - 对事业有好处,有趣 等待.去纽约度一次假(3个月).等待创业签证. 很容易 - 不是那么有趣 也许选第二个,不行就选3?\n 最终我选择了2,我已经对JavaScript web应用研究了很久,我要写一本这方面的书,为什么不边做环游世界的旅行、边写书呢?这也是我一个梦想呀.我从oneworld买了一份环游世界的机票(比你们想象的要便宜),决定下周去我的第一站,南非.\n如果你从来没有到过非洲,你应该去一次.那里的景色原始而美丽,对那些没有体验过这种景色的人,你很难用言语描绘明白.几年前我就喜欢上了南方,那时我在东海岸做了一个为期3个月的冲浪旅行.这次,我只有一个月的时间,穿越特兰斯凯,从开普敦到德班.当我在南非旅行时,我的写作也开始了,把早期向O\u0026rsquo;Reilly提交的书的框架里的数章填充了材料.\n特兰斯凯是南非非常具有乡野特色的地方,到处是连绵的小山,一些小村庄和土堆的茅屋.他们仍然沿袭着酋长制度,有一个首领,大多数的当地人靠捕鱼为生.我们在高低不平的土路上颠了两天才到达我心仪的地方,一个美丽的海湾,叫做咖啡湾(Coffee Bay).在那里,我休整了一下,从网上下载了一些相关资料,为更远的海湾远征做准备.\n我还清晰的记得我们走了数里地来到那个未开垦的海滩,我们从那些一个个被黄沙和小丘孤立的村庄穿行而过.有一个地方,我们要过一条大河,我们需要游过去,我把背包举过头顶,以免里面的相机和iPod遇到水.非洲是一个让你脱离尘世的地方,解放你的思想,重新认识人生最重要的东西是什么.\n下一站是香港,在那里,我度过了我的21岁生日,接着,我从陆路由新加坡到越南河内.很多人不相信香港70%的面积由自然公园覆盖,我徒步走了几条精彩的景观路线,非常的精彩壮观,比如:香港龙脊.有几天,我在boot.hk这个网站上闲逛,这是一个协作工作的网站,我顺便教了一个同行的游客如何使用Ruby.然后,到了夜里,我跟Soho里的一些冲浪爱好者狂欢到凌晨.\n从泰国到柬埔寨到越南是我这次旅行中做喜欢的部分,如果你从没有到过亚洲,你绝对应该去一次.这些国家非常的漂亮,气候非常的好,食物美味可口,人们非常友善.吴哥窟是世上最神奇的地方之一,每个人都应该去看看.是Trey Ratcliff的照片把我吸引到了那里,我的很多其它旅游目的地也是受了他的影响.那个家伙是很多旅游地的第一宣传者.\n在一些无名的小博客中,我听有人说过一个很远的美丽的小岛,在柬埔寨的海边.说小岛的Sihanoukville这个地方有个酒吧,说只能坐小渔船到那里.我,还有几个非常好的朋友,乘坐晚上的大巴,开始寻找这个传说中的酒吧.搜索差不多进行了一整天,每一个问过的酒吧都把我们指向另外一个酒吧.最终,我们问了出来,并在第二天早晨做短程巴士去了那个地方.\n上面的照片上是海岸边一个10美元一晚的小木屋.从当地居民区离开后,我们的队伍像小岛上唯一的人,我们随性自由的奔跑.白天我们懒懒的躺在海滩上,吃着岛上厨师准备的鲜美可口的水果沙拉,在夜晚,我们在到处是浮游生物的海里游泳.\n下一站是越南,我们沿着湄公河支流来到一个边界上的小镇,我们是这里唯一的西方人,交流成了最大的问题.幸运的是,我们发现一个也许是镇上唯一会说英语的人,他骑车当我们的向导.当我的信用卡被那里的一个自动取款机吞掉了后,他提供了我很大的帮助!\n我们的队伍分成了几路,在我到达越南时,我的书正在按计划完成,进行的非常顺利.此时,我在西贡多待了几周,让我在书的好几章上有了重大的进展,正好是中国旧历新年,气氛非常的壮观热闹.\n接着是日本,澳大利亚,新西兰和夏威夷.我很难把我所有的感受都在这篇文章里写出来,但说这是此生难忘的一段历程是不为过的.把如此多的美景都放到一个国家里,太让人赞叹了,我说的正是新西兰.我最喜爱的一段记忆是沿着Wanaka的一个湖边在阳光下跑步,还有就是背着食物和生活用品,徒步数天穿越Routeburn的大山.在这个国家的旅途中,我结识了好几个值得一生相伴的好友.这是一个真正的天堂.\n就在我环绕新西兰的南部岛屿时,我的书终于完成了,提交给了技术编辑校对.\n接下来是纽约和旧金山,这两个神奇的地方到处是天才的程序员,有些人我很幸运的认识.Techcrunch Disrupt办的很精彩(我高度推荐hackathon).\n在从纽约到旧金山的中途停留期间,我在各种公司了进行了不少的求职面试,最终在Twitter公司找到了一份做前端开发的工作.要在那里和杰出的团队一起工作,我不能不高兴的颤抖,而去旧金山,同样也是我此生的一个梦想.\n当签证的事办下来了后,我去了中、南美洲旅行,同时开发了我的一个小工程:一个JavaScript MVC框架库,叫做Spine.我到了哥斯达黎加,巴拿马,秘鲁,Bolvia,和阿根廷. 秘鲁是我的最爱,尽管那里的海拔给我带来了不少麻烦,我大部分的时间都在探险.下面的图片是哥斯达黎加传说中神奇猎鹰,是在我爬下世界最深的峡谷时拍到的.\n当我在哥斯达黎加时,微博上有个叫Roberto的家伙给我发了条信息,说他读了我的书,问我是否有兴趣一起冲浪.我欣然同意,坐上去圣何塞的汽车,在几天后和他会了面.那天我们一起在他海边的公寓里开发Spine和Ruby项目,使用移动硬盘,用汽车电源给笔记本充电.当电量不足后,让太阳能板补充能量,我们去冲浪.\n我推荐大家去写一本书,特别是边旅游边写书.可以想象,如果我不去旧金山去看一看,我可能还在旅途中,做顾问,去创业.当作家并不能让你直接的挣到很多钱,但它绝对能提升你的身份地位,给你带来很多潜在的机会.事实上,写作过程让我真正享受的是,我可以认真深入的研究一个题目.\n这一年是我这辈子目前为止最好的一年,而我感觉今后的一年会更好.当我如今定居下来后,我并没有感觉旅行对我的吸引力减少了；我始终把签证放到一个口袋里,而另一个口袋里装着钱包,当召唤降临,随时准备离开.\n可是,这篇文章并不是关于我的旅行,它是要发送一个信号:\n对于程序员来说,有个得天独厚的条件,就是这种职业可以远程工作或边旅游边工作,这是其它职业办不到的.当然,也不都是这样,在我的旅途中,我没有碰到第二个跟我的做法相似的程序员.这种情况让人悲哀.我想向程序员们送出的信息是,不要再找借口了,行动起来,你可以做到.一个人只有一生,我可以向你保证,这样的生活才不枉世间走这一遭.\n就像我,我感到极度的幸运,能这样的生活,去发现我的热情所在,去做每天我喜欢做的事情.你可以看出,大部分我现在的境遇并非偶然或侥幸,这是计划,追求,工作的结果.\n一份汗水,一份收成.\n这篇文章的目标不是做一些自我陶醉似的炫耀和大话,而是向大家演示如何立下目标,鼓励大家去做相似的事情.想清楚你现在的处境,这一年内你想得到什么,制定出一系列具体的能让你到达这些目标的步骤.追随你的梦想.\n原文来自Alex MacCaw Traveling Writing Programming 本译文出自外刊IT评论 旅行,写作,编程\n","permalink":"https://xguox.me/traveling-writing-programming.html/","tags":["Translation"],"title":"[Repost]旅行,写作,编程"},{"categories":["Translation"],"contents":" 在我的上一篇文章traveling, writing and programming中, 涵盖了我前一年的旅行, 写作, 编程. 而在这一篇文章中, 将会具体地对整一个旅程进行规划, 包括航班, 费用, 活动, 住宿等. 我会向你展示一个廉价实惠的计划.\n Travel is fatal to prejudice, bigotry, and narrow-mindedness. (旅行是克服偏见、自大、狭隘的最好方法.) \u0026ndash; Mark Twain\n 远途旅行, 开阔视野, 我觉得这对于常年呆在硅谷的人来说尤为重要. 旅行可以让你见识到人们真真切切遇到的一些问题, 并使你有这样的机会去想办法解决掉, 而不是满脑子都是一些在家的琐碎想法. 一个新的视角, 一个新的想法,也或许就在你的旅途之中诞生.\n选择去哪里 在最开始, 选取目的地会是一件让人头疼的事, 尤其是当你又对其他的国家都一无所知的时候更是如此. 前年, 我在南非呆过三个月, 于是,我决定从那些熟悉的地方开始进入到我的旅行节奏.\n在作出选择之前, 我参观了Trey Ratcliff 的 HDR 摄影博客. 事实上, 下面的所有照片都是来自于Trey. 这家伙已经把全世界都游遍了, 甚至还到过许多不可思议的地方. 我只是简单地浏览了他所到过的地方, 然后把我觉得最美丽的列了个清单. 其中列表如下:\n 南非, 香港, 新加坡, 马来西亚, 泰国, 柬埔寨, 越南, 日本, 澳大利亚, 新西兰, 夏威夷, 纽约, 旧金山, 哥斯达黎加, 巴拿马, 秘鲁, 玻利维亚, 阿根廷\n 我的目标是尽可能多的在一年内游览完, 然后在未来某个时候把一些地方再细致地重游一遍. 如果你没有这么多的时间, 我的建议是, 专注于某一个区域, 比如亚洲. 关于下一次旅行, 我的计划是:\n 从北京启程, 坐火车到西藏, 往下到尼泊尔, 环游印度, 直到孟买, 然后飞往东南亚, 穿越泰国北部,柬埔寨,越南和老挝.\n 所以, 换句话说, 你想去哪取决于你的时间, 预算和意愿.\n安排航班 相较之过去的种种麻烦, 现在的环游世界机票已经让旅行变成一件轻松的事了. 曾经, 我的祖父与女王的信使搭乘一架破旧的飞机一同前往不丹. 在那次飞行中途, 飞机的一个引擎坏了, 结果只能紧急迫降. 更糟糕的是, 在进行了必要的维修后, 他们又得继续搭乘这架飞机继续行程. 如今时代变迁了.\n你可以逐个逐个航班的预订, 这会给你更大的灵活性, 也可以直接预订环球机票一次性把所有事情搞定. 出于经济上的考虑, 我选择了后者, 因为很多我需要的航班如果单独预订的话要贵的多.\n环球机票的价格范围一般在$3K 到 $7,5K USD 之间. 我的16个航班大概花了$7K. 但实际上, 还是取决于你有的时间, 你飞往的地区以及从哪开始旅程. 这里有个秘诀, 就是在一个不太富裕的国家开始和结束你的旅程, 因为那样的机票通常都会便宜的多.\n我是在OneWorld(寰宇一家)预订的机票, 他们的服务让我非常高兴. 他们的集团包括有美国航空, 英国航空, 国泰航空 和 澳洲航空等. 并且还提供了一个很方便的在线预订工具来计划你的旅程. 无须给旅游代理哪怕一点的佣金, 你可以自己规划你的旅程. OneWorld的一个很好的地方在于它允许你免费更改航班的日期. 但航班的地点更改不是免费的, 所以你必须在做决定前确定你的选择是正确的.\n环球机票还有一些限制, 这跟点到点的航线有所不同. 有一些会限制你的飞行里程, 有些则是限制了实际的航班数量. 大部分都会限制你在一个地方只能飞一次, 并且航行的方向是一般固定不变(比如一路往东). 在每一个地方你必须呆至少两周的时间, 大部分的票也都会在一年后到期. 之所以有这些限制的原因是他们不希望有人利用他们来通勤 , 所以在买的时候请记住这些.\n我建议你只买洲际的航班, 而短途的航班则只在需要的时候才买. 在亚洲和南美这些地方之间游玩不需要专门的乘坐飞机, 完全可以搭乘当地的公交, 旅游车, 又或者买一辆车(?), 或者徒步行走. 强烈建议你尽量的减少空中的飞行, 更多的在陆上游玩, 这样你看到的, 经历的会更多.\n###行装\n只带上一些你确实需要的, 尽可能轻装上阵, 只用你的背就能搞定那就最好了. 从安全的角度着想你也应该这么做. 因为你携带的越多, 你就必须多加一双眼看着.\n我刚刚从ebay买了一个90L的背包. 然后在里边塞了一个星期的衣服, 一条毛巾, 一套相机设备. 而实际上我在马来西亚就已经把我的三脚架和潜水服寄了回来. 对我来说他们太过笨重了. 当你发现一些需要的东西忘了带可以直接在当地买. 事实上,我一直坚持这种简约的生活方式, 同时这也是我喜欢旅行的其中一点.\n预算\u0026amp;住宿 住宿是件简单的事, 尤其当你在一些亚洲很便宜的地方游玩的话. 因为我自己本身有这个预算, 所以一般都住在酒店 - 不包括一些在亚洲或者南非的古怪的酒店. 好的酒店比较难找到, 这需要你专门的在网上做一些调查. Hotel World, Lonely Planet, Wikitravel都是不错的选择. 有些地方的设施对旅客非常的友好便利, 例如新西兰, 而有些则相反, 比如夏威夷和日本.\n酒店也是一个非常好的地方可以结识到朋友, 还可以从他们那得到一些游玩的建议. 另外你也会遇上一些有趣的人. 我曾经遇到过的有半导体芯片设计师,专业潜水员和量子加密专家\u0026hellip;我结识了一些驴友并和他们建立友谊, 这都是旅行所带给我的乐趣.\n整一年我的总预算大概是$15K, 包括我的所有食宿. 这个数目对我来说刚刚好, 我不必住在特别的廉价的地方. 当然, 在你的银行卡上有一定的缓冲预算是好的, 因为这样你可以有个更安心的行程.\n#现金与电子产品\n我去的每一个城市都有ATM, 所以取得现金不是什么大的问题. 但我的卡曾经三次因为被怀疑欺诈行为而被冻结了. 我建议你告诉你的银行你的行程, 以免有什么不必要的麻烦. 出于安全考虑, 把你的一些信用卡分别放在不同的地方. 最好使用一些不会收取国际交易费的卡, 否则这可能会是一比大的支出. 很多地方不支持刷卡, 所以现金在手是必须的.\n我已经有将近一年没有使用手机了, 取而代之的是我的iPod和Skype. 这其实并没有你听起来那么疯狂, 跟全世界断开联系一段时间事实上感觉挺好的. 由你来选择人们能否联系上你. 如果你确实需要一台手机, 做好国际长途的计划, 并且随时盯紧 - 否则的话被盗那是瞬间的事.\n我去哪都随身带着我的Macbook和Nikon单反, 另外还有其他的一堆电子产品. 坦白地说, 这个世界并不是像人们想象的美国西部那般纷乱. 你只需要用平常心去对待就可以了. 或许我是幸运地, 因为在我的旅途中没有一样东西被偷.\n插头(插座)也是一些必备的东西之一. 在亚洲使用的插座都是相似的, 可能你经常会遇到插不上的情况. 同样的, 在拉丁美洲, 也是没有接地线的插座. 美式三角插是用不上的, 所以你需要一个国际插头适配器.\nWIFI几乎无处不有(在我的印象中越南的网速比美国还要好的多). 网络问题只在一些非洲偏远的地方才会遇上. 如果你想要去一些特别偏远的地方, 那么可能还需要带上一个3G的无线网卡. 在当地购买通常是最好的选择.\n活动和规划 不管做什么, 永远不要选择套票. 我一般初头的两晚呆在飞机抵达的城市, 接着从那启程. 有时候, 我甚至不会逗留. 你需要给你的计划一些容错空间, 不要试图计划好没一个细节. 通常地, 你的计划会在你抵达后从其他旅行者那得到一些建议因此而改变. 随机应变那是必须的.\n我个人从不去使用一些指南书籍, 我的基本指南来自于Lonely Planet. 想便宜一些买到的话则在你登陆的国家买这些指南, 而不是在机场或者你自己的国家买.\nWikiTravel是我每天都用到的很棒的资源. 但是, 最好的资源还是来自当地人和驴友们的建议. 我曾经在一本黑色的小型笔记本上记下了所有他们给我的好的建议. 我记得我遇到的一个阿根廷人在我的书上画了一张南美洲地图, 并列出了在秘鲁和智利他所推荐去的地方.\n同伴 同行的伙伴是会相互牵扯住的. 所以我建议自己一个人旅行, 强迫自己在路途上遇到更多有趣的人和结识更多朋友. 如果你和其他人一同旅行, 那么请确定你们是非常好的朋友, 因为长时间的旅途会让人变得烦躁.\n签证 如果你很幸运的是一名美国或者欧洲公民, 那签证将不会有什么大问题. 唯一一个在过境时没有给我签证的国家是越南. 我只好在柬埔寨一间酒店里给了点钱一个家伙让他帮我把签证送到越南大使馆并处理之. 这事情值得提前做一些研究,不过通常来说是没问题的.\n安全保险 For health insurance I just used World Nomads. They\u0026rsquo;ve fairly competitive rates, and are one of the more dependable options.\nThat said, it\u0026rsquo;s often a case of staying lucky and not doing anything stupid. All the insurance in the world won\u0026rsquo;t help you if you have an emergency in a remote part of Africa.\n生活方式 在离开之前我一般会在一个地方逗留上几天, 有时候最多会是一周. 当然, 基于那么频繁的转移, 偶尔的搁浅还是很舒服的. 每当产生那样感觉,我会停留在一个地方几个星期.比如, 为了使得我的书取得一些进展, 我在胡志明市停留了两周.\n锻炼方面, 如果不去冲浪的话我会选择在海滩上跑至少半小时. 有更多的时间可以全身心地投入到你的健康, 是旅游的主要好处之一. 每天的冲浪和长跑, 持续几个月后换来的是我从未有过的强壮的身体.\n同样的智力上的锻炼也是需要的, 我选择了写书来达到这个目的. 除此之外, 我还会做些阅读或者编程. 保持智力上的平衡是很重要的, 尤其在这么长时间的旅途中.\n读完书后, 我会开始hack一些开源项目, 并开发了spine. 事实上, spine的第一个版本是我在新西兰做长途车的时候写的. 插上耳机, 祛除杂念的专注于此, 很不可思议吧.\n每当我旅行到一个新的城市, 就会提前在Ruby邮件列表发帖看看是否有人有兴趣来个聚会什么的. 很多时候都是成功的, 我会和这些人一起聚会然后向他们请教一些问题. 在东京和香港我还做了技术演讲. 在悉尼和开普敦则和当地的Ruby团队聚餐了.\n经验 周游世界可以让你见识,经历到的比你想象中要多的多. 你可以获得国王般的口福, 从你不认识名字的马来西亚的美味水果, 到在阿根廷你所尝试过的最美味的牛排. 你可以在新西兰爬山, 以及秘鲁大峡谷徒步旅行. 还可以在哥斯达黎加难以忘怀的冲浪, 以及就像迎接世界末日那样的开party. 你会遇上一些令人惊奇的人, 改变生活的经历.\n这一切都很简单,周游世界,将会是你做的最好的决定之一.\n总的开销 整个行程的净成本大概是$22K, 我在为时一个月的事前咨询之前已经支付了其中的一大部分. 或许你会觉得这很夸张, 为了一年的旅行咨询一个月的时间. 我并没有刻意削减我的预算, 我知道有很多人可以花更少的钱完成这一切. 总而言之, 有志者, 事竟成.\n边旅行边工作完全也可以成为一种新的生活方式. 如果你是一个程序员, 完全可以远程办公, 至少可以是一年之中的一部分时间远程. 你的客户不会关心你在哪里, 他们只关心你是否把事情做完. 要达到这样的位置所要付出的跟你成为一个成功的contractor是一样的. 那就是实践 和 网络.\n坦白地说, 唯一要顾虑的事情只有一个, 时间不多了. 你身上的包袱, 顾虑会随着你的年龄的增加而增加, 在往上, 你想干这些事就更难了. 你应该丰富的是你的履历, 而不仅仅是金钱.\n动力无非由两部分组成, 想的程度和可行性. 这些个东西我都完成了. 所以, 也希望看到更多的程序员远途的旅行.\n原文来自Alex MacCaw How to travel around the world for a year.\n","permalink":"https://xguox.me/how-to-travel-around-the-world-for-a-year.html/","tags":["Translation"],"title":"如何在一年里周游世界"},{"categories":["Tools"],"contents":" Fxxk Jabber 回想起初初接触编程语言的时候, 特喜欢折腾IDE的玩意, 什么visual studio啊, Eclipse啊, Netbeans, Aptana的会不会用好不好用都不管了, 装进PC再说, 怎么说, 很傻13的觉得我的PC里有这些工具我就牛13了, 熟不知真正的编程知识却木有学到, 后来搞Ruby开始放弃笨重的IDE改玩Vim和Sublime Text, 当时Textmate被捧上天不过木有Mac耍不起. 换了Mac了, Textmate2也开源了, 于是也装上耍了耍. 没多久又继续用Sublime Text了. OOXX来来去去, 这两货其实真没差多少. 好吧, 是我用的都不够透彻. (T . T)\nTheme 有时候蛋疼起来可以折腾某个主题某个配色好长的时间, 即使这对我的编程技能木有任何实质性提高. 强推一款theme - Flatland, 随着Flat UI的兴起, ST的主题肿么能落后啊. 附带的icon更是比Sublime原本的霸气的不是一点两点啊. 貌似在哪见过, 想不起来了.\n这UI还行吧? 我自己也懒得再去调了, 额外的把cursor换了个色就是了(原本的灰色真心不够骚啊, 老是眼瞎找不着)\n说到theme就想起这个正在用着的Knockdown Package -Markdown文件专属的主题, 再配上Sublime Text的Enter Distraction Free Mode 用写静态Blog很带感叻, 对不对. 有时候, 巨爱这个模式. 舍不得Sublime的原因之一\n之前也试用过MarkdownEditing, MarkdownEditing的高亮做的没Knockdown那么好(说实话, Knockdown的代码块的高亮配色不怎么符合我的style, 太过偏暗偏沉, 但总比木有高亮来得强), 挺喜欢ME的默认放在center这一点, 但是, 在Enter Distraction Free Mode下, 这个优点也就荡然无存了.\nPackages 每个用Sublime新手老手都会介绍强推的Package Control 插件管理就不再累赘啦. Actually, 想装啥插件可以直接在里头搜, 或者网页版搜索, 一般都会给出相应的github开源链接. 装什么插件就更是因人而异的. 比如如果从来不写Markdown的那就木有必要装我上面说的那货. 又比如, 如果是重度Hacker News 用户的话, 直接搜hacker news(其实在\u0026rdquo;Install Package\u0026rdquo;上输入hack就已经跑出来一堆相关让你选择), 当然也不是啥都有的, 如果搜的是reddit就没看到结果了.\n个人用的目前装着的packages有:\n All Autocomplete 这货是在已打开的文件的基础上拓展匹配的 knockdown Sass Sass的高亮, 补全\u0026hellip; CoffeeScript CoffeeScript高亮,补全, Compile等\u0026hellip; JSHint 检查Javascript的语法规范, 要先在装了node, 然后npm install -g jshint Emmet 前身Zen Coding, 貌似所有的packages之中排在最前的. 几乎被吹捧成为神器中的神器了. HTMLpretty 个人觉得解决缩进等格式问题的神器, 好吧, 即使木有这个偶本身也是具有良好编码规范的. Git Web Inspector 纯属贪新鲜装上耍耍, 取代浏览器的inspector不太可能. ERB Insert and Toggle Commands 快速输入\u0026lt;%= %\u0026gt;或者\u0026lt;% %\u0026gt; Hacker News 不常用, 而且要自己改改shortcut, 否则会占用了显隐侧边栏的快捷键. PlainTasks TODO插件一枚. 配合Sublime的各种快捷键各种操控很nice  Settings-User 先把custom的show一下, 也没啥的, 都是些无关痛痒的修改. 个人爱好吧. 有个必须有的就是\u0026quot;scroll_past_end\u0026quot;: true, 经常写了一大车页面布满了, 下面的就滚不起来, 眼睛老要往下瞄很不爽. 设置以后就可以自由控制底部显示. 基本上自己看着Settings - Default 还是老规矩按需修改啦.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  { \u0026#34;theme\u0026#34;: \u0026#34;Flatland.sublime-theme\u0026#34;, \u0026#34;color_scheme\u0026#34;: \u0026#34;Packages/Theme - Flatland/Flatland.tmtheme\u0026#34;, \u0026#34;tab_size\u0026#34;: 2, \u0026#34;font_options\u0026#34;: [ \u0026#34;bold\u0026#34; ], \u0026#34;font_face\u0026#34;: \u0026#34;Courier New\u0026#34;, \u0026#34;font_size\u0026#34;: 20.0, \u0026#34;bold_folder_labels\u0026#34;: true, \u0026#34;hot_exit\u0026#34;: false, \u0026#34;open_files_in_new_window\u0026#34;: false, \u0026#34;create_window_at_startup\u0026#34;: false, \u0026#34;scroll_past_end\u0026#34;: true, \u0026#34;wide_caret\u0026#34;: true }    Key Bindings - User 快捷键同样跟Settings可以多看Default绝对比看路边3322的像我这些要靠谱, 完整的多. 忘了在哪学到的{ \u0026quot;keys\u0026quot;: [\u0026quot;ctrl+command+r\u0026quot;], \u0026quot;command\u0026quot;: \u0026quot;reveal_in_side_bar\u0026quot; } 挺实用的, 直接在侧边栏显示当前的文件所在的目录.\n1 2 3 4 5 6 7  [ { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl+command+r\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;reveal_in_side_bar\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;command+shift+.\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;erb\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;super+v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste_and_indent\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;super+shift+v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste\u0026#34; } ]    Shortcut 列一些个老是忘记用快捷键的操作, (有时候又无故发现一些快捷键)\n1 2 3 4 5 6 7 8 9 10  focus到侧边栏: \u0026#39;ctrl + 0\u0026#39; Command + D:选择词.重复按下可以增加选择下一相同的词.按下 Ctrl + Command + G 可选中所有相同的词. Command + Option + /:块注释. Control + M:跳转到对应的括号. Control + Shift + M:选中当前括号内的内容,重复按下可增加选择括号本身. Command + Shift + J:选中当前缩进级别下的所有代码. Command + Option + .:闭合 HTML 标签.(说常用也不常用, 但又貌似有用) 按住option 拖拽多列 按下 Control + Shift + 方向键,可以选中矩形区域的文本. 选择数行文本末端,选中区域然后按下 Shift + Command + L.   PS: 官方的symlink貌似有点不妥,\n1  ln -s \u0026#34;/Applications/Sublime Text 2.app/Contents/SharedSupport/bin/subl\u0026#34; ~/bin/subl   稍作修改为:\n1  ln -s \u0026#34;/Applications/Sublime Text 2.app/Contents/SharedSupport/bin/subl\u0026#34; /usr/local/bin/subl  ","permalink":"https://xguox.me/jabber-sublime.html/","tags":["Tools"],"title":"扯点Sublime"},{"categories":["Translation","JavaScript"],"contents":" LEVEL: INTERMEDIATE Publications \u0026amp; subscriptions 是在Meteor里最基本也是最重要的概念之一, 但同时也是最难理解的. 很大程度上是由于这其中跟传统的构建web的方式有很大的不同. 过去, 我们习惯思考,定义API用在客户端和服务器之间进行数据传递, 而在Meteor中, 客户端和服务端的数据是同步的. 我们使用publications来精确地控制如何同步. 最初人们觉得这个概念有一些难以理解的部分原因是由于Meteor像\u0026rdquo;魔术\u0026rdquo;那样为我们做了这一切. 这些\u0026rdquo;魔术\u0026rdquo;都非常有用, 而具体发生了什么则都被掩盖封装起来了(就如魔术为何如此奇幻一样). 那么, 现在就让我们拨开这些魔术表层的面纱一探究竟. 在此, 我们将会学习到一到两个窍门.\nDefining Publications 从本质上讲, publication(使用subscription与之相连)是从服务端(源)collection到客户端(目标)collection的传递数据方法. 同时, 把subscription想象成一个漏斗连接着标准的数据存储(与mongodb数据库交互的源集合)与客户端缓存(目标集合, 相应数据的备份或者子集).\nsubscription精确地控制着哪些数据该通过这个漏斗, 同时负责同步两端的数据. 通过添加多个subscription到服务端数据存储, 我们就可以实时地, 有效地, 安全地保持各个客户端的数据同步.\n这里头所用的隧道协议叫作 DDP(Distributed Data Protocol). 想要了解更多关于DDP可以观看Matt DeBergalis(Meteor的founder之一)在(The Realtime Conference上的演讲. 又或者是 Chris Mather的这个视频更详细的为你介绍DDP的概念.\n现在, 我们了解了基础部分, 让我们往更深层探个究竟吧.\nAutopublish 当你创建了一个最基本的Meteor应用之后(比如使用 meteor create), 它会自动的启用 autopublish这个package. 首先的, 我们先来看看它究竟为干了些什么.\nautopublish是移除了对subscriptions的需要还是只为你保管subscriptions 这取决于你如何看待它. autopublish所做的是自动地把服务端的所有数据镜像到客户端. 这是怎么做到的呢? 假设你在服务端有个posts的集合. 那么 autopublish会自动把在Mongo 的posts 集合所找到的每一个post发送到客户端给一个也同样叫posts 的集合.\n所以, 如果你使用autopublish, 你就不用再去管subscriptions了. 数据在哪都可以访问, 所有事情变得各种简单. 当然, 很明显这是有问题的, 你不可能在每一个用户的机器上都缓存有你的整个app的数据库备份.\n出于这个原因, autopublish只适合用在你的app刚起步还没考虑到subscriptions的时候.\nPublishing Full Collections 当你移除掉autopublish以后, 很快你就会发现你的数据都会在客户端消失不见. 有一种方法可以很简单地取回这些数据, 那就是简单的复刻autopublish, 并把一个collection全部publish. 例如:\n1 2 3 4  Meteor.publish(\u0026#39;allPosts\u0026#39;,function(){ return Posts.find(); });    同样地, 我们也还是把整个collection都publish了, 所不同的是, 现在对哪个collection进行publish是可控的. 在上边这个例子, 我们publish了posts collection, 而comments collection没有被publish.\nPublishing Partial Collections 更高一级的粒度控制是只publish某个collection的一部分. 比如, 只作用于属于某个author的posts:\n1 2 3 4  Meteor.publish(\u0026#39;somePosts\u0026#39;,function(){ return Posts.find({\u0026#39;author\u0026#39;:\u0026#39;Tom\u0026#39;}); });    代码都很简单, 但是究竟在底层发生了些什么呢?\n如果你有读过Meteor的文档, 可能会对added()和ready()在客户端设置记录的属性感到惊讶, and struggled to square that with the Meteor apps that you\u0026rsquo;ve seen that never use those methods.(I am so sorry 这句没想着怎么翻译好)\n这其中的原因是, Meteor提供了一个非常重要的便利 - _publishCursor()方法. 可能你从没见过它的使用? 因为有可能不是直接的调用,但如果你在调用一个publish函数并返回一个游标(cursor)(例如: Posts.find({'author':'Tom'})), 那么这就是_publishCursor.\n当Meteor看到somePosts这个publication返回来一个游标(cursor), 则表明它自动调用了_publishCursor()publish这个cursor. 下面这是_publishCursor()所做的事情:\n 在服务端查找这个名字的collection 从cursor中取得所有匹配的documents并发送到客户端的同名集合(在这用到的是added()) 只要某个document被增删改, 都会被同步到客户端(在cursor上使用.observe(), 并使用.added() .updated() .removed()来完成)  那么,在上边的例子, 我们就可以很简便地确保只有用户感兴趣的posts(written by Tom)会出现在他们的客户端缓存之中.\nPublishing Partial Properties 在上边我们已经看到了如何publish我们的一部分posts, 但是我们还可以做的更精确一些. 下面看看如何publish特定的记录吧. 跟前边的一样, 我们先用find()得到一个cursor, 不过这一次我们会排除掉一些\n1 2 3 4 5 6  Meteor.publish(\u0026#39;allPosts\u0026#39;,function(){ return Posts.find({}, {fields: { author: false }}); });    当然的, 我们还能够结合这两种技巧. 比如我们想要返回所有来自Tom的posts同时, 可以这么写\n1 2 3 4 5 6  Meteor.publish(\u0026#39;allPosts\u0026#39;,function(){ return Posts.find({\u0026#39;author\u0026#39;:\u0026#39;Tom\u0026#39;}, {fields: { author: false }}); });    总结 现在, 我们已经知道了如何publish从所有collections的所有documents的所有属性(通过autopublish)到特定collections的特定documents的特定记录.\n这包括了你能用Meteor的subscription所能做的所有基本东西, 而这些简单的技巧应该\n有时候, 你可能会需要更深层的组合, 联接, 合并 publication, 而这些我们将会再找个时间详谈.\n原文来自Tom Coleman Understanding Publications and Subscriptions\n","permalink":"https://xguox.me/understanding-publications-and-subscriptions.html/","tags":["Translation","JavaScript"],"title":"Meteor: Understanding Publications and Subscriptions"},{"categories":[],"contents":"早之前就听说了Hexo, 也就是Octopress的node.js版本, 不过是源自台湾. setup的话其实中文英文都无妨吧. 尽管阅读起来肯定是较之中文困难, 不过也基本习惯了.\n基本上按照官方给出的guide很容易搭建好. 不过还是不准备现在就撤出Octopress奔赴Hexo, 尽管基于node.js的Hexo在generate, deploy的时候有一定的执行速度优势, 不过目前post不太多, so有感觉出差距, 但不大.\n因为不熟node, 还是遇上了麻烦, root下的db.json就不太明白. 导致在修改tags的时候死活修改不了, 查了一下github上以前的issue才知道得把db.json这货rm掉. 否则不管怎么generate, 怎么deploy都只会有增无减.\n另一个跟Octopress比较不同的是Markdown的解析器. Octopress用的是Liquid, 而Hexo用的是Marked. 大部分情况下都没什么差别. 而之所以发现这个不同是因为刚开始的generate的时候不断报错\n1 2 3 4 5 6 7 8 9 10 11 12 13  undefined:11 if ((typeof _context !== \u0026#34;undefined\u0026#34; \u0026amp;\u0026amp; typeof _context.\u0026gt; posts !== \u0026#34;undefined ^ SyntaxError: Unexpected token \u0026gt; at Object.Function (unknown source) at createRenderFunc (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/swig/lib/swig.js:44:10) at createTemplate (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/swig/lib/swig.js:96:14) at getTemplate (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/swig/lib/swig.js:124:20) at Object.exports.compile (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/swig/lib/swig.js:186:14) at file.read.async.waterfall.compiled.replace.options.gutter (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/lib/plugins/processor/index.js:128:27) at async.iterator.fn (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/async/lib/async.js:573:34) at async.waterfall.wrapIterator (/usr/local/Cellar/node/0.8.20/lib/node_modules/hexo/node_modules/async/lib/async.js:489:34) at process.startup.processNextTick.process._tickCallback (node.js:244:9)   用尽各种蛋疼的办法才发现原来是有一篇文章里头的一段underscore代码块解析出了问题\n1 2 3  {% raw  %} \\{\\{ \u0026gt; posts}} {% endraw  %}   吐血啊, 找了我老久. 原来是这个东西忘了套上 \u0026ldquo;{ % raw % }\u0026rdquo; \u0026ldquo;{ % endraw % }\u0026rdquo; 在Octopress(Liquid)里没套上的话则直接无视掉, 而Hexo(Marked)则直接嘣掉.\n现阶段的plugins自然还是跟Octopress有些差距的, 不过挺看好的. 虽然暂时不撤往Hexo. 但还是弄了个github pages, 喜欢折腾吧\u0026hellip;\n另外也顺带把404页面改成扣扣的公益页面了. 尽管知道不太可能起到什么作用\u0026hellip;\n","permalink":"https://xguox.me/hexo-blog-node-octopress.html/","tags":[],"title":"继续折腾 -- Hexo版Blog"},{"categories":["JavaScript"],"contents":"貌似有好一段时间没有在这儿小发一些闷骚或者情感泛滥什么的, 都是用自己蹩脚的英语水平翻译些英文post. 这些天想部署个Meteor app玩一玩, 结果一玩就各种出事了.\nvps退货, godaddy的莫名失单, 部署各种卡壳\u0026hellip;\n因为最近比较多的关注前端的东西, 同时在自己local耍着SachaG / Telescope这个基于Meteor的项目.\nMeteor的系统是超级alpha的, 所以很多问题遇上了也没人帮忙, 解决. 比如之前在Stack Overflow. 而部署这一块, 是我比较陌生的. 包括以前搞Rails那会, 也没真正意义的整一整部署. 绝大部分时间都在本地跑啊跑. 所以, 在Meteor, 也是这一块最让我头疼.\n从vps到appfog到heroku, 各种转了一圈, 最终还是选定了heroku. 原因之一就是资料相对多, 出错情况也较少.\n幸得以前一直搞得是Ruby/Rails. 所以装heroku的时候没啥压力, 不然可能这又得搞上一会. 直接下载了官方的heroku toolbelt无压力安装, login, clone\u0026hellip;\nand then 跟着 Heroku buildpack for Meteorite 这个buildpack完成接下来的:\n1  heroku create --stack cedar --buildpack https://github.com/oortcloud/heroku-buildpack-Meteorite.git   Please note: 在这如果你想把你的项目名改成想要的比如我改成了frontn.herokuapp.com, 在heroku的项目页面上修改名字的同时, 记得把repo的git remote 也相应的更改\n否则就会报错\n1  heroku ! Resource not found   好吧, 很明显我自己傻13了才会知道要这么做的.\n接着到了最关键的push了. 当我git push heroku master的时候, bla,blah blah\u0026hellip;下来还以为这次会真的很顺畅. 结果oops的一嘣\n1 2 3  -----\u0026gt; Launching... ! Heroku push rejected, Please verify your account to install this add-on For more information, see http://devcenter.heroku.com/categories/billing Verify now at https://heroku.com/verify   纳尼, 原来这货还要用到一个mongo的数据库插件, 在这里是Mongohq, 原本没注意看以为直接 heroku addons:add mongohq:sandbox就OK (sandbox是免费餐)\n结果毫无疑问的继续嘣\u0026hellip;\n1 2 3 4 5  Adding mongohq:sandbox on frontn... failed ! Please verify your account to install this add-on ! For more information, see http://devcenter.heroku.com/categories/billing  ! Verify now at https://heroku.com/verify    Google之才知道原来Heroku在装插件前得通过传说中的credit card卡认证, 我辣时就更嘣了. 对于我等无产阶级而言, credit card仿佛就跟神一般滴遥不可及. 你想买3刀的godaddy域名?不好意思, 请出示credit card; 你想用name.com买域名? 不好意思, 请出示credit card; 你想买linode的vps?有credit card吗? 亲\u0026hellip;正当我开始准备放弃heroku这条路时, 忽然无意中发现了虚拟信用卡这货, 顺藤摸瓜发现, 柳暗花明又一村.\n工行国际e卡 + entropay(怎么觉得我像在拍广告的)\n虽然门路是找着了, 不过弄得我好不蛋疼. 因为mac上工行网银不支持插U盾, 于是又启动了以前的那台老爷机, 一卡一顿的, 唉, 为了搞到手就忍忍吧\u0026hellip;结果成功申请后唬弄半晌才知道原来要先用软妹b买美刀才能花, 而且至少买50刀. 我现在要50刀搞毛线撒\u0026hellip;没法子, 只得照办了. 然后是entropay的虚拟卡\u0026gt;\u0026gt;\u0026gt; OOXX一轮下来总算是把heroku的验证给通过了, 感动啊\u0026hellip;\nheroku addons:add mongohq:sandbox\ngit push heroku master\n好吧, 总算是成功了. 我容易吗\u0026hellip;\n最后总结, heroku相对的来说除了出错情况较少, 可用资料多之外, 管理上也比自己搭vps要方便些, feel了一下MongoHQ的那个插件还不错\u0026hellip;这样, 也就可以花更多的时间的app的开发上. 而不是折腾这出错那无解. 好吧, 尽管咱都爱折腾, 不过, 往你想的地方折腾不更好吗? 其实这些也就都是Paas的优势啦, 专注核心开发, 减少运维成本\u0026hellip;\n其他的待续吧, 相比Rails, 或许底层它怎么干还没搞透, 但起码的知道它干了什么, 可Meteor我还真不知道它都干了些什么, 比Rails更magic.\n","permalink":"https://xguox.me/deploy-meteor-on-heroku.html/","tags":["JavaScript"],"title":"最近在折腾 -- deploy Meteor on heroku"},{"categories":["Translation"],"contents":" 想象一下,你正在一个马路口等待着过马路. You push the button to call the walk signal, and you take out your phone. 你想要完成某件事: 可能是查看邮件, 可能是给你的to-do列表添加一条item,又或者是阅读你的twitter. 你仅有有限的时间来完成这其中的一件事.\n这所要花的长短取决于用户在你的网站上完成他们想做的事情的时长.这很重要.\n据Google的一项研究, 用户的搜索结果页面每增加半秒会致使营业额和广告收入减少20%. 另外一篇来自Amazon的类似报道发现, 每增加额外的100毫秒加载时间, 销售额则会减少1%. 用户期望在两秒以内加载完整个页面, 否则第三秒后,高达40%的用户会选择离开.\n你能保持这样的速度? 如果你的网站包含有丰富的内容, 许多动态元素, 更大的JavaScript文件和复杂的图形, 那么我们当中的很多人的回答可能会是 \u0026ldquo;no\u0026rdquo;.\n现在,是时候让我们把性能优化作为如何为各种设备设计, 搭建, 测试每一个我们创建的网站的基本组成部分了.\nDesigning for performance 网站的性能优劣从网站设计开始.权衡一个设计选择对你的网页速度的影响 你的网站的转化率. 你真的需要8个深浅不同的蓝色吗?这个宽度为1000px的背景图片还需要加什么值吗? 使用icon font来取代sprite会导致页面变得更重,渲染更慢, 还是会比使用原始图片更快呢?\n并非所有的设计决策都是有利于性能的. 我发现一个略微减慢了页面速度的button样式可以提高转化(conversions), 对于仅需要牺牲小小的页面速度来说, 这是值得的.\n但有时候, 我们更需要关注的是性能. 我曾经重新设计一个登录页面, 需要在上边为其添加大量的图片, 我不确定性能上变差了是否会对转化带来负面影响, 于是我在一个 A/B test 上为一小部分的用户推出了这个新的设计来看看会有什么样的反响. 新的设计加载时间是之前的两倍, 我看到了一个很高的 exit rate 和更低的转化率, 于是我们沿用了之前的轻量级设计. 犯错了没关系 \u0026ndash; 它能给你一个参考的基准.\n在另外一次试验, dyn.com主页上着重介绍的一个缩略图部分, 需要在10个图槽内交替显示26张图片.\n于是我的teammate把这26张的图片放到一个sprite里头, 结果:\n CSS,JavaScript,以及images的体积有所变大, 主页的总大小增加了60k 请求数减少了21% 页面总加载时间减少高达35%  这也就证明了这个试验是值得的: 我们也不确定这是否能给页面带来提速, 但我们都觉得这是值得学习的试验.\nCoding for performance 清理你的HTML, 或许其他的一切也会跟着变好.\n首先, 重命名你的HTML中的非语义化元素. 这可能会是最艰难的, 但只要你开始依据像nav,article这样的语义而不是通过图样或者网格来思考主题, 就会有显著的进展. 通常情况下, 我们是通过更复杂的CSS选择器来得到非语义化名称, 这样的话我们就会添加了不必要的ID和元素到HTML当中, 这反而会造成我们的CSS不够简洁, 同时也难以正确的使用某些特性.\n其次, 就是整理你的CSS. 第一件事要做的是清除无效的选择器. 我在writegoodcode.com中的一次研究发现, 在一个CSS文件中添加一些无效的选择器实际上会致使页面的加载时间增加5.5%. 更有效的CSS选择器也因为在样式表中更易阅读且有语义从而更易于将来的重设计和自定义样式. 多用途,可编辑的代码通常可以带来高性能. 在前面提到的那个研究中, 通过整理CSS文件我节省了39%的文件大小.\n再次, 重点维护你的div. 通常你的标记越整洁, 那么你的CSS就会越小, 将来编辑,重设计也就更容易. 这不仅仅是提高了你的页面加载时间, 同时也为你节省了开发的时间.\n最后, 专注于开发多用途的代码, 这样可以节省时间且减少CSS和HTML的大小. 更少的CSS和HTML可以明显地提高将来代码的可维护性和可重构性, 更轻的页面对加载速度有着积极的影响.\nOptimizing requests 请求是指浏览器获取某样东西比如一个文件或者一个DNS记录. 你的标记越简洁, 浏览器也就更少的发出请求, 同时用户花在等待数据在客户端和服务端来回的时间就更少.\n除了简洁的标记外, 还要做的是最小化JavaScript请求 \u0026ndash; 只在确实需要的时候才加载之. 如果某一文件不是确实需要, 则不必在每一个页面都加载之. 不要在响应式设计中加载某一个只在大屏幕下才需要用到的JavaScript文件. 例如, 使用简单的链接来取代使用社交脚本. 你也可以异步加载JavaScript从而不会导致所有的内容都被JavaScript给屏蔽掉.\n尽管, 通常情况下响应式设计需要更多的CSS和图片(页面更重), 但还是可以通过减少请求来得到更快的页面加载速度.\nOptimizing images 同样的, 尽你所能的减少图片的请求. 首先, 我们可以建立sprite. 在 writegoodcode.com 的 研究中, 我发现一个案例使用icons sprite 后减少了16.6%的加载时间. 我喜欢通过创建一个背景重复的sprite来整理图片. 可能你需要创建的是水平重复或者垂直重复的.\n接下来, 创建一个no-repeat backgrounds 的透明sprite. 这个sprite可以包含你的logo和icons. 如果你希望更先进一些, 可以使用像Grunticon这样的工具, 它可以使用SVG icon和background images,并基于用户浏览器的兼容性计算得出如何最好的为之服务.\n在整理好所有新的图片后, 通过像ImageOptim这类的优化器运行. 同样的, retina-sized 的图片可以通过使用extensive compression使之变小, 不过最终结果不是特别显著.\n现在, 看一看哪些图片你可以使用CSS3的渐变来替代. 这样做不仅仅是为了减少加载时间, 而且还可以使得在将来编辑页面变得极其简单, 因为开发人员无须找到原始图片进而编辑,生成,优化等.\n最后, 来看看使用Base64 encode, 它可以让你在CSS文件中嵌入图片而不是通过调用一个单独的URL. 通常看起来的结果如下:\n1 2 3  #nav li:after { content:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAI0lEQVQIW2P4//8/w8yZM//DMIjPAGPAMIiPWxCIMQQxzAQAoFpF7lGFr24AAAAASUVORK5CYII=); }   在一个小圆圈内随意添加一些字母和数字在很多地方都可以看到, 包括 dyn.com . 每次当你想要在你的设计中使用这类嵌入的图片, 可以让你节省一次图片请求. 这么嵌入图片会使你的CSS文件变得更大, 因此你需要测试嵌入前后的页面加载时间确保是有意义的.\nMeasuring performance 下面这部分比较有趣一些: 确认你的努力是否得到了回报.\nGoogle的PageSpeed和Yahoo!的YSlow都而可以为你提供一些可以提高你的页面速度的建议, 包括确定哪个元素阻塞了页面渲染, 页面中不同的组件如HTML以及CSS的大小.\n同时, 我也推荐YSlow的拓展 3PO. 它可以检查你的网站所集成的一些流行的第三方脚本比如Facebook, Twitter, Google+等. 这个插件可以给你提高建议来进一步优化你的页面上的这些社交脚本进而提升页面速度.\n自从我使用了PageSpeed 和 YSlow的建议来提高页面性能以后, WebPageTest.org已经成为我首选的基准参考工具. 它非常详细地给出关于请求, 文件大小, 时间等信息, 并且提供不同地方不同浏览器的测试.\n确定基准可以在你设计的时候帮助你排除故障. 测量性能并分析结果可以帮助你的页面在大屏幕或者小屏幕上提速. 你可以测试或者定位像conditional loading of images这类技术从而在开发中更轻松应对性能问题.\nThe impact of web performance 网页的性能影响着你的用户\u0026ndash;这意味着了解, 测量,提高性能问题是每个人的工作. 所有的这些技术都可以提高页面的加载速度. 进而可以显著的提高你的网站的用户体验.\n用户更高兴了也就意味着更高的转化率, 无论你是按照什么来衡量, 或是收入, 注册数, 回访数, 还是下载数. 在更快的页面速度下, 用户可以使用你的网站在短时间内完成他们想要做的事\u0026ndash;即使可能只是在等待红绿灯.\n原文来自Lara Swanson Improving UX Through Front-End Performance\n","permalink":"https://xguox.me/improving-ux-through-front-end-performance.html/","tags":["Translation"],"title":"Improving UX Through Front-End Performance"},{"categories":["Translation","JavaScript"],"contents":" 由于某些原因,关于Meteor一直都有着各种各样的误解.或许是因为Meteor刚正式推出的时候, 缺失很多重要的功能.而很多不知道的是,这些缺陷在后来早就被修复了.\n为了给Meteor正名,下面列举的是5个最常见的误解.\n误解#1:\u0026ldquo;Meteor的数据不安全\u0026rdquo; 在Meteor的第一个Demo推出之时, 数据安全和用户权限这些东西是没有的.任何客户端的用户都能够访问,修改数据库的数据然后发送返回到服务端.\n但这第一个版本只是作为一个提示告诉大家Meteor来了, 并不意味着它可以被用来打造一个真实的应用产品. 打那之后, 现在的Meteor已经有了一个内置的身份认证包(authentication package).\n造成误解的另一个原因是, 许多新手为了更容易地快速上手常常在创建Meteor应用的时候, 遵循默认的把所有的数据发布给客户端.\n正如Meteor的开发者之一 Avital Oliver在StackOverflow上的回答那样\n 当你使用Meteor的命令行工具创建一个应用的时候, 默认的会包含有两个packages:AUTOPUBLISH和INSECURE. 这两个packages能使得每个客户端都拥有所有的服务器数据库读写权限效果.这是非常好的原型工具(目的只为了开发),但通常这不适合用于应用的生产环境.当你需要发布到生产环境的时候,请去掉这两个packages\n 类似地,也因为Meteor可以共享服务端和客户端的代码,但实际上,这并不意味着所有的代码都可用于客户端.\n所有放在server/目录下的东西都是只能在服务端执行使用的, 同时Meteor也让你可以定义服务端安全方法,这些方法可以在客户端调用.\n误解#2:\u0026ldquo;Meteor对SEO不友好\u0026rdquo; 与传统的网页应用不一样,Meteor不会在服务端搭建HTML页面.取而代之, 服务端只发送数据, 然后在客户端决定如何渲染之(这就是Meteor的\u0026rdquo;data on the wire\u0026rdquo;原则)\n这种做法的缺点就是, 当JavaScript被禁用的话, 你的网站几乎跟一个空白页面无异.\n你或许会觉得这会迷惑搜索引擎.但幸运地是, Meteor有解决的方案,那就是使用Spiderable Package.\n这个package使用的是PhantomJS 在服务端预渲染后提供给网络爬虫.实际上网络爬虫得到的网页跟你看到的是一样的.\n误解#3:\u0026ldquo;Meteor不支持第三方packages\u0026rdquo; 有很多的讨论是关于Meteor所用的不是Node的标准包管理\u0026ndash;NPM. 而事实上, Meteor压根就没有不允许使用第三方程序包.\nAnd it\u0026rsquo;s true that the vanilla install of Meteor does not come with a package manager. 但庆幸的是, 本书的合著者之一Tom Coleman, 同时也是Meteor包管理器Meteorite和packages资源库Atmosphere的开发者.\n尽管当下Meteorite 和 Atmosphere还不是Meteor的官方部分, 但它们已经得到广泛的使用并将会在未来被合并到Meteor的核心部分.事实上,这方面的工作已经在Meteor的核心代码的engine分支中开展,预期会被很快解决.\n误解#4:\u0026ldquo;Meteor是完全封闭的(walled garden)\u0026rdquo; 一个密切相关的异议是,Meteor可以支持第三方packages,但这些第三方packages却严格地限制于流星的生态系统.\n为什么有成千上万的Node packages 在那还要使用自定义的package呢?这又是否与开源精神相悖呢?\n首先,需要意识到一点很重要的就是, 在Meteor中可以任意的使用node packages.尽管在当下这么做可能还会有一些棱角使得接口不那么顺利,但这种情况在engine分支添加Node support 并发布后将得到很大改善.\n其次,Meteor不仅仅是Node.js的一个框架, 更是一种全新的网站应用开发方式和构想.\n换句话说,想知道为什么Meteor不使用NPM无异于问为什么它不能使用Ruby Gems. 有很多人建议所有的JavaScript代码应该都用NPM来管理, 但这么做的话会有一些细节上的问题, 正如核心开发者解释说.\n误解#5:\u0026ldquo;Meteor只适用于原型\u0026rdquo; \u0026ldquo;Meteor非常适用于小规模快速原型的项目,但它不适合用于大规模的应用.\u0026rdquo;\n在某种程度上, 这很难争个谁对谁错.毕竟Meteor的主页的版本号还是以0开头.\n但事实是Meteor还没准备到一个黄金时间,比起内在的局限性,在如此青涩的时期(不到一岁)Meteor还有许多需要做的.\n再说, 即使是现在, 有时牺牲一点稳定性来做交换得到Meteor带来的轻松且快速的开发还是值得的.如果你希望看到一些具体的Meteor案例,我可以推荐看看Telescope吗?\nSo,我坚信,在Meteor这样快速发展之下,今天的技术原型将会是明天thousand-user 的应用.\n总结 现在,即使我们正在写这本关于Meteor的书, 但我们首先依旧会承认一点, 如同其他的技术那样,Meteor同样也有着自己的缺陷.\nBut I do believe Meteor has enough potential to make it worth evaluating it on its own merits. 所以如果你犹豫不决是否使用Meteor, 希望我的建议能说服你你选择它.\n原文来自 Sacha Greif 5 Meteor Misconceptions\n","permalink":"https://xguox.me/5-meteor-misconceptions.html/","tags":["Translation","JavaScript"],"title":"5 Meteor Misconceptions"},{"categories":["Translation","JavaScript"],"contents":" 最近,有许多关于 Derby.js的激动人心的讨论涌现在了我的Twitter Timeline. 我从未使用过能够帮你做这么多\u0026ndash;实时同步客户端和服务端 的框架. 从本质上讲, 这使得我们可以自己编写一个代码量很少的应用可以让两个人编写同一个 text field–live. 而 Derby帮你处理了在 models 和 views 之间所有的同步. 就如 Google Docs 的协作编辑那样.\n这非常的伟大, 但经过深入的研究, 发现 Derby.js 并没有我想象中的那么成熟\u0026ndash;目前还没有到1.0版本. 当然, Node.js 和 Meteor 也同样没有. 但相比起来, 似乎Derby缺少的更多一些. 比如, 据我目前知道的, 还没有一个好使的方法来处理sessions. 或许是因为缺乏文档的原因吧, 但是, 据说Derby的团队目前正在开发authentication. 如果有谁有一些关于Derby.js 处理sessions的新手指引, 我会很乐意去研究的.\n另外一个我经常见到被拿来与Derby.js做比较的框架是Meteor. 与Derby相似的是,它也能在多个客户端下实时更新views, 尽管做法上可能跟Derby有点不同. Derby可以较容易的使用多种数据库, 而Meteor则只亲近于MongoDB. 事实上, 通过如Mongoose客户端接入数据库的API与你在服务端所期望的已经非常接近了.\n比起现在node的一些缺点和争议, Meteor看起来是非常有趣的选择用来建立有实时需求的应用. 个人觉得还是Derby基于传统回调的编程形式更吸引我, 但在Derby的强大背后,却缺乏健壮的文档和一个大的开发者社区, 这无疑是个很大的打击. 或许这会随着时间推移而有所改变吧, 但比起Meteor来说还是会慢很多, 因为后者最近获得了1100万美元的资金. 这笔财政资金确保了Meteor的存在以及得到持续的支持. 对于那些需要财政与发展稳定的框架的开发者而言, 这笔资金只会让Meteor更加优胜.\n今天,让我们一起来看看如何新建一个真实的但又简单的Meteor应用. 本质上说, 这是基于Tom的 Vimeo screencast的一个新手指引. 与Tom的 Vimeo screencast最大的不同是处理事件的方式. 比起复制粘贴一个Meteor示例的代码, 我会一步一步的通过自己的方式来处理使用Enter键来提交一则讯息. 让我们开始吧.\nCreating a Meteor App Derby和Meteor 他们共有的一个大加分是他们各自的命令行工具. 与Derby使用Node的内置的 npm 工具所不同的是, Meteor使用的是它自己的.\n在终端(Mac OS X 和 Linux),执行如下的命令. (在这之前请确保你已经安装了Node)\n1  $ curl https://install.Meteor.com | /bin/sh   接下来的事Meteor会自己搞定了.\n要新建一个项目, 先转到你的工作目录然后运行下边的代码. 这会创建一个目录, 里边包括有Meteor和一个最基本模板程序.\n1  $ Meteor create chat   现在, 你可以转到该目录并运行下面的代码让它跑起来\n1 2 3  $ cd chat $ Meteor Running on: http://localhost:3000/   想要看到这个最基础的应用程序, 你只需要在任意一款不过时的浏览器下打开http://localhost:3000/\n只要你想, 你可以使用Meteor内置的Meteor deploy命令来部署你的应用到Meteor自己的服务器上\n1  $ Meteor deploy my-app-name.Meteor.com   只要你更新保存了你的代码, 所有连接上的浏览器都会实时更新其页面.\nDeveloping the Chat App 通过Meteor create命令生成的文件夹里, 你可以看到几个不同的文件. 如果你的系统设置了显示隐藏文件, 那还可以看到一个.Meteor文件夹. 这个文件夹包括的Meteor本身, 以及MongoDB数据库的文件.\n在你的项目的根目录, 你可以看到的还有chat.html chat.js chat.css. 这三个文件自解释?的. HTML包含了这个应用的模板和视图, 并都通过chat.css来添加样式. 而这个JavaScript文件则包含了在客户端和服务端执行的代码. 这非常的重要\u0026ndash;请不要把任何东西比如配置数据和密码都放在这里, 因为任何人都能通过查看你的应用程序的源文件看到.\n用你最喜欢的文本编辑器打开chat.js. 我个人使用的是Sublime Text 2, 因为它很简洁并且有多光标功能. 你可以在chat.js看到下面这些代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  if (Meteor.is_client) { Template.hello.greeting = function () { return \u0026#34;Welcome to chat.\u0026#34;; }; Template.hello.events = { \u0026#39;click input\u0026#39; : function () { // template data, if any, is available in \u0026#39;this\u0026#39;  if (typeof console !== \u0026#39;undefined\u0026#39;) console.log(\u0026#34;You pressed the button\u0026#34;); } }; } if (Meteor.is_server) { Meteor.startup(function () { // code to run on server at startup  }); }    分别注意到if语句里的Meteor.is_client和Meteor.is_server部分. 在这里边的代码分别只在客户端和服务端下执行. 这就是Meteor共用代码的能力了.\n删除掉Meteor.is_server这段代码并把if (Meteor.is_client)里边的代码删除了,只留如下部分:\n1 2 3 4  if (Meteor.is_client) { }    注意到, 当你保存了你的脚本之后, 浏览器会马上重新加载这个新脚本.\nCreating the View 在我们正式对这个脚本文件动工之前, 我们需要先新建一个视图用来展示聊天记录. 在编辑器里打开chat.html并删除body标签里边的代码. 包括名为hello的template标签.只留如下部分\n1 2 3 4 5 6 7  \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;chat\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt;   接着在body标签里添加下面这句\n1  \\{\\{ \u0026gt; entryfield}}   Meteor使用的模板系统与Mustache很相似.大括号{{}}表示要呈现的内容. 通过简单地在两对大括号里添加内容如{{hello}}, 模板系统会用hello这个变量的值来替换它. 后面会更详细的介绍.\n注意到了在entryfield这个词前面有个大于号\u0026gt;了吗? 使用该符号来指定渲染哪一个模板.\n接下来使用下面这段代码来新建一个名叫entryfield的模板\n1 2 3  \u0026lt;template name=\u0026#34;entryfield\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;name\u0026#34; placeholder=\u0026#34;Name\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;message\u0026#34; placeholder=\u0026#34;Your Message\u0026#34; /\u0026gt; \u0026lt;/template\u0026gt;   在这个例子中,template标签只有一个属性\u0026ndash;用来表示这个template的名字. 也就是当我们渲染的时候需要用来指定的名字.\n查看浏览器, 你会发现页面已经刷新了, 输入框已经呈现出来了.\n接下来, 在body里边添加另外的一个mutache标签用以渲染讯息列表\n1  \\{\\{\u0026gt; message}}   最后, 我们还需要新建一个名叫messages的模板. 在entryfield模板下面添加下面这段代码\n1 2 3 4 5 6 7  \u0026lt;template name=\u0026#34;messages\u0026#34;\u0026gt; \u0026lt;p\u0026gt; \\{\\{#each messages}} \u0026lt;strong\u0026gt;{{name}}\u0026lt;/strong\u0026gt;- {{message}} \\{\\{/each}} \u0026lt;/p\u0026gt; \u0026lt;/template\u0026gt;   注意到each子句. 在Meteor中你可以使用如下的语法来遍历一个数组模板\n1 2  \\{\\{#each [name of array]}} \\{\\{/each}}   使用each循环时,上下文会有所改变. 当引用变量的时候, 实际上你引用的是每一个数组元素的值. 例如,在我们的chat应用中, 我们遍历了数组模板\u0026rdquo;messages\u0026rdquo;里边的每个元素, 该数组可以像下面这样,\n1 2 3 4 5 6 7 8 9 10 11  [ { \u0026#34;name\u0026#34;: \u0026#34;Andrew\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hello world!\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hey, Andrew!\u0026#34; } ]    然后, 在each循环中, 你可以看到{{message}}{{name}}, 这会引用 每一个数组元素的值来替代(Andrew 和 Bob 替换 name, 以及各自的问候信息.)\n当返回到你的浏览器, 你还看不到任何的改变. 因为讯息数组还没被传送到模板, 所以Meteor遍历不到任何东西来呈现.\n你的chat.html最后应该是这样的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;chat\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \\{\\{\u0026gt; entryfield}} \\{\\{\u0026gt; messages}} \u0026lt;/body\u0026gt; \u0026lt;template name=\u0026#34;entryfield\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;name\u0026#34; placeholder=\u0026#34;Name\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;message\u0026#34; placeholder=\u0026#34;Your Message\u0026#34; /\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;template name=\u0026#34;messages\u0026#34;\u0026gt; \u0026lt;p\u0026gt; \\{\\{#each messages}} \u0026lt;strong\u0026gt;{{name}}\u0026lt;/strong\u0026gt;- {{message}}\u0026lt;br/\u0026gt; \\{\\{/each}} \u0026lt;/p\u0026gt; \u0026lt;/template\u0026gt;   The Javascript 从现在开始, 我们处理的大部分代码都是客户端代码, 所以, 除非特别说明, 以下的代码都是在if (Meteor.is_client)代码块中.\n在我们编写展示讯息的代码之前,让我们先新建一个Collection. 从本质上讲, 这是一组Models. 换句话说, 在这个chat应用的环境下, Messages collection保存着整个聊天记录, 而每条讯息记录是一个Model.\n在if语句前, 添加如下代码来初始化Collection:\n1 2  Messages = new Meteor.Collection(\u0026#39;messages\u0026#39;);    因为我们希望这个Collection可以在客户端和服务端被创建, 所以我们把它写在了客户端代码块之外.\n由于Meteor为我们做了大部分的工作, 要展示聊天记录是非常容易的. 只需要把下面的代码添加进if语句里边.\n1 2 3 4  Template.messages.messages = function(){ return Messages.find({}, { sort: { time: -1 }}); }    让我们拆开来分析这段代码:\nTemplate.messages.messages = function(){ … }\n第一部分Template表示我们正在修改一个模板的行为.\nTemplate.messages.messages = function(){ … }\n第二部分messages是模板的名字, 表示是在修改哪一个模板.\n例如,如果我们想要对\u0026rdquo;entryfield\u0026rdquo;模板做些什么, 只需把代码改成Template.entryfields.variable = function(){ … }(在这里, 请别这么做)\nTemplate.messages.messages = function(){ … }\n第三部分的这个messages代表的是一个这个模板里的一个变量. 还记得我们的each循环遍历messages吗? 这就是那个mesaages.\n当你打开浏览器时, 页面还是没有什么改变. 这是意料之中的事, 因为我们只抓取的讯息, 而没有展示出来.\n此时,你的chat.js应该是这样的. 是否很惊讶就只需在服务器写这么些代码我们就能展示一个实时的聊天记录应用.\n1 2 3 4 5 6 7 8  Messages = new Meteor.Collection(\u0026#39;messages\u0026#39;); if (Meteor.is_client) { Template.messages.messages = function(){ return Messages.find({}, { sort: { time: -1 }}); } }    Adding a Message through the Console 这部分的内容是可选的, 当然它有助于你调试程序. 你可以直接跳过往下学习建立form来响应key press.\n如果你想要测试你的讯息显示代码, 你可以手动插入一条记录到数据库. 打开你的浏览器控制台, 并输入如下:\n1  Messages.insert({ name: \u0026#39;Andrew\u0026#39;, message: \u0026#39;Hello world!\u0026#39;, time: 0 })   这将会在数据库中新建一条记录, 如果正确的操作了的话,那浏览器就会即刻更新这条讯息在页面上.\nThe Message Entry Form 返回到chat.js, 我们即将要完成的是允许用户在输入框中提交聊天讯息到数据库. 在if语句里头最下面添加下面这段代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Template.entryfield.events = { \u0026#34;keydown #message\u0026#34;: function(event){ if(event.which == 13){ // Submit the form  var name = document.getElementById(\u0026#39;name\u0026#39;); var message = document.getElementById(\u0026#39;message\u0026#39;); if(name.value != \u0026#39;\u0026#39; \u0026amp;\u0026amp; message.value != \u0026#39;\u0026#39;){ Messages.insert({ name: name.value, message: message.value, time: Date.now() }); name.value = \u0026#39;\u0026#39;; message.value = \u0026#39;\u0026#39;; } } } }    这段代码比较多, 让我们一步步分析. 你应该还有印象, 紧跟Template之后的词表示我们即将要修改的template的名字. 跟前面修改的模板是messages不一样的是, 这里我们要修改的是entryfield. template的event属性会包含有一个对象, 这个对象的keys的格式如下:\n1  \u0026#34;[eventname] [selector]\u0026#34;   比如, 如果我们想绑定了一个click事件给了ID为hello的按钮. 则需要像下面这样添加一个events对象\n1  \u0026#34;click #hello\u0026#34;: function(event){ … }   在我们这个例子中,我们绑定了一个keydown事件给了ID为message的输入框. 这个输入框在我们一开始的chat.html已经建好.\n在events对象里, 每一个key都有一个函数作为它的值. 当事件被触发的时候, 这个event对象会被作为第一个参数传给这个函数.在我们的chat应用, 每一次在输入框中按下一个按键(keydown), 这个函数都会被调用.\n在函数里的代码非常简单. 首先, 我们先要检测按下的是否\u0026rdquo;enter\u0026rdquo;键(\u0026ldquo;enter\u0026rdquo;的keycode 是13). 接着,通过ID来得到两个输入框中的DOM元素. 最后, 我们检查确保输入框中的值不为空, 用户不允许发送空的名字和讯息.\n下面的代码需要注意了, 它会被直接插入到数据库中\n1 2 3 4 5 6  Messages.insert({ name: name.value, message: message.value, time: Date.now() });    你会发现,这其实跟我们直接在浏览器控制台中输入的代码很相似, 只是我们用DOM元素的值来替换固定值. 另外, 欧文木讷添加了时间值来确保是按照时间来排序.\n最后,我们把两个输入框设置为空''\n现在, 你可以打开浏览器尝试在输入框输入名字和讯息. 按下\u0026rdquo;enter\u0026rdquo;后,输入框会被被清空, 而且这则讯息会立马出现在你的输入框下边. 使用另外一个浏览器窗口或者其他浏览器打开同样的链接(http://127.0.0.1:3000). 尝试输入一则新的讯息, 此时你就会看到Meteor的强大之处.无须单独写一行代码来更新讯息记录, 就可以实时同步在不同的客户端浏览器中.\nConclusion While Meteor is pretty cool to work with and there are some pretty useful applications for it, like Derby.js, it is immature. For examples of this, just browse through the documentation and look for the red quotations. For example, the documentation states the following about MongoDB collections:\n Currently the client is given full write access to the collection. They can execute arbitrary Mongo update commands. Once we build authentication, you will be able to limit the client\u0026rsquo;s direct access to insert, update, and remove. We are also considering validators and other ORM-like functionality.\n 对于任何一个产品而言, 用户拥有数据库的所有读写权限都是非常大的问题, 非常危险.\n原文来自Andrew Munsell Introduction to Realtime Web with Meteor and Node.js\n另附上Tom的 Vimeo screencast(需FanQiang) http://av.vimeo.com/31624/419/93304834.mp4?aksessionid=d86b5b612c86d70bd2022ae1050aa1be\u0026amp;token=1362992353_ce10e413b18fa6f2d0e3483ef6b82894 640 320 http://b.vimeocdn.com/ts/278/431/278431171_960.jpg\n","permalink":"https://xguox.me/introduction-to-realtime-web-with-meteor-dot-js-and-node-dot-js.html/","tags":["Translation","JavaScript"],"title":"Introduction to Realtime Web with Meteor and Node.js"},{"categories":["Translation","JavaScript"],"contents":" 引言 想知道这个新奇又好玩的东西\u0026ndash; Meteor是如何工作的?那就太好了,你来对地方了. 我会向你展示一个Meteor项目的方方面面 并会给你一些最好的提示让你应用在未来自己的Meteor项目当中.\nWhat is Meteor Meteor能让你打造非常动态的的网页同时代码量出乎意料之少. 记住一点,Meteor目前还在一个超级测试的版本(原文发布时候为 preview 0.3.8, 翻译时为 preview 0.5.7). 所以, 请不要因为工作的不够完美而感到意外. Meteor是基于Node.js, 因此, 你所写的大部分代码也将会是JavaScript. 这没什么好惊奇的. 如果你想要一些好的资源来快递提高你的JavaScript, 可以看一看 JavaScript Garden. Meteor存储数据所使用的是MongoDB. 尽管如此, 实际上,Meteor使用的是minimongo作为接口. 它只是拥有相对完整的功能而非标准MongoDB的所有功能. 你并不需要确切地知道MongoDB是如何工作的, 但我还是建议你看一看Meteor 的Collections文档以便知道你能做些什么. Meteor目前使用的模板引擎只能是 handlebars. 但在未来支持使用别的模板引擎也是很有可能的.(翻译该文章时已经支持使用其他的如 Jade ) 最后, 只要你开发网站, 了解 HTML 和 CSS 都是必须的.\n基础 一个Meteor项目包含的大多都是JavaScript文件. 如果你放置任意一个 *.js 文件在你的项目的任何位置, Meteor会自动帮你加载并运行之. 你在Meteor项目中写的每一个JavaScript文件都会被部署到服务端和客户端(额\u0026hellip;也不完全是这样的,继续往下看吧!). 之所以Meteor这么,这么的cool,原因之一就是:当你写了一个JavaScript函数,在客户端和服务端都可以使用之! 更cool的是, 比如,你在项目的任意一个地方放置 *.less, 那么这些文件都会自动被编译并发送至客户端以及从而包含进页面当中. 有时候,可能你希望分离开服务端和客户端的代码. 幸运地是,\nMeteor有这么一对标识可以帮你: Meteor.is_server 和 Meteor.is_client 下面的例子在浏览器的JavaScript控制台中会输出 Hi. I'm CLIENT,而在服务端中则会输出 Hi. I'm SERVER\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // This function is available on both the client and the server. var greet = function(name) { console.log(\u0026#34;Hi. I\u0026#39;m \u0026#34; + name); } // Everything in here is only run on the server. if(Meteor.is_server) { greet(\u0026#34;SERVER\u0026#34;); } // Everything in here is only run on the client. if(Meteor.is_client) { greet(\u0026#34;CLIENT\u0026#34;); }    这非常的简单. 在客户端和服务端共用代码使你能够做到最大化的代码复用,这大大地减少了开发时间.\n项目文档结构 可能, 你无数次地希望不再服务端和客户端之间共享所有的一切. 比如你有一些私密的算法在服务端执行,而你绝不希望Meteor把这些算法发送至客户端让全世界都看得见. Meteor提供了两个\u0026rdquo;特殊\u0026rdquo;的目录来帮助分离客户端和服务端的代码:[project_root]/client/和[project_root]/server/. 在server目录的JavaScript只会在服务端执行而不会发送至客户端, 反之亦然. 这就使得我们无须在各个地方都使用Meteor.is_client 和 Meteor.is_server.取而代之, 对应的放置你的代码就OK了.\n项目的文件结构非常重要, 尤其在考虑到文件的先后加载顺序的时候. 假定有两个文件, 其中一个依赖于另外一个, 你知道你的JavaScript的加载顺序吗? 其规则如下:\n [project_root]/lib里的文件会最先被加载. 显而易见的, 库需要放在这个文件 文件会根据目录的深度来进行加载排序,放在深一层的文件会先加载 同一级的文件会按照字母顺序来排序加载. main.* 会最后加载, 当有代码需要在其他所有的库或脚本之后执行这会很有用.  Meteor有一些特殊的目录来帮助你解决分离 client/server的代码以及加载的顺序:\n [project_root]/lib/ 该目录下的代码会在你的 client/server 代码执行之前加载 [project_root]/client/ 这个目录下的代码只会在浏览器端而不会在服务端执行 [project_root]/server/ 这个目录下的代码只会在服务端而不会客户端执行 [project_root]/public/ 静态文件放在这个目录下, 并且你可以很轻松地在你的html中指向 image.jpg [project_root]/.Meteor/ 一些特殊的,和项目相关的的信息放在这里, 比如你安装的某些模块. 你几乎可以不用关心这里边的东西  响应(Reactivity) Meteor 通过 响应数据源(\u0026ldquo;reactive\u0026rdquo; data sources) 和 上下文(context) 帮你省去了当数据改变时手动替换页面的麻烦. 当响应数据源被更新后,响应上下文会重新运行(A reactive context is just a function that will get re-run if it contains a reactive data source that gets updated.) 刚开始的时候要把思维转换过来可能有些难, 下面的例子将为你解释清楚. 下面是一个html页面和一个叫cool_dudeMeteor模板, 以及一个在客户端运行的JavaScript函数\u0026ndash; 传一个username的值给模板用以渲染.\n1 2 3 4 5 6 7  \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \\{\\{\u0026gt; cool_dude }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  1 2 3  \u0026lt;template name=\u0026#34;cool_dude\u0026#34;\u0026gt; \u0026lt;p class=\u0026#34;important\u0026#34;\u0026gt;{{ username }} sure is one cool dude!\u0026lt;/p\u0026gt; \u0026lt;/template\u0026gt;  1 2 3 4 5  // On the client: Template.cool_dude.username = function() { return \u0026#34;Andrew Scala\u0026#34;; };    当页面被渲染的时候会输出 \u0026quot;Andrew Scala sure is one cool dude!\u0026quot;\n模板即是响应的上下文: 如果它依赖于响应数据源, 那么当数据源改变时它会重渲染自身. 客户端的session一种响应数据源. 它只会在客户端存储一些键值对, 且当页面刷新的时候被擦除.\n让我们把上面的例子模板上下文改为响应数据源:\n1 2 3 4 5 6 7 8 9 10  // When the app starts, // associate the key \u0026#34;username\u0026#34; with the string \u0026#34;Andrew Scala\u0026#34; Meteor.startup(function() { Session.set(\u0026#34;username\u0026#34;, \u0026#34;Andrew Scala\u0026#34;); }); Template.cool_dude.name = function() { return Session.get(\u0026#34;username\u0026#34;); };    现在,模板将会在Session里取到username的值. 在响应上下文我们有一个响应数据源. 如果存储在Session的username的值更改了, 模板会重新渲染新的值到页面上. 如给username设置一个新的值:\n1 2  Session.set(\u0026#34;username\u0026#34;, \u0026#34;Bill Murray\u0026#34;);    只要执行了这行代码,不管在哪, 页面都会立马输出 Bill Murray sure is one cool dude! 在这里, 我会列出其他的响应上下文和数据源,你也可以到 Meteor的Reactivity文档自行阅读.\n发布/订阅 Note: 在每一个项目的根目录下执行 $ Meteor remove autopublish, 否则它会默认发布你的所有数据到客户端, 这显然不是个好的做法\n服务端发布数据给客户端使用, 同时,客户端向服务端订阅已发布的数据. 在刚开始, 想要明白服务端发布数据和客户端订阅数据的关系会有些困难. 常规的经验是这样的: 客户端只能访问当前时间点需要操作的数据, 除此之外的均不能访问. 举个例子, 如果你有一个聊天应用, 客户端不能够接收你的网站上每一个频道的所有信息, 而只能是当前所访问的频道的信息. 同样的也不能够知道别的频道的用户.\n下面这是一个关于 发布/订阅 的方面教材, 客户端可以看到数据库中的每一条信息:\n1 2 3 4 5 6 7 8 9 10 11 12  var Messages = new Meteor.Collection(\u0026#34;messages\u0026#34;); if(Meteor.is_server) { Meteor.publish(\u0026#34;messages\u0026#34;, function() { return Messages.find({}); }); } if(Meteor.is_client) { Meteor.subscribe(\u0026#34;messages\u0026#34;); }    当在客户端Messages.find({}),可以得到数据库里的每一条信息. 我们可以指定一个参数使得在订阅的时候缩小范围只获取实际需要的信息(如在\u0026quot;cool_people_channel\u0026quot;频道中的信息)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  var Messages = new Meteor.Collection(\u0026#34;messages\u0026#34;); if(Meteor.is_server) { Meteor.publish(\u0026#34;messages\u0026#34;, function(channel_name) { return Messages.find({channel: channel_name}); }); } if(Meteor.is_client) { Session.set(\u0026#34;current_channel\u0026#34;, \u0026#34;cool_people_channel\u0026#34;); Meteor.autosubscribe(function() { Meteor.subscribe(\u0026#34;messages\u0026#34;, Session.get(\u0026#34;current_channel\u0026#34;)); }); }    Meteor.autorun是一个响应上下文, 意思是只要有响应数据源更新,所有在里头的东西都会重运行. 我们把当前所在的频道存储在Session的\u0026quot;current_channel\u0026quot;. 如果这个session值改变, 那么订阅(subscription)也就更新, 这样我们就能够访问其他的信息了. 如果用户想要加入\u0026quot;breakfast talk\u0026quot;这个频道. 我们可以运行Session.set(\u0026quot;current_channel\u0026quot;, \u0026quot;breakfast_talk\u0026quot;), 这会触发autosubscribe, 让我们可以且仅可访问\u0026quot;breakfast_talk\u0026quot;频道下的信息.\n或许,你很多次都希望发布所有的collection到客户端. 仔细思考客户端实际需要的是什么. 比起发送所有的文档, 只发送特定领域会来得明智些.\n服务端方法 由于客户端不允许查看数据库中有什么东西, 你一对会好奇客户端又是如何存储信息的. 解决办法是使用Meteor的服务端方法(Server Methods). 你在服务端定义了所有的函数用以一些危险的做法比如修改和更新数据, 然后让客户端像调用常规方法那样call这些函数并返回值. 客户端永远也不会看到具体的实现,也不会自己修改数据. 这些都是服务端做的事. 想要添加一个用户到数据库中, 假定有一个叫create_user 的方法\u0026ndash;传入一个username并让服务端插入一条记录. 它会返回给客户端一个ObjectID这样客户端可以抓取到这个用户的信息并做任何接下来要做的事.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  if(Meteor.is_server) { Meteor.methods({ create_user: function(username) { console.log(\u0026#34;CREATING USER\u0026#34;); var USER_id = Users.insert({name: username}); return user_id; }, }); } // Remember, the client\u0026#39;s browser only ever sees the code below: if(Meteor.is_client) { var username = \u0026#34;Andrew Scala\u0026#34;; Meteor.call(\u0026#34;create_user\u0026#34;, username, function(error, user_id) { Session.set(\u0026#34;user_id\u0026#34;, user_id); }); }    在这个例子中我的 user_id被设置到客户端session中, 当任意模板使用到我的user_id 的时候均立马可以得到更新.\n保护你的数据 默认的, 你可以在客户端打开一个JavaScript 控制台并运行数据库查询.这非常的不安全.最糟糕的情况是, 当我们在访问你这么cool的Meteor应用时可以运行Users.remove({})来擦除你的所有数据.\nMeteor即将会做一些措施来更好的保护你的数据, 但截至目前为止这是唯一的方法可以做到保护的. 这些代码被包含在了基于 Meteor的 madewith网站中.这个代码片段阻止了从客户端进行insert/update/remove等操作. 把下面这段代码放在你的服务端代码中的任意位置:\n1 2 3 4 5 6 7 8 9 10 11 12  // Relies on underscore.js. In your project directory: // $ Meteor add underscore Meteor.startup(function() { var collections = [\u0026#39;collection_name_1\u0026#39;, \u0026#39;collection_name_2\u0026#39;]; _.each(collections, function(collection) { _.each([\u0026#39;insert\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;remove\u0026#39;], function(method) { Meteor.default_server.method_handlers[\u0026#39;/\u0026#39; + collection + \u0026#39;/\u0026#39; + method] = function() {}; }); }); });    敬请关注 准备好开发一个真实的Meteor项目了吗?一起期待吧, Part 2 就在路上了.届时将引导你完成一个完整的app!\n原文来自Andrew Scala Learn Meteor Fundamentals and Best Practices Ps:文章有点老,XD\u0026hellip;不过就像该篇文章的作者所说, Meteor 现在还是处于一个 super-beta 阶段, 版本更新自然会相对来得快些. 据说1.0版本将会在不到一年的时间内发布.\n","permalink":"https://xguox.me/learn-meteor-fundamentals-and-best-practices.html/","tags":["Translation","JavaScript"],"title":"Meteor 基础与最佳实践"},{"categories":["Translation","JavaScript"],"contents":" LEVEL: BEGINNER 学习一门语言或框架的最好方法之一是从零开始开发一个应用程序.\n但是,当你开发了N次的To-do list应用之后, 或许你会想要一些稍微新鲜的玩意. 所以这一次, 让我们来看看一些实际的应用的代码, 这里我们用的是 Telescope\nTelescope是一个基于Meteor的开源社交新闻网站. Reddit or Hacker News的实时版本.\n我们即将看到的代码是简化版的post_edit模板及其控制器(controller). 听名字你应该就知道, 这个模板的作用是提供一个简单的表单来编辑一篇已经存在的post. post有标题(title), 链接(URl), 主体(body), 以及一个或多个的类别(categories). 该程序实际的表单中还会有一些额外的东西, 但我们现在不需要关心那些.\n这篇文章的目的是让你对Meteor的模板有个大概的认识. 如果你在其他的环境下使用过模板系统就会感到很熟悉, 如果没的话,那就现在开始学习吧.\nBasic Organization Meteor模板系统有两个部分组成,分别是: HTML模板和JavaScript控制器. HTML部分只是纯粹的有变量占位符的哑巴模板, 所有的实际动作都是通过JavaScript产生的.\n顺便说句, Meteor通过template标签的name属性来得到这个模板的名字(\u0026lt;template name=\u0026quot;post_edit\u0026quot;\u0026gt;), 因此,建议使用同一名字命名所有相关的文件和模板本身, 如在这个例子中的post_edit.html和post_edit.js\npost_edit.html template\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \u0026lt;template name=\u0026#34;post_edit\u0026#34;\u0026gt; \\{\\{#with post}} \u0026lt;form\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;Title\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;title\u0026#34; type=\u0026#34;text\u0026#34; value=\u0026#34;{{headline}}\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;URL\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;url\u0026#34; type=\u0026#34;text\u0026#34; value=\u0026#34;{{url}}\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;Body\u0026lt;/label\u0026gt; \u0026lt;textarea id=\u0026#34;body\u0026#34; value=\u0026#34;\u0026#34; class=\u0026#34;input-xlarge\u0026#34;\u0026gt;{{body}}\u0026lt;/textarea\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;Category\u0026lt;/label\u0026gt; \\{\\{#each categories}} \u0026lt;label class=\u0026#34;checkbox inline\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;category_{{_id}}\u0026#34; type=\u0026#34;checkbox\u0026#34; value=\u0026#34;{{_id}}\u0026#34; name=\u0026#34;category\u0026#34; {{hasCategory}} /\u0026gt; {{name}} \u0026lt;/label\u0026gt; \\{\\{/each}} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-actions\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;delete-link\u0026#34; href=\u0026#34;/posts/deleted\u0026#34;\u0026gt;Delete Post\u0026lt;/a\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \\{\\{/with}} \u0026lt;/template\u0026gt;   post_edit.js controller\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  Template.post_edit.helpers({ post: function() { return Posts.findOne(Session.get(\u0026#39;selectedPostId\u0026#39;)); }, categories: function() { return Categories.find().fetch(); }, hasCategory: function() { var post = Posts.findOne(Session.get(\u0026#39;selectedPostId\u0026#39;)); return _.contains(post.categories, this.name) ? \u0026#39;checked\u0026#39; : \u0026#39;\u0026#39;; } }); Template.post_edit.events = { \u0026#39;click input[type=submit]\u0026#39;: function(e) { e.preventDefault(); var selectedPostId = Session.get(\u0026#39;selectedPostId\u0026#39;); var post = Posts.findOne(selectedPostId); var categories = []; $(\u0026#39;input[name=category]:checked\u0026#39;).each(function() { categories.push($(this).val()); }); var properties = { title: $(\u0026#39;#title\u0026#39;).val(), url: $(\u0026#39;#url\u0026#39;).val() body: $(\u0026#39;#body\u0026#39;).val(), categories: categories, }; Posts.update(selectedPostId, {$set: properties}, function(error) { if (error) { alert(error.reason); } }); }, \u0026#39;click .delete-link\u0026#39;: function(e) { e.preventDefault(); if(confirm(\u0026#34;Are you sure?\u0026#34;)) { var selectedPostId = Session.get(\u0026#39;selectedPostId\u0026#39;); Posts.remove(selectedPostId); } } };    代码看起来很多, 但是只要拆开来看,即使你从未看过Meteor的代码也会觉得很容易.\nHandlebars Tags 首先来快速浏览下HTML部分. 你一定注意到这种奇怪的标签用法了吧. 这些都是Handlebars标签用来让我们展示动态数据. 先来看第一个 handlebars 标签 \\{\\{#with post}}\nThe \u0026ldquo;with\u0026rdquo; block helper\n1 2 3 4 5  \u0026lt;template name=\u0026#34;post_edit\u0026#34;\u0026gt; \\{\\{#with post}} \u0026lt;!-- do something with \u0026#34;post\u0026#34; --\u0026gt; \\{\\{/with}} \u0026lt;/template\u0026gt;   block helper是一种快捷的方式告诉我们要在哪个模板对象上工作. 在\\{\\{#with post}}block里, 我们的应用知道this代表一个post. this也是传说中方法的上下文.\nTemplate Helpers 你可能会好奇,我们是从哪里得到post, 找到你的controller里的\n1 2 3 4 5 6 7 8  //The \u0026#34;post\u0026#34; template helper  Template.post_edit.helpers({ post: function() { return Posts.findOne(Session.get(\u0026#39;selectedPostId\u0026#39;)); } });    在这我们要做的是添加一个postTemplate Helper到我们的post_edit模板. Template helpers 是一些简单的JavaScript方法允许你使用 Handlebars template.\n在这个例子中,不管任何时候我们的Handlebars模板调用post, 我们都能在我们的Posts collection 中找到current post并返回之. 要做到这样首先我们需要用到selectedPostId这个通过路由设置的Session变量(but that\u0026rsquo;s for another lesson)\nObject Properties 继续往下, 我们将看到第二个handlebars标签{{title}}\n1 2 3 4 5  //The \u0026#34;title\u0026#34; template helper \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;Title\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;title\u0026#34; type=\u0026#34;text\u0026#34; value=\u0026#34;{{title}}\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt;   尽管不能用什么方式告诉你这个{{title}}不是一个Template helper, 而是我们的post对象的属性之一. 无须任何其他的工作,Meteor很神奇地提供给了我们使用的.\nNested Objects 链接(URL)和主体(Body)表单域用的都是相同的模式, 而类别(Category)域则有些不同了.在这里, 我们希望通过类别检查展示一系列复选框给所有网站的预定义类别.\n首先的工作是准备一系列的类别选择, 我们通过\\{\\{#each categories}}这个block helper来完成:\nLooping over categories with the \u0026ldquo;each\u0026rdquo; helper\n1 2 3 4 5 6 7 8  \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;Category\u0026lt;/label\u0026gt; \\{\\{#each categories}} \u0026lt;label class=\u0026#34;checkbox inline\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;category_{{_id}}\u0026#34; type=\u0026#34;checkbox\u0026#34; value=\u0026#34;{{_id}}\u0026#34; name=\u0026#34;category\u0026#34; {{hasCategory}} /\u0026gt; {{name}} \u0026lt;/label\u0026gt; \\{\\{/each}} \u0026lt;/div\u0026gt;   这个helper完成了两件事: 遍历categories对象, 以及在代码块里使用this来表示每一个遍历结果.\n或许你已经猜到, categories像posts那样是一个Template helper. 在这个例子中,我们只返回了在我们的数据库中的一系列类别(categories)\nThe \u0026ldquo;categories\u0026rdquo; template helper\n1 2 3 4  categories: function() { return Categories.find(); }    还记得我说过block helper 会改变this的值吗?这就是为什么我们能看到{{_id}}而不会混乱. 因为这事在block helper里边, 因此可以知道,我们处理的是category的ID,而不是post的ID\nMore Helpers 到目前为止, 我们的helper简单地查询了我们的collections并返回了数据. 但helper却不仅限于此. 让我们一起看一看 {{hasCategory}}这个helper\nThe \u0026ldquo;hasCategory\u0026rdquo; template helper\n1 2 3 4 5  hasCategory: function() { var post = Posts.findOne(Session.get(\u0026#39;selectedPostId\u0026#39;)); return _.contains(post.categories, this.name) ? \u0026#39;checked\u0026#39; : \u0026#39;\u0026#39;; }    还记得吧, 我们是在block \\{\\{#each categories}}里调用了这个helper, 这就意味着这里this表示的是category\n那么我们现在是首次得到我们的current post对象, 然后检查看current category是否包含在这个post的categories里.如果包含有, 则我们很容易地返回\u0026rdquo;checked\u0026rdquo;字符串用以标记我们这个复选框被选中.\n你可能已经注意到, _.contains()是一个 Underscore方法. Underscore是一个非常有用的JavaScript工具包并被内置绑定到了Meteor里边.\nTemplate Events 我们现在的代码足够用来加载一篇post的标题,URL,主体,以及类别了. 但还是需要一种保存这个表单的方法. 也就是下面的模板事件(template events)\nTemplate events\n1 2 3 4 5 6 7 8 9  Template.post_edit.events = { \u0026#39;click input[type=submit]\u0026#39;: function(e) { // do something when the users clicks input[type=submit]  }, \u0026#39;click .delete-link\u0026#39;: function(e) { // do something when the users clicks .delete-link  } };    如你所见, 定义 template events其实就是简单把事件和CSS选择器放在一起. 在这里我们有两个处理监听,一个是提交表单,另一个是删除post. 先来看看表单提交处理:\nOur form submit handler\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  \u0026#39;click input[type=submit]\u0026#39;: function(e) { e.preventDefault(); var selectedPostId = Session.get(\u0026#39;selectedPostId\u0026#39;); var post = Posts.findOne(selectedPostId); var categories = []; $(\u0026#39;input[name=category]:checked\u0026#39;).each(function() { categories.push($(this).val()); }); var properties = { title: $(\u0026#39;#title\u0026#39;).val(), url: $(\u0026#39;#url\u0026#39;).val() body: $(\u0026#39;#body\u0026#39;).val(), categories: categories, }; Posts.update(selectedPostId, {$set: properties}); },    处理方法带有两个参数, event 和 template 实例. 现在, 我们可以忽略template 实例. 使用e.preventDefault();来开始你的处理可以很好的阻止浏览器执行默认的事件行为.\n如你所见, 这里有大量的$符号. 除了Underscore, Meteor还绑定了jQuery. 看吧, 你已经了解了Meteor的一半了.\n再来一次, 处理categories相较于其他的来说比较麻烦一些. 通过使用jQuery, we\u0026rsquo;re checking for checked checkboxes(敢不敢把这句话说上10次,LOL)并使用他们的ID来填充给一个数组.\n接下来我们就能够建立我们的属性对象, 并使用Meteor的update()可以简单地提交我们的修改给数据库.\n删除post更是简单 Our post delete handler\n1 2 3 4 5 6 7 8  \u0026#39;click .delete-link\u0026#39;: function(e) { e.preventDefault(); if(confirm(\u0026#34;Are you sure?\u0026#34;)) { var selectedPostId = Session.get(\u0026#39;selectedPostId\u0026#39;); Posts.remove(selectedPostId); } }    只需要简单地把要删除的ID传送给Meteor的remove()方法就搞定.\nOther Callbacks Meteor给管理模板提供了一些其他的回调, 比如created和render回调\n尽管我们在这里没有使用到, 但只要Meteor完成渲染一个模板, 那么rendered回调可以很容易的通过jQuery来做一些DOM操作.\nConclusion CRUD (Create, Read, Update, Delete)操作经常就是组成网站的主要功能. Meteor能如此简单的完成这些工作显然是要大加分的.\n更重要的是, Meteor代码所依赖的很多模式和库都是你所熟悉的.这意味着可以非常快的掌握,只需要一些JavaScript知识.\n当然,随着您的需求的发展,你需要深入挖掘这个框架的复杂性.但是即使你只知道Meteor最基本的功能你也同样能作出一些东西来, 我想,这大概就是为什么Meteor非常好玩的原因之一.\n原文来自 Sacha Greif A Look at a Meteor Template\n","permalink":"https://xguox.me/a-look-at-a-meteor-template.html/","tags":["Translation","JavaScript"],"title":"A Look at a Meteor Template"},{"categories":["Translation","JavaScript"],"contents":" LEVEL: BEGINNER 当你看到这,也就说明你听说了Meteor这东西, 并对它产生一些好奇. 那就让我们创建一个最基本的应用来耍一耍吧. 希望能以此学习到一两样东西.\n准备工作: 安装Meteor 要安装Meteor那就再简单不过了. 只需要执行下面这条命令 curl https://install.Meteor.com | /bin/sh\n只要你是在支持的平台上运行,跟着步骤来就基本没什么问题.\n接下来让我们创建一个简单的app来确定是否安装成功. Meteor create forum 如果安装成功, 那执行下面的步骤就可以把这个app跑起来了\n1 2  cd forum Meteor   如果这一切都顺利的话, 那么在浏览器打开http://localhost:3000, 就会看到: 设置我们的forum 这个使用Meteor生成的简单的应用程序并不能做些什么; 只是或多或少示范了如何呈现模板和响应事件.\n而今, 我们只需要用浏览器的控制台(比如Chrome的Inspector)来捣鼓一些我们这个应用的数据. 因此,我们不必太过在意现有的用户界面. 一切从简. 在创建一个posts集合并在一个列表中一一展示前,我们先在 forum.html 这么干:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Forum\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% raw %} {{\u0026gt; posts}} {% endraw %} \u0026lt;/body\u0026gt; \u0026lt;template name=\u0026#34;posts\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Posts\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; {% raw %} \\{\\{#each posts}}{% endraw %} \u0026lt;li\u0026gt;{% raw %}{{name}}{% endraw %}\u0026lt;/li\u0026gt; {% raw %}\\{\\{/each}}{% endraw %} \u0026lt;/ul\u0026gt; \u0026lt;/template\u0026gt;   当编辑完成上面这段代码后, 你应该会看到浏览器自动重载了页面. 现在, 除了一个\u0026rdquo;Posts\u0026rdquo;标题,你应该什么也看不到. 为什么会这样呢?\n我们刚刚使用了{%raw%}{{ #each }}{%endraw%}这个helper来告诉Meteor创建了一个无序列表并且在其里边的每个li标签展示一条post. 但是在这里我们的posts为空. 所以页面中什么也没显示出来. 那么接下来让我们来生成一些posts吧.\n在forum.js, 我们创建了一个叫posts的Collection, 并使用之前所建立的posts模板加载之:\n1 2 3 4 5 6 7 8 9 10  var Posts = new Meteor.Collection(\u0026#39;posts\u0026#39;); if (Meteor.isClient) { Template.posts.helpers({ posts: function() { return Posts.find(); } }) }    这看起来只是一个简单的变化,但事实并非如此. 我们做的事是在客户端和服务端创建了一个叫作posts的Meteor Collection. 这是什么意思呢?\n在服务端,当数据被添加进来之后, 会被插入写进到一个mongo数据库(mongo内置包含在每个Meteor应用程序). 当执行查询的时候, Meteor会查找到这个数据库. 这样我们其实就已经把这些posts永久性的存贮起来了.\n在客户端,posts 集合会自动连接到服务端并在浏览器,本地镜像或者缓存中创建这个服务端集合. 这样, 当在浏览器中新建posts,这些posts就会直达服务端从而插入到mongo数据库中,并传输到其他连接上的本地的posts集合. 很难理解是吧?你很快就会知道这一切是怎么回事.\n添加完以上的js以后, 你不会看到我们的应用程序发生任何的改变; 这是因为我们还没添加任何一条的数据. 那么现在开始添加吧.\n在你的浏览器控制台中操纵数据 让我们来仔细看看这个神奇的集合. 在这我们会使用到浏览器控制台, 尽管我们假定你一直跟着我们的步伐并使用了Chrome的Inspector, 但其他的一些浏览器控制台(如Firebug等)也一样可以完成.\n首先, 让我们看看集合里有些什么东西. 运行下面的代码,我们会告诉posts模板查找该集合中的数据:\n1 2  Posts.find() » ‣LocalCollection.Cursor   Cursor是一个匹配指定的查询(在本例中为空)所得到的Meteor数据源. 现在看起来不那么有趣, 让我们使用下面这条语句来提取数据:\n1 2  Posts.find().fetch() » []   由于还没有一条post, 这会返回一个空数组. 让我们添加上一条吧!\n1  Posts.insert({name: \u0026#39;A Brand New Post\u0026#39;});   由于Meteor的响应式渲染( reactive rendering )使得HTML与底层的数据模型保持同步, 因此我们可以马上在页面上看到这条post 等一等,现在我们能够在客户端中从这个集合里抓取到这条post\n1 2  Posts.find().fetch() » [‣Object]   我们能够在本地集合中看到这篇post的存在. 对此没什么概念吗? 试试刷新页面;或者在一个新的tab打开http://localhost:3000,这篇post还在是吧!\n为什么会这样呢?在程序内部,Meteor已经同步了这个post对象({name: 'A Brand New Post'})到服务端, 并将其存储到mongo数据库中,这样,在所有的客户端都能访问到这篇post.我们同样能够在mongo的控制台中查看到它;在命令行中运行Meteor mongo(该命令运行的同时必须把项目跑起来),你也可以使用相似的接口来查看到:\n1 2  \u0026gt; db.posts.find() { \u0026#34;name\u0026#34; : \u0026#34;A Brand New Post\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;a72d6722-262b-4dc5-80f6-564796a6cc95\u0026#34; }   花上一点时间想想这对你来说有多么的神奇 (with no work on your part!), 然后肆意地在浏览器的控制台操纵insert update remove 命令(文档在这),同时在其他tab,其他浏览器甚至其他机器上(只要能看得到)看看同步的效果\n原文转自 Tom Coleman Getting Started with Meteor\n","permalink":"https://xguox.me/getting-started-with-meteor.html/","tags":["Translation","JavaScript"],"title":"Getting Started with Meteor"},{"categories":["Translation","Ruby"],"contents":" ECMAScript 5 中引入了一种定义,可以切换让你的JavaScript代码是否在所谓的 严格模式(strict mode)下执行.因为在strict mode下,当浏览器遇到一些脚本中包含有不好的代码做法时,即使没有语法上的错误也会抛出runtime错误,因而常常被吹捧为一种显著有益于捕获程序员错误的方式.\n在strict mode下运行被认为可以更少的出错 举个栗子,在定义一个变量时,缺少关键字\u0026rdquo;var\u0026rdquo;,如果在non-strict mode下会默认为该变量为全局变量,而在 strict mode下则会抛出一个runtime错误.\nNon-Strict Mode:\n1 2 3 4 5 6 7  (function () { x = 1; console.log(x); }()); //=\u0026gt; 1    Strict Mode:\n1 2 3 4 5 6 7 8  (function () { \u0026#34;use strict\u0026#34;; x = 1; console.log(x); }()); //=\u0026gt; ReferenceError: x is not defined    但strict mode太爱出风头了 有些时候这样的严格会是一把双刃剑.我们都希望知道在我们的代码中的这些无声的bug,但绝对不会希望看到这些bug在我们运营上线的网站上抛出runtime错误给了我们的最终用户.尤其是,当这些代码实际上都按我们的期望正常运行着的时候,只是这些代码有一些\u0026rdquo;bug\u0026rdquo;\n可以同时使用两种方式吗? 你不会得到许多的同情,而应该开始抱怨JavaScript暴露了一个bug给你的用户而你自己却不知道,除此之外,你可能会想这可以有两全其美的方法-在测试代码下使用strict mode,而生产环境下不使用.但这个想法有几个问题,首先,这很可能会使你的生产代码运行起来比测试代码要慢的多(见下文),然后,这两种代码可能跑起来会有所不同(见下文).\nNon-Strict Mode 下代码可能跑得要慢一些 或许这会跟你的直觉相悖,可能你会认为更多的错误检测和一些额外的\u0026rdquo;事情\u0026rdquo;一定会使得引擎做得更多从而你的代码跑得更慢.但Chris Heilmann总结出了使用strict mode跑的更快的原因.他的解释如下:\n Strict mode 修复了那些误区使得JavaScript引擎性能得到优化: 相较于非strict mode而言,strict mode代码能跑的更快. Firefox 4 还未优化strict mode ,但其后续版本会.\n 也就是说是否去掉strict mode 在生产环境意义并不是那么的明显, 但如果没有令人信服的考虑,去掉strict mode的话你实际上是改变了你的代码的执行方式.下面是几个例子.\n是否为Strict Mode,函数的上下文会有所不同 在non-strict mode,任意全局函数中,保留字\u0026rdquo;this\u0026ldquo;默认指向全局对象(对于浏览器而言,这个对象会是 window对象).而在strict mode,\u0026rdquo;this\u0026ldquo;默认则为undefined Non-strict Mode:\n1 2 3 4 5  (function () { console.log(this); }()); //=\u0026gt; Window    Strict Mode:\n1 2 3 4 5 6  (function () { \u0026#34;use strict\u0026#34;; console.log(this); }()); //=\u0026gt; undefined    在一个函数中使用 call() 你可以设定 this 的指代,但在这两种不同的模式下,你用来充当上下文的值会巧妙地变得有些不一样. 在 Non-strict Mode,如果上下文的值是原始的(primitive),比如数字 1,那它将会被转换成一个对象,一个Number对象.而在 strict mode ,这个原始的值 1, 将会被保留. Non-strict Mode:\n1 2 3 4 5 6  console.log(function () { return this; }.call(1)); //=\u0026gt; Number {}    Strict Mode:\n1 2 3 4 5 6 7  console.log(function () { \u0026#34;use strict\u0026#34;; return this; }.call(1)); //=\u0026gt; 1    是否为Strict Mode,arguments对象的行为有所不同 arguments 对象包含了引用函数的形参.在 non-strict mode下,修改arguments对象的值同时也会修改对应的形参的值,而在Strict Mode之下,两者是独立的. Non-strict Mode:\n1 2 3 4 5 6 7 8  (function (a, b) { arguments[0] = arguments[1]; console.log(a, b); }(\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;)); //=\u0026gt; two two    Strict Mode:\n1 2 3 4 5 6 7 8 9  (function (a, b) { \u0026#34;use strict\u0026#34;; arguments[0] = arguments[1]; console.log(a, b); }(\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;)); //=\u0026gt; one two    为什么你无论如何也要移除严格模式 考虑到在你的代码中的所有可能的变更行为,你可能会认为有明确的理由不去除\u0026rdquo;use strict\u0026rdquo;,毕竟,你也不希望你的做法会改变代码原本的运行方式.对不?但事实上,你确实会遇上这些问题,即使你保留了\u0026rdquo;use strict\u0026rdquo;.这是因为,你的\u0026rdquo;use strict\u0026rdquo;在一些旧版本的浏览器下会直接被无视,它们不认识这句声明.因此,对于你的其中一些用户,你还不如把\u0026rdquo;use strict\u0026rdquo;去掉更好.这样,他们也将得到同样的而在你想象的现代浏览器中不会得到的不严格行为.如果希望使用严格模式的同时支持旧版本的浏览器,你需要明白所有的严格模式下是否有效的副作用,然后避开对它们的依赖.\n你可能会觉得兼容性胜利了以及最好从来都没有出现过什么\u0026rdquo;use strict\u0026rdquo;的效果.又或者,你可能 觉得你了解了所有的副作用影响并坚持使用\u0026rdquo;use strict\u0026rdquo;,但还有另外一个问题需要考虑到的.如果通过连接和压缩你在生产环境下的代码来优化了HTTP请求,你可能会遇到一个与\u0026rdquo;use strict\u0026rdquo;相关的问题.当在函数中声明了 \u0026ldquo;use strict\u0026rdquo; , 它只适用于该函数的词法作用域(lexical scope ).但如果在函数外声明,则将会作用于整一个文件.How could this come back to bite you?让我举个栗子\u0026hellip;\n在我的一些工作项目中,我们有时候会引入第三方的实用工具库到我们的应用程序中.而且很有可能会引入多个.当然,我们自己的代码是 concatenation-friendly ,因为我们只使用了函数范围内的 严格模式,但假设一下如果某一个第三方库在文件顶部\u0026ndash;不在任何函数域内声明了\u0026rdquo;use strict\u0026rdquo;.现在也想象一下另外一个第三方库不是严格的安全.通过串联这两个库你几乎可以肯定会得到不同的行为以及一些以往不会抛出的错误.\n当然也有一些方法可以缓解这些问题:避免使用任何在函数域外声明\u0026rdquo;use strict\u0026rdquo;的库,或者你可以放弃连接使用这些库并且 take the HTTP request hit to performance.这些个看起来都有些局限性,不那么吸引人.\n因此,不顾所有可能的移除严格模式副作用,在特定情况下,你可能会决定选择在生产环境下移除\u0026rdquo;use strict\u0026rdquo;.\n原文来自Michael Mathews,Should You \u0026ldquo;Use Strict\u0026rdquo; in Your Production JavaScript?\n","permalink":"https://xguox.me/should-you-use-strict-in-your-production-javascript.html/","tags":["Translation","Ruby"],"title":"你应该在生产环境下使用JavaScript的严格模式(strict mode)吗?"},{"categories":["Translation"],"contents":" First off,以俺目前的各种水平,想写出一些好的 technical articles 是有点难度的啦.于是乎,我选择了翻译国外一些好的文章.不管多么新鲜的技术,就算找不着中文资料,也一定没少英文文档.比如最近发现的几个挺不错weekly(其实也不是最近才发现,也订阅了有大半年吧,不过其实是只订不阅,或者阅的比较少.惰性心理啊.)以及一些国外牛人BLOG.Share下我的文章的主要来源吧.\n Smashing Magazine HTML5ROCKS CSS-Tricks JavaScript Weekly Ruby Weekly(最近几乎没怎么研究Ruby了) DailyJS nettus+ Paul Irish Rey Bango JavaScript.is(Sexy)  Obviously, 不可能有一篇新的我就翻译一篇. 要是那样的话我精尽人亡都搞不过来. 而且, 尴尬 + 惭愧的是, 本身我自己的Google Reader里也是常年1000+ unread =. =\n其实以前初初学Rails也是跟着全英文的Ruby on Rails Tutorial,另外也尝试翻译过Rubymonk以及近段时间翻的一些,本人的英文水平实在很渣,CET6不上300分的成绩也曾经考过.于是,都翻比较蹩脚.就好比之前那几篇,好些地方原本读原文还明白了, 翻译过后反而看不懂在讲什么.有些则是只会半句.好吧,的确尴尬!不过,会好起来的.\n说到英文水平,尽管老早的就知道重要性,却从未好好学习过,只能现在硬着头皮自愿加强迫自己上,看英文的电视剧(尽量不跟着字幕),用英文的社交网站(圈多些国外的),阅读外文的文章(好吧,几乎也就是一些科技类的), 听英文podcast(各种听不懂)\u0026hellip;\nPS,最近看@linux也经常翻译一些文章,不过他翻译的更多比较文学性的.\nUpdate: 2015.8 Udemy 最近释出的 Ruby on Rails Tutorial: Learn From Scratch 看着挺赞的, 不过没时间翻译了.\n","permalink":"https://xguox.me/about-translation.html/","tags":["Translation"],"title":"About My Translation"},{"categories":["Translation","JavaScript"],"contents":"几乎所有开发人员都会花一些时间在JavaScript上,有些时候是在使用JSON.stingify(以及与之相对应的,JSON.parse).JSON - Javascript Object Notation - 已经成为许多开发人员理想的数据交换格式(the go-to data-interchange format)- 并且有许多语言支持序列化为JSON,而不仅仅只有JavaScript本身.如果哪天你半夜起来又无法入睡了,可以查一查关于JSON的历史(tl;dr – Douglas Crockford is the brain behind it)\n在写JavaScript的时候,我们用JSON.stringify将某个值序列化为一个字符串值来表示一个对象.\n1 2 3 4 5 6 7 8 9 10 11 12  JSON.stringify({ name: \u0026#34;Jim Cowart\u0026#34;, country: \u0026#34;Jimbabwe\u0026#34; }); // 输出结果: \u0026#34;{\u0026#34;name\u0026#34;:\u0026#34;Jim Cowart\u0026#34;,\u0026#34;country\u0026#34;:\u0026#34;Jimbabwe\u0026#34;}\u0026#34;  JSON.stringify(\u0026#34;Oh look, a string!\u0026#34;); // 输出结果: \u0026#34;\u0026#34;Oh look, a string!\u0026#34;\u0026#34;  JSON.stringify([1,2,3,4,\u0026#34;open\u0026#34;,\u0026#34;the\u0026#34;,\u0026#34;door\u0026#34;]); // 输出结果: \u0026#34;[1,2,3,4,\u0026#34;open\u0026#34;,\u0026#34;the\u0026#34;,\u0026#34;door\u0026#34;]\u0026#34;    我不会反复地讨论关于序列化的规则,在这里你可以读到更多相关的.但最基本的,你需要知道下面这些:\n undefined 值、函数或者XML值会被忽略,除非当\u0026hellip; 如果你的数组当中含有 undefined值,函数或XML值,该数组中的这些值将会被当成 null  Let\u0026rsquo;s see about that:\n1 2 3 4 5 6  JSON.stringify({ doStuff: function() { }, doThings: [ function() {}, undefined ] }); // 输出结果: \u0026#34;{\u0026#34;doThings\u0026#34;:[null,null]}\u0026#34;     \u0026ldquo;太棒了,Jim.我知道了.它是一种数据交换格式.而行为是不会被序列化的.这些天我使用的大多数库都是在底层为我处理序列化(Most of the libraries I use these days handle serializing for me under the hood.).那为什么要专门写一篇post关于 JSON.stringify 呢?\u0026rdquo;\n 这个我知道,但是有一些实用的技巧迟早我们会需要用上的.Many of the larger applications I\u0026rsquo;ve worked on recently have had debug flags that can be flipped to enable various console logging from different components active on the page. 而我们明显不希望总是通过无数行console.log来筛选信息 - 但当这变得有需要的时候,让它的可读性更高是不是更nice呢?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // what if this: \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Jim Cowart\u0026#34;,\u0026#34;location\u0026#34;:{\u0026#34;city\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Chattanooga\u0026#34;,\u0026#34;population\u0026#34;:167674},\u0026#34;state\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Tennessee\u0026#34;,\u0026#34;abbreviation\u0026#34;:\u0026#34;TN\u0026#34;,\u0026#34;population\u0026#34;:6403000}},\u0026#34;company\u0026#34;:\u0026#34;appendTo\u0026#34;}\u0026#39; // could be formatted like this, automagically? \u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;Jim Cowart\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Chattanooga\u0026#34;, \u0026#34;population\u0026#34;: 167674 }, \u0026#34;state\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Tennessee\u0026#34;, \u0026#34;abbreviation\u0026#34;: \u0026#34;TN\u0026#34;, \u0026#34;population\u0026#34;: 6403000 } }, \u0026#34;company\u0026#34;: \u0026#34;appendTo\u0026#34; }\u0026#34;    事实上,JSON.stringify可以传入3个参数 (JSON.stringify(value [, replacer [, space]]).其中,第三个参数 - \u0026ldquo;space\u0026rdquo; - 允许你指定一个字符串字符、或者使用缩进、或是一个数字.如果你传的是一个数字,那相应的空格数(最大为10)会被作为缩进量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  var person = { name: \u0026#34;Jim Cowart\u0026#34;, location: { city: { name: \u0026#34;Chattanooga\u0026#34;, population: 167674 }, state: { name: \u0026#34;Tennessee\u0026#34;, abbreviation: \u0026#34;TN\u0026#34;, population: 6403000 } }, company: \u0026#34;appendTo\u0026#34; }; //如果你希望缩进量为2 个空格, // 你可以这么干: JSON.stringify(person, null, 2); /* produces: \u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;Jim Cowart\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Chattanooga\u0026#34;, \u0026#34;population\u0026#34;: 167674 }, \u0026#34;state\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Tennessee\u0026#34;, \u0026#34;abbreviation\u0026#34;: \u0026#34;TN\u0026#34;, \u0026#34;population\u0026#34;: 6403000 } }, \u0026#34;company\u0026#34;: \u0026#34;appendTo\u0026#34; }\u0026#34; */ // 如果你希望使用 tab 缩进,那么 // 你可以传入 \\t 作为第三个参数 // 以此来告别空格缩进 JSON.stringify(person, null, \u0026#34;\\t\u0026#34;); /*输出结果: \u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;Jim Cowart\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Chattanooga\u0026#34;, \u0026#34;population\u0026#34;: 167674 }, \u0026#34;state\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Tennessee\u0026#34;, \u0026#34;abbreviation\u0026#34;: \u0026#34;TN\u0026#34;, \u0026#34;population\u0026#34;: 6403000 } }, \u0026#34;company\u0026#34;: \u0026#34;appendTo\u0026#34; }\u0026#34; */    那么-第二个参数呢?在上面的例子中简单地传了一个null.关于 \u0026ldquo;replacer\u0026rdquo; 参数-它可以是一个数组或者是一个函数.如果是一个数组,它将只输出你在该数组中所想要包含的keys.\n1 2 3 4 5 6 7 8 9  // 假定 person对象是上一例子中的那个, JSON.stringify(person, [\u0026#34;name\u0026#34;, \u0026#34;company\u0026#34;], 4); /* 输出结果: \u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;Jim Cowart\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;appendTo\u0026#34; }\u0026#34; */    如果这 \u0026ldquo;replacer\u0026rdquo; 参数传入的是一个函数,那么这个函数需要有两个参数:key 和 value :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // a bit contrived, but it shows what\u0026#39;s possible // FYI - 被序列化的值是第一个传给replacerFn的东西, 也即是这个对象的每一个key,因此需要检查key的真假. var replacerFn = function(key, value) { if(!key || key === \u0026#39;name\u0026#39; || key === \u0026#39;company\u0026#39;) { return value; } return; //返回 undefined 并忽略 } JSON.stringify(person, replacerFn, 4); /* produces: \u0026#34;{ \u0026#34;name\u0026#34;: \u0026#34;Jim Cowart\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;appendTo\u0026#34; }\u0026#34; */    你可以使用替换方法来处理得到 黑名单成员的姓名(与此相对应的是,使用白名单数组作为\u0026rdquo;replacer\u0026rdquo; 参数).我发现这种方法是有用的,尤其当我需要迅速并且有针对性的得到一个DOM元素的值,但我希望阻止一个序列化循环引用的异常(当我drank some antifreeze 或是试图做些荒谬的做法如JSON.stringify(document.body)的时候).Feel free to browse this fiddle for a simple example of using a replacer function to blacklist based on member name(s).\n当然,另一个得到自定义某个对象的JSON序列化的选择是在对象中使用\u0026rdquo;toJSON\u0026rdquo;方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  var person = { name: \u0026#34;Jim Cowart\u0026#34;, location: { city: { name: \u0026#34;Chattanooga\u0026#34;, population: 167674 }, state: { name: \u0026#34;Tennessee\u0026#34;, abbreviation: \u0026#34;TN\u0026#34;, population: 6403000 } }, company: \u0026#34;appendTo\u0026#34;, toJSON: function () { return { booyah : this.name + \u0026#39; , employer: \u0026#39; + this.company }; } }; // 上面的toJSON返回的值 // 实际上被字符串化的: JSON.stringify(person, null, 4); /* produces: \u0026#34;{ \u0026#34;booyah\u0026#34;: \u0026#34;Jim Cowart , employer: appendTo\u0026#34; }\u0026#34; */     \u0026ldquo;说真的,Jim.比起在控制台写更漂亮的JSON我们有更重要的事情要做.\u0026rdquo;\n You don\u0026rsquo;t have to convince me, I\u0026rsquo;m with you 110%. 但我会说,当你为了调试已发布的信息的可视性或者logging websocket payloads而 wire-tapping a message bus 的时候更好看的JSON格式化会让一切都不同.(囧 、o(╯□╰)o)能够通过白名单替换数组(或者黑名单替换函数)来有选择的序列化对象,可以很方便地在DOM元素/事件上帮助你跟踪问题,或者简单地让你在复杂的交互上有更好的可视化,without de-railing you with serialization exceptions.\n原文来自 Jim Cowart ,What You Might Not Know About JSON.stringify()\nXguoX - Actually,其实这个在Nicholas C.Zakas的《JavaScript高级程序设计》第20章关于JSON的介绍讲的也很详细.比我这蹩脚翻译要好得多.\n","permalink":"https://xguox.me/what-you-might-not-know-about-json-dot-stringify.html/","tags":["Translation","JavaScript"],"title":"你可能不知道的JSON.stingify()"},{"categories":["Jabber"],"contents":"回看2011的最后一篇,2012的那个元旦,上一次的夜游白云山看那看不到的日出,想来,我的这两元旦都\u0026rdquo;不平淡\u0026rdquo;呀.只是这次的不平淡不太妙. 老规矩,跟上次那般先回顾一下这个2012.首先,庆幸一下这个世界还没到走到末日；接着,热烈祝贺中共18大胜利召开\u0026hellip;(扯远了好伐)\n看上那篇写着的,其实2011那会也就那么点东西而已,没多疯狂的,比起2012的简直小巫见大巫.这其实,2012,也没什么疯狂的,学会蛋定地看事情. 简单来个大小事件回顾:厦门义工旅,瑞卡初实习,奔赴大上海,参与简书,各种进医院,升大四\u0026hellip;\n我常说自己最大的缺点不是XX性格 || XX知识之类的,而是我的身体状况弱爆了.似乎,以前遇到的所有困难都不算什么,这一次却狠狠地阻挠了我一番.以至于我的所有计划都随之暂停.不过我的心态也算是得到一场小锻炼吧.就像前面所说,蛋定地看事情.\n出600D入a77想来是个错误的决断,只怪那次东栅行太纠结折腾了以至于小冲动了.否则,差价上再存点小钱把linux的35L败过来也不错下.后来本是想入xpro1的(说实话现在也还想)奈何银子不甚充裕\u0026hellip;\u0026hellip;\n不幸地还有在这患病的节骨眼撞上了大学毕业季,学校各种蛋疼的事有,据说各个校园正开展着各种招聘会啥的,话说到现在我一个都没去过,简历也没份像样的,正装也没一件.想来还没正正规规的为自己的毕业工作筹划一番.换句话说是不是在大上海过得太安逸了叻,又还能安逸多久呢?\n大方向呢还是这么走着.还y大方向,说的很正经似的,说白了还不是写代码的.这一年,比较纠结的是linux方面没什么大进步,还是那么不入流.Rails有些进步但不甚明显,最近在医院换口味玩儿sinatra,这方面的中文资料那就杠杠的比Rails的还少.JS写了不少,不过也只是仗着jQuery的易上手.前端设计很大程度是在依赖Bootstrap.\n还有不足的就是阅读的不够多,包括书,和RSS.尤其RSS,也就订阅那么几个,好经常就爆了.\n前面说的,蛋定地看事情.其实自己也不确定是否做到了,初初住院几天我甚至怀疑自己有间歇性的狂燥症.\n曾经有某一天的某一刻,我脑海中忽然疑惑是否有\u0026rdquo;带\u0026rdquo;这个字,以至于我当时还不自觉的就在虚空中比划着.这些天恰巧在看KK的《失控》,其中,关于black patch psychosis(黑盲性精神错乱)的描述让我感觉自己搭上了(当然,其实我当时的条件并没有书中的实验来的残酷).据传,一些人在极度无聊的时候出现了诡异的幻觉.没有互联网的世界的确失色很多,至少对我来说.\n从昨晚到现在,好多人纷纷改签名发微博2012怎么滴2013怎么滴(也是今天1月1扣月租才上看到的,手机老早没流量了).\n2013,想有一些改变.首要的,当然是身体好起来.其次,希望自己的文字功夫有所提高,一直很欣赏那些写出语言平时而却寓意深远蕴味深刻的作家,虽然我也写了不少,但都是些流水帐,如果哪一天我不写代码,我希望自己会是摄影师\u0026amp;作家,最平凡的文字最平凡的视角去刻画这一切.最后,Javascript至少要能实在地说自己熟悉.\n","permalink":"https://xguox.me/2013-new-year.html/","tags":["Jabber"],"title":"2013 new year"},{"categories":["Jabber"],"contents":"前天,新装了win8,还是因为技术各种不到位,结果原本的ubuntu活生生的被干掉了.于是乎,我就总安慰自己,哎,毕竟不是计算机专业的,是菜的啦.想来这是我第二次栽在了系统安装.不过这次ubuntu的丢失远不及上一次来的壮烈.目测这次最大的损失也就Octopress里的一些post.因为后来写的好几篇都没有push到github,于是,只好用最2的方式去还原了.\n也因此,才发觉距离写上一篇的时间都有两个多月了.以往一头半个月也能写上一两篇的.只是,这两个月也过得太\u0026hellip;(压抑?好像不是.抑郁?也不算吧.总而言之,就是糟糕吧)因为这糟糕的身子.直到现在,看着自己不过百斤的体重,瘦削的身板.另一方面,简书正值冲刺时刻.再一方面,临近期末,临近毕业\u0026hellip;总之,一切因为这场突如其来的恶疾而似乎变得糟糕了. 最大的可惜应该要数没能去RubyConf.门票开始发售的第一天就果断买了门票,接着,坐等11月17.接着\u0026hellip; 再看着上一篇,还想着怎么怎么地计划,结果都变成了浮云\u0026hellip;\n尽管这么一切,但是近些天却也relax的一些. 似乎是因为去尝试了一些新东西,比如买了几本新书,当然了,不是技术类的.又或者,在学javascript,好吧,其实JS对我包括很多人一定不是什么新东西,但是,对我包括很多人,个中的一些、很多知识一定还是新鲜的.另外Larry说的,看一看Spine.js,还是很有兴趣的.还有Sinatra呀. 甚至乎,如这换了个主题,看着多了份舒适.(想来是不是之前那个配色有点不着调了) 尝试一些新鲜的事物,总会把一些喜悦感激发出来的.\nAnyway,最近,有句话在我脑海闪过很多次. 如果若干年后事情对你已经不重要,现在何必为其不堪而难过.So,我回来写博客了.看着页面多了一篇新的post,舒服不少,尽管一如既往的流水.\n As The last ship sailed towards the distant horizon I sat there watching on a rock My mind slowly drifting away Forming into my… Dreamtale\n ","permalink":"https://xguox.me/positive-energy.html/","tags":["Jabber"],"title":"Positive Energy"},{"categories":["Jabber"],"contents":"101假期临近了,前不久,Larry刚去了张家界回来,Linux也准备去青海,好是羡慕呀.时间再往前推一些,班级刚组织了一趟毕业旅游,是去阳朔的(小吐槽下,Y的身边的朋友的毕业旅游居然80%都是去阳朔漓江的,小数一下至少上10个不同的班级.那边有这么的霸气么?),因为在上海这边不方便回去了,我也就错过了. 早之前,朋友们讨论过关于这个大学或者夸张点的说人生最后个暑假是该去找找实习工作还是好好地疯玩个够.哎,也没什么好刻意去选择的.正经事能干,不代表就一定要牺牲掉休闲的时光的撒,漫长两个月,还能做除了这两样之外更多的事啦!\n关于旅行,之前,没什么钱在身可以去玩玩穷游,也小转了个厦门、香港、澳门的,好吧,如果上海也算的话,就加一个吧,其实也不用花费很多,要没时间的,逃课也没啥,现在总不能旷工去了.当然我知道请个一天半天加上个周末去relax一下老大们会给的,不过现在项目都还没正式上线,木有稳定下来,还是得放多点心再说吧. 不过上海有个不错的地方,以上海为中心幅射出去的能玩的地方还是很多的,都比较近,基本周末可以搞定.就好比上次跟小泡他们一起乌镇那样.\n暂定给了自己一个小目标,尽快把海岸线的省份都去一趟.上海以南的都去了,接着就是往北了,江苏离上海近,找个周末基本没什么问题.山东的话,要不改天跟大双肥去也行.哇咔咔… 等海岸线游遍过后往内陆推进,咔咔咔咔咔….亲,YY够了!\n","permalink":"https://xguox.me/recently-feeling.html/","tags":["Jabber"],"title":"最近有感,关于玩"},{"categories":["Jabber"],"contents":"简书前身: Maleskine\n首先,我木有想到的是linux居然比我还早写了一篇Maleskine相关的,不过我应该要猜到的.不过我写的可能与他写的不太相同(废话,相同的话你还写!!!).他写的更多是Maleskine的相关起源历史,而我要写的则是对Maleskine的概述,以及开发过程的感想.\n据不科学统计,Maleskine应该是我作为一名Coder或者说准Coder所开发的第一个产品.我应该庆幸当初我所接收的任务不是我们的另外一个产品Instatrip.虽然我对照片很感兴趣,但是对于像我这样从未用过、短期内也未必会使用到Instagram的人来说,那不是个好选择.相反,尽管平时写东西的积极性不高,但也隔三差五的会在这写点东西,而且Octopress本身也是直接使用Markdown作为写作模板(这有两毛钱关系吗?).\n另外,取这个标题的灵感,来源即将到来的MOP.玻璃渣每发布一个新的资料片都会有一段前夕用以过渡.比如风暴前夕,冰封前夕,灾变前夕… 好吧,关于wow咱暂时不在这扯多了.\n既然linux也发文吊大家胃口,那我就来解解吧. \u0026gt; 这是一个约两人月的产品. 这是一个还在beta版的产品. 这是一个使我的CSS从0到1的产品. 这是一个使我的Javascript从0到1的产品. 这是一个我远离家乡实习上海所写的产品. 这是一个我还木有掌握ajax就使用到了pjax的产品. 最重要的,这是一个我自己也经常需要使用的产品.\n(￣ε(#￣)表喷我呀,这些绝对都是真的好伐=.= 在这以前,我可是从未写过CSS , Javascript的,顶多就翻翻文档,copy别人的有个认识了解罢了.记得第一次参加上海RubyTuesday时候Daniel 说他现在不管是写什么项目,几乎花的最多的时间还是在Javascript上.其实,不单只像他那样的高玩是这样,反正我自己开发这个项目下来感觉花的更多的时间是在Javascript而不是Ruby/Rails.关于js,又不得不提到coffee,打自我进来没多久,larry就倡导+要求我们用coffee写js,相比较过后我也更倾向coffee的语法.但是,惭愧的是一直都没学,原因嘛…就不为自己找那么多借口啦!咔咔,B的larry很蛋疼的把所有JS改用coffee,现在我可就不管叻!得学啦!!! 另外,要知道,对于Web开发者来说,浏览器是不亚于用户的一个存在!我也总算是体会到了调各种各样的浏览器的蛋疼了.尤其是IE,尤其是IE9- .最近发现firefox上也来问题.我怎么觉得chrome各种蛋定的…\n一路过来,问题肯定是遇上不少的啦.但我自己印象较深的还属一个:命名.这是一件有学问,很重要,伤脑筋的事情.我记得我来到上海就分别从两个人口中听到这么一句:\n There are only two hard things in Computer Science: cache invalidation and naming things.\n 缓存无效化表示没什么概念,但是命名却体会上了.尽管这只是一个小项目,但毫无疑问,命名的好坏很大程度的决定代码的可读性,从而决定了开发的效率.好多次,我自己写的一些方法、变量或者CSS类等等自己都忘记它在哪定义的,要干什么的.当其他人问起时,更是各种囧!!!\n关于Maleskine,我想到一句比较那个的,\u0026ldquo;Write your word,I will handle the other things!\u0026rdquo; 好吧,我的确正在努力handle着other things 关于记录东西,笔记本,记事本什么的,很多人会想起evernote.我自己也是.其实evernote也是个杠杠的产品.很可惜的是它不支持markdown,并且我感觉evernote功能灰常丰富,但是却很多华丽却不实用,又或者说是复杂晦涩.当然啦,我并不是要踩evernote.只是它并不是我想要的.(写在后话:本文写的时间断断续续,octopress的一大好处,写了也不急着部署发布上去.某天无意中看到linux的写的那篇default note,发现关于evernote的看法还挺相似的)\n这段时间下来,我感觉更多的是在执行开发任务,各种todo,接下来,我自然希望更多的是加入自己的想法、元素进去.当然也希望各位使用或喜欢的朋友提出你们宝贵的意见与建议ლ◕ิ‿◕ิლ (怎么感觉这句话好官方呀)\n","permalink":"https://xguox.me/writing-before-the-maleskine-release.html/","tags":["Jabber"],"title":"写在 Maleskine 上线前夕"},{"categories":["Jabber"],"contents":"每一个礼拜都没带着什么意识就来到了周五、周末,接着又没带什么意识的进入工作日.\n这个周末有点不同的是煽情的神经又崩了.因为毫无意识地在网上很蛋疼的重温了一部TVB的剧.从小看着TVB的剧长大,忘了从什么时候开始,渐渐地就少看电视了,但是,居然还记得不少里头的老剧情还有很多扣人心弦的插曲.现在,情愿去看那些剧不断的炒冷饭也懒得看现在的那些更蛋碎的剧.为免继续看着煽情,搞定一部后就没继续煲其他的.\n上次他们问我周末都去哪,我还真就不好意思说我宅着了.我不知道没有单反在手是不是我不出去的真是原因还是懒的借口.\n不过,休息好,明天跑步!\n","permalink":"https://xguox.me/the-past.html/","tags":["Jabber"],"title":"....sickness"},{"categories":["Jabber"],"contents":"还差个几天,来到上海就一个月了.又想写写东西了.\n昨天大炮他们刚走,也来了将近10天吧.也多得他们的到来我才真正意义的走出小区到外边走走,期间还包括周末的乌镇行.他们在这的几天没少为我煲粥煲汤的,只是苦了大双,那么小个房间…反正他们在的这些天我的说话量说话语速各种高.\n他们回去了,我的600D也转手卖了.接下来就是要宅的了.说起600D,还得纪念它从11年11月11日陪我到现在的日子啊.这两次的明珠塔和西栅估计成了它的\u0026rdquo;绝唱\u0026rdquo;了.说到西栅,又不得不说说我那天的东栅悲剧.整一个东栅,毫不夸张的说简直就是在随着人流不断地走啊走,什么人文风景的,在那样的人流量,那样的酷热天下都是蛋疼.反正就是次印象不好的出行,我一次相机都没抬.放眼望去,各种红圈长炮在那扫摄.真的,10个有6到7个都是拿着单反,还有2~3个拿着卡片机手机的,最后还有个,好吧,不是麻豆,是导游!不得不说,我佩服当时在那专心摄影的人,佩服他们的毅力,杠杠的.也还好西栅挽回了些乌镇的面子吧.\n其实我也还木有完全的驾驭好600D,尽管它不是什么高端机器.尤其感觉测光方面我还差远了.尽管600D算是较小的单反,但是也根本谈不上轻便,尽管我努力让它轻便.so,我的下个目标是要轻便到我日常生活几乎可以随时抬机的.要是\u0026rdquo;Linux\u0026rdquo;看到一定又捣鼓我买iPhone了.\u0026ldquo;Linux\u0026rdquo;一直那个 我还有大双 买iPhone.卖了单反后手上有点闲钱,但我还是不想换手机,尽管,貌似现在对我来说手机是最需要更换的.其实我也不知道为什么.在我的未来败家清单中,手机排在很靠后的位置.\n没了单反,也就不怎么想去ChinaJoy了.这两天貌似很火热呀,我也很想去看看玻璃渣什么个情况.这又说到玻璃渣,据说MOP 9月25上市,只是不知道国服什么情况.WoW在我心目中的地位还是不低的.\n最后,不得不说的就是工作了.\n每天一如既往的学习,进步,自由.Rails方面进步较为缓慢,更多的是学习到了许多前端的知识,CSS和JavaScript.说起来JS也因为他们俩的到来停滞了看书,也长进的不明显.\n七月快完了,接着就是八月了.\n晚了,脑袋不好使了,还是睡觉算了.\n","permalink":"https://xguox.me/about-a-month.html/","tags":["Jabber"],"title":"About a Month"},{"categories":["Jabber"],"contents":"来到上海两周多些,好事是有不少,但坎坷的也是不少.前几天纠结了好一些时间转房子的事情,刚刚解脱了,而今又胃病再次来袭,还花了好大一笔.家里人各种担心,都纷纷劝我回去.\n我是坚持继续留下来了.不过接下来的时间得更加倍照顾自己才是了.我可不想这都还没真真正正的进入到团队就要打退堂鼓.幸好,这胃对工作影响不大,就是饮食得注意再注意.\n今天大泡和某鸡也远飘来这边,呃,好吧,他们俩是旨在旅行顺路探望探望我,也顺便解解我的寂寞.我都很久没有这么流利的跟别人说话交流了. 原本是想利用周末把一些零散的知识整理整理,貌似又没这时间了.\n","permalink":"https://xguox.me/an-unusual-trip.html/","tags":["Jabber"],"title":"不平坦的魔都路"},{"categories":["Translation","Ruby"],"contents":" 自从写了Rails 3 Remote Links \u0026amp; Forms Definitive Guide有一个问题也接踵而至:\n我们该如何使远程链接或者表单取回 js.erb而不是一个html的partial? 在上一篇中,我们请求一个html partial并通过ajax回调插入到页面中.但如果我们想要javascript被执行?或者XML、JSON被解析?又或者显示出纯文本呢?\nEqual parts Rails \u0026amp; jQuery 首先,我们需要明白Rails3中 :remote =\u0026gt; true 的过程. It\u0026rsquo;s equal parts Rails and jQuery magic.但表担心,这只是一个小魔术,捆绑成4个步骤的过程: 1. Rails告诉jQuery,\u0026ldquo;哥们,把你的ajax绑定到这霸气的表单/链接\u0026rdquo;,通过 :remote =\u0026gt; true 这个选项. 2. jQuery劫持?元素的click/submit事件并绑定到.ajax()方法.现在,元素提交通过使用ajax取代在浏览器中加载一个新页面. 3. 当这个元素被click/submit时,Rails接收到ajax网页请求并响应以一些内容. 4. jQuery接收到响应的内容有.ajax()方法hi-jacked在我们的的元素中,并为我们提供回调用以在页面中处理这个响应.\n在这篇文章中,我们正在探索各种各样的、使我们能够指定ajax的响应格式并据此做出处理的方式,这实际上涵盖上述所有4个步骤.\n步骤 1 \u0026amp; 2:设置data-type 当浏览器发送web请求,在浏览器到服务器,服务器返回浏览器的过程中,部分 请求/响应 的header能够指定内容的格式.当在浏览器中加载一个页面,内容的类型通常从url的拓展可以推断得出.尽管如此,jQuery能够直接在ajax请求的header部分设置我们所想要的data-type.\njQuery允许dataType参数 jQuery的.ajax()方法提供一个可选的dataType参数用以指定我们所想要的响应data-type.这允许jQuery在请求的HTTP Accept header指定格式类型,然后封装响应的内容到适当的数据对象使得更容易操作.\n 在jQuery1.4中,如果你不指定响应的data-type,jQuery会检查响应的MIME类型头并智能的猜测其为data-type,动态的转换响应对象的data-type\n 在.ajax()文档页面有更多的信息,但基本的类型有如下:\n  dataType Behavior   \"xml\" 返回一个能呗jQuery处理的XML文档   \"html\" 返回纯文本的html,调用标记中的所有\u0026lt; script\u0026gt;标签   \"script\" Evaluates the response as JavaScript并以纯文本格式返回   \"json\" Evaluates the response as JSON 并返回一个javascript对象   \"jsonp\" 在一个JSON块中使用JSONP加载响应   \"text\" 返回一个纯文本字符串   Rails.js设置dataType参数 Rails UJS驱动通过我们在远程链接/表单中指定的data-type属性来设置我们的ajax dataType参数.如果我们不明确地指定data-type,那默认的data-type将使用公有的$.ajaxSettings.如果我们没有提前设置它,那么一个通用的请求发送出后将会收到任意类型的响应.\n1 2  dataType = element.attr(\u0026#39;data-type\u0026#39;) || ($.ajaxSettings \u0026amp;\u0026amp; $.ajaxSettings.dataType);    老版本的UJS驱动会默认一个在script 中的data-type,而不是发送一个通用的请求.看上去,这像是一个明智的做法,但如果在我们controller的action中没有定义format.js的话, Rails将会抛出一个异常. 而在较新版本的UJS驱动中则简单地使用jQuery的\u0026rsquo;/\u0026lsquo;默认dataType.这会告诉服务器,\u0026ldquo;不管你收到什么都给我\u0026rdquo;.然而,这会让controller响应以在Responder中所列出的第一种格式(见下一节).这样的话,如果 format.html 列在 format.js 前面的话,app会响应以HTML格式(这意味着将会尝试跳转到POST或DELETE方法的ajax请求).这并不是我们想要的.\n所以在最新版本的UJS驱动中,我们找到如何设置默认情况,比如说,当它告诉服务器,\u0026ldquo;尽管我更希望得到JS格式,但我会得到所有你得到的.\u0026ldquo;这样,如果所有可用的响应格式包括format.js都被定义,那么返回的将会是JS格式.当然,如果format.js没有被定义,那controller将会继续按照列表顺序返回第一个.\n1 2 3 4  // Simplified for clarityif (dataType === undefined) {  xhr.setRequestHeader(\u0026#39;accept\u0026#39;, \u0026#39;*/*;q=0.5, text/javascript\u0026#39;); }    步骤 3 :在Rails的controller中的响应 现在我们已经有了一个ajax请求,同时,在该请求的header指定我们希望得到的data-type.我们的Rails app接收到这个请求,就会路由到相应的action,并渲染出一个响应.\n我们的controller决定渲染些什么内容作为响应,以及是什么样的格式.respond_with 和 respond_to 这两个在Rails的Responder类中的方法会检查请求的 Accept 头(通过dataType设置)以渲染出适当的响应.\n对于基于对象的data-types,如XML和JSON,我们通常会对其序列化并返回一个指定格式的对象.这就是Responder默认所做的.而对于给予内容的data-types,如JS或html,我们通常会渲染一个 js.erb 或 html.erb 文件.\n 技术上,我们也能有一个自定义的 json.erb 或 xml.erb模板来渲染一个自定义的数据对象.Responder会查找这些模板,如果存在则渲染之.\n 步骤 4 : jQuery处理响应 从上一篇文章可知,我们绑定了我们处理响应的jQuery代码在ajax:success,ajax:error,ajax:complete这些回调中.\n现在,我们对于各种不同的data-types及如何设置它们有了新的认识,我们能够修改我们的响应回调来处理我们所想要的data-type.在上一篇文章中,我们的回调绑定处理了一个HTML响应.\n处理HTML响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  $(\u0026#39;#i-want-html\u0026#39;) .bind(\u0026#39;ajax:success\u0026#39;, function(evt, data, status, xhr){ var $this = $(this); // Append response HTML (i.e. the comment partial or helper)  $(\u0026#39;#comments\u0026#39;).append(xhr.responseText); // Clear out the form so it can be used again  $this.find(\u0026#39;input:text,textarea\u0026#39;).val(\u0026#39;\u0026#39;); // Clear out the errors from previous attempts  $this.find(\u0026#39;.errors\u0026#39;).empty(); }) .bind(\u0026#39;ajax:error\u0026#39;, function(evt, xhr, status, error){ // Display the errors (i.e. an error partial or helper)  $(this).find(\u0026#39;.errors\u0026#39;).html(xhr.responseText); });    同样的,我们能很轻易的修改上面的代码来处理JSON响应:\n处理JSON请求:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $(\u0026#39;#i-want-json\u0026#39;) .bind(\u0026#39;ajax:success\u0026#39;, function(evt, data, status, xhr){ var $this = $(this); // do something with \u0026#39;data\u0026#39; response object  $this.find(\u0026#39;input:text,textarea\u0026#39;).val(\u0026#39;\u0026#39;); $this.find(\u0026#39;.errors\u0026#39;).empty(); }) .bind(\u0026#39;ajax:error\u0026#39;, function(evt, xhr, status, error){ var responseObject = $.parseJSON(xhr.responseText), errors = $(\u0026#39;\u0026lt;ul /\u0026gt;\u0026#39;); $.each(responseObject, function(){ errors.append(\u0026#39;\u0026lt;li\u0026gt;\u0026#39; + this + \u0026#39;\u0026lt;/li\u0026gt;\u0026#39;); }) $(this).find(\u0026#39;.errors\u0026#39;).html(errors); });    最后,如果是要处理一个JS请求,则我们不需要绑定任何的回调函数.记住,如果是 data-type: \u0026lsquo;script\u0026rsquo; ,jQuery会自动执行页面中的JavaScript响应.\n处理JS请求:\n1  //Nothing!   如果 我们的响应没有自动为我们执行,那我们的处理方式可能是像下面这样:\n1 2 3  $(\u0026#39;#i-want-js\u0026#39;).bind(\u0026#39;ajax:complete\u0026#39;, function(evt, xhr, status){ eval(xhr.responseText); });    注意,我们已经绑定了回调 ajax:complete 来取代ajax:success|error.这是因为,我们的success/error处理都在js.erb文件.\n 上面的处理方式并不非完全和自动执行函数一致.上述的函数会在 $(\u0026lsquo;#i-want-js\u0026rsquo;) 元素的上下文执行我们的脚本,同时,jQuery在 $(window) 上下文自动执行我们的脚本.\n 现在,让我们来个总结\n使用js.erb的例子 就像上一篇文章,我们会创建一个comment并通过ajax提交.所不同的是,这次我们会响应以 js.erb\n在controller中,我们会添加一些响应html、hs、json请求的功能.\ncomments_controller.rb\n1 2 3 4 5 6 7 8 9  class TestCommentsController \u0026lt; ApplicationController respond_to :html, :js ... def create @comment = Comment.new( params[:comment] ) flash[:notice] = \u0026#34;Comment successfully created\u0026#34; if @comment.save respond_with( @comment, :layout =\u0026gt; !request.xhr? ) end end   如果你对Rails3的Responder不熟悉,下面的代码效果是一样的:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class TestCommentsController \u0026lt; ApplicationController ... def create @comment = Comment.new( params[:comment] ) respond_to do |format| if @comment.save flash[:notice] = \u0026#34;Comment successfully created\u0026#34; format.html { redirect_to(@comment), :layout =\u0026gt; !request.xhr? } format.js { render :js =\u0026gt; @comment, :status =\u0026gt; :created, :location =\u0026gt; @comment, :layout =\u0026gt; !request.xhr? } else format.html { render :action =\u0026gt; \u0026#34;new\u0026#34;, :layout =\u0026gt; !request.xhr? } format.js { :js =\u0026gt; @comment.errors, :status =\u0026gt; :unprocessable_entity } end end end   现在,我们来设定index页面来列出所有的comments,以及包括我们的ajax表单用以创建一个新comment.我们不需要在我们的表单中写任何data-type这鸽HTML5属性,Rails UJS会默认我们需要JS\nindex.html.erb\n1 2 3 4 5 6  \u0026lt;div id=\u0026#34;comments\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;%= form_for :comment, :remote =\u0026gt; true, :html =\u0026gt; { :id =\u0026gt; \u0026#39;new-comment-form\u0026#39; } do |f| %\u0026gt; \u0026lt;%= f.text_area(:body) %\u0026gt; \u0026lt;div class=\u0026#34;errors\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;%= f.submit %\u0026gt; \u0026lt;% end %\u0026gt;   最后,我们需要创建 js.erb模板,该模板会被执行并插入响应到我们的页面中.\ncreate.js.erb\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  var el = $(\u0026#39;#new-comment-form\u0026#39;); \u0026lt;% if @comment.errors.any? %\u0026gt; // Create a list of errors  var errors = $(\u0026#39;\u0026lt;ul /\u0026gt;\u0026#39;); \u0026lt;% @comment.errors.full_messages.each do |error| %\u0026gt; errors.append(\u0026#39;\u0026lt;li\u0026gt;\u0026lt;%= escape_javascript( error ) %\u0026gt;\u0026lt;/li\u0026gt;\u0026#39;); \u0026lt;% end %\u0026gt; // Display errors on form  el.find(\u0026#39;.errors\u0026#39;).html(errors); \u0026lt;% else %\u0026gt; // We could also render a partial to display the comment here  $(\u0026#39;#comments\u0026#39;).append(\u0026#34;\u0026lt;%= escape_javascript( simple_format( @comment.body ) ) %\u0026gt;\u0026#34;); // Clear form  el.find(\u0026#39;input:text,textarea\u0026#39;).val(\u0026#39;\u0026#39;); el.find(\u0026#39;.errors\u0026#39;).empty(); \u0026lt;% end %\u0026gt;    写在最后! 重申一次,这里我们不需要绑定任何ajax回调.It just works.\n 有时候,javascript响应不会像它本应该的那样执行,取而代之的是作为一个字符串返回.这通常意味着有在这个响应的javascript中有一些异常.这是很烦人的,但它不会从自动执行响应中抛出任何可见的javascript错误.\n See Rails js.erb Remote Response not Executing.\n","permalink":"https://xguox.me/rails-3-remote-links-and-forms-part-2-data-type-with-jquery.html/","tags":["Translation","Ruby"],"title":"Rails 3 Remote Links and Forms Part 2: Data-type (With jQuery)"},{"categories":["Translation","Ruby"],"contents":" blahblah:  这些天遇上一些困难请教Larry的时候他没少给我一些英文的资料,虽然能够阅读,但是始终觉得翻译回中文以后再看也方便些.不过有些地方翻的比较蹩脚呀.\n原文:Rails 3 Remote Links and Forms: A Definitive Guide\n Spoiler Alert:如果你喜欢魔法,请停止阅读这篇文章.因为,让Rails的表单、连接、输入等如此强大的Rails 3 UJS驱动—-Rails.js,当你知道它的工作原理后,你就知道其实它没有多么的神奇.\nRails.js做什么  它找到远程链接、表单和输入,并通过Ajax覆盖这些的click事件以提交到服务器端. 它触发6种javascript事件来让你绑定一些回调函数用以处理和操纵这个Ajax响应 它处理从服务器端得到的Ajax回应  Rails.js不做什么 请注意到上述最后一点是加了删除线的.当启用新的Rails3远程功能时候,这似乎是混乱的最大来源.因为已经根深蒂固的习惯于Rails2的 link_to_remote 与 remote_form_for,我们期待Rails3也同样的会为我们的表单、链接处理Ajax响应.但是Rails3却没有这么做,它把这些都交给你去完成. Rails会做的是帮你把包裹搬上房间,但不会帮你打开包裹.This is by design;为什么你想要侍从帮你使用你的东西呢?同样的,每个元素的数据实际处理方式很大程度上是独一无二的,并且取决于你再各种情况下工作所得的的数据.所以,Rails把这一部分留给你来做决定,而我们则回到了过去.\nHTML5的角色 你可能已经听说Rails3将使用HTML5.的确是的.但HTML5所扮演的实际角色并没有你想象中的那么重要. 是否还记得Rails.js所做的第一样东西.在它能覆盖所有远程元素的click事件和submit事件前,它需要先找到这些所有的元素. 因此,我们需要一种方法来指定我们的远程元素.在以前,我们可能会添加 class=\u0026ldquo;remote\u0026rdquo; 到我们的远程元素.但现在,所有的class属性都跟样式相关联了.难道就没有一种办法能区分出远程元素而不用污染到我们的CSS类. 这,在HTML5中是有的.有这么一段HTML5规范:现在,你可以添加任意标签到一个HTML元素,只要这个标签的起始部分是\u0026rdquo;data-\u0026ldquo;.因此,下面这是一个有效的HTML标记:\n1  \u0026lt;a hre=\u0026#34;stuff.hml\u0026#34; data-rocketsocks=\u0026#34;whateva\u0026#34;]] \u0026gt; Blast off!\u0026lt;/a]] \u0026gt;   Rails 3充分利用了这种新的HTML标记,它通过在HTML元素中添加 :remote =\u0026gt; true 来转换所有的链接和表单使其拥有了标签 data-remote = true\n1  \u0026lt;%= link_to \u0026#34;Get remote sauce\u0026#34;, {:action =\u0026gt; \u0026#34;sauce\u0026#34;}, :remote =\u0026gt; true, :class =\u0026gt; \u0026#34;button-link\u0026#34; %\u0026gt;# =\u0026gt; \u0026lt;a href=\u0026#34;sauce\u0026#34; class=\u0026#34;button-link\u0026#34; data-remote=true\u0026gt;Get remote sauce\u0026lt;/a\u0026gt;   现在Rails.js通过如下的选择器找到所有的远程链接:\n1 2 3  $(\u0026#39;form[data-remote]\u0026#39;) $(\u0026#39;a[data-remote],input[data-remote]\u0026#39;    并且分别通过一个ajax请求到form的action和link的href属性从而重载了submit和click动作, Then it does the same thing wirh remote forms and inputs, using the form\u0026rsquo;s action. 就是这样,HTML5只是提供一个方便、语法、方式给我们用以指定及选择劫持哪个元素(???)\n处理Ajax响应 那么,我们到底是如何根据ajax响应做一些事情呢?幸运的是,当Rails.js远程发送你的请求时,它同时也触发了6个自定义事件,并传输相应的数据/响应到每一个事件.你也可以绑定你自己的处理函数到这些事件. 这6个事件是:\n1 2 3 4 5 6  ajax:before ajax:loading ajax:success ajax:failure ajax:complete ajax:after   Update:  在这篇文章写下后,Rails团队对 jQuery UJS driver 做了些改动.在最新版本的Rails.js中,只剩下了以下4个回调事件:\n 1 2 3 4  ajax:beforeSend ajax:success ajax:complete ajax:error   这样,在你的页面中,你将会像下面这样绑定这些事件到一个函数中:\n1 2 3  $(\u0026#39;.button-lnk\u0026#39;).bind(\u0026#39;ajax:success\u0026#39;, function(){ alert(\u0026#34;Success!\u0026#34;); });    你会发现,通过绑定一些函数到实体/事件,在Rails中的所有的javascript功能都是很方便的.\nBinding is good,因为它不是引人注目的.javascript功能和html标记分别作为完全独立单位但通过javascript绑定在一起的存在.如果用户没有用到javascript,那么javascript所绑定到html实体的js函数将永远不会被执行,用户所得到的,只是干干净净、有效的HTML\n把他们都放在一起 解释的足够多了,下面让我们建立一个加载一些内容到页面的远程表单.我们会提供给用户即时反馈,完整地处理所有的错误,以及能让用户重置表单再做一次.有了以上知识的武装,希望下面的没有人会看起来以为是魔法. 接下来,我们将假定在我们的Rails app中,有一个Comment model,且该model在数据库中有一个text类型的\u0026rdquo;content\u0026rdquo;字段.通过提交一个远程表单,我们能让用户创建一个comment,并将这个comment插入到页面中.\n 你可能注意到,我们直接请求了一个html的响应,但却以JSON的形式返回一些错误.我们使用 respond_with 覆盖了许多魔术方法.这一切说明我们在请求/响应中有多大的控制权.\n接着的Part2将通过一个更production-appropriate的例子叙述了如何请求一个JS(xml,json,文本或其他)的响应.\n View 1 2 3 4 5 6  \u0026lt;%= form_for @comment, :remote =\u0026gt; true, :html =\u0026gt; { :\u0026#39;data-type\u0026#39; =\u0026gt; \u0026#39;html\u0026#39;, :id =\u0026gt; \u0026#39;create_comment_form\u0026#39; } do |f| %\u0026gt; \u0026lt;%= f.text_area(:content) %\u0026gt; \u0026lt;div class=\u0026#34;validation-error\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;%= f.submit %\u0026gt; \u0026lt;% end %\u0026gt; \u0026lt;div id=\u0026#34;comments\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;   Controller 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  respond_to :html, :xml, :json... def create @comment = Comment.new( params[:comment] ) if @comment.save respond_with do |format| format.html do if request.xhr? render :partial =\u0026gt; \u0026#34;comments/show\u0026#34;, :locals =\u0026gt; { :comment =\u0026gt; @comment }, :layout =\u0026gt; false, :status =\u0026gt; :created else redirect_to @comment end end end else respond_with do |format| format.html do if request.xhr? render :json =\u0026gt; @comment.errors, :status =\u0026gt; :unprocessable_entity else render :action =\u0026gt; :new, :status =\u0026gt; :unprocessable_entity end end end end end   显而易见的,我们需要一个显示comment的partial模板 _show.html.erb .希望你知道如何做到.\n在这一点上,我们有一个提交到远程服务器上的表单,并且我们的服务器响应以我们在view中的partial,并准备插入到页面中去.但有一点是,它并不是真正的被插入到页面中.\nJavascript 是时候绑定一些处理函数到那些触发的ajax事件.因此,在我们的表单页面中,我们将希望加载这个javascript(可能会在标签或者在一个分离开的js文件)\n Update:\n下面这段已经更新以绑定到最新版本的Rails.js所允许的回调.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  $(document).ready(function(){ $(\u0026#39;#create_comment_form\u0026#39;) .bind(\u0026#34;ajax:beforeSend\u0026#34;, function(evt, xhr, settings){ var $submitButton = $(this).find(\u0026#39;input[name=\u0026#34;commit\u0026#34;]\u0026#39;); // Update the text of the submit button to let the user know stuff is happening.  // But first, store the original text of the submit button, so it can be restored when the request is finished.  $submitButton.data( \u0026#39;origText\u0026#39;, $(this).text() ); $submitButton.text( \u0026#34;Submitting...\u0026#34; ); }) .bind(\u0026#34;ajax:success\u0026#34;, function(evt, data, status, xhr){ var $form = $(this); // Reset fields and any validation errors, so form can be used again, but leave hidden_field values intact.  $form.find(\u0026#39;textarea,input[type=\u0026#34;text\u0026#34;],input[type=\u0026#34;file\u0026#34;]\u0026#39;).val(\u0026#34;\u0026#34;); $form.find(\u0026#39;div.validation-error\u0026#39;).empty(); // Insert response partial into page below the form.  $(\u0026#39;#comments\u0026#39;).append(xhr.responseText); }) .bind(\u0026#39;ajax:complete\u0026#39;, function(evt, xhr, status){ var $submitButton = $(this).find(\u0026#39;input[name=\u0026#34;commit\u0026#34;]\u0026#39;); // Restore the original submit button text  $submitButton.text( $(this).data(\u0026#39;origText\u0026#39;) ); }) .bind(\u0026#34;ajax:error\u0026#34;, function(evt, xhr, status, error){ var $form = $(this), errors, errorText; try { // Populate errorText with the comment errors  errors = $.parseJSON(xhr.responseText); } catch(err) { // If the responseText is not valid JSON (like if a 500 exception was thrown), populate errors with a generic error message.  errors = {message: \u0026#34;Please reload the page and try again\u0026#34;}; } // Build an unordered list from the list of errors  errorText = \u0026#34;There were errors with the submission: \\n\u0026lt;ul\u0026gt;\u0026#34;; for ( error in errors ) { errorText += \u0026#34;\u0026lt;li\u0026gt;\u0026#34; + error + \u0026#39;: \u0026#39; + errors[error] + \u0026#34;\u0026lt;/li\u0026gt; \u0026#34;; } errorText += \u0026#34;\u0026lt;/ul\u0026gt;\u0026#34;; // Insert error list into form  $form.find(\u0026#39;div.validation-error\u0026#39;).html(errorText); }); });     这些函数每一个都能轻易地抽象化,这样他们能轻易地或者自动地应用在我们所有的远程表单中.\n 这些绑定也以同样的方式工作于远程链接中.但我想我们已经使用了一个远程表单在这个例子中,由于这需要一些额外的但没有明显区别的步骤(例如清除表单或增添验证错误)\n同时,需要注意到:从ajax事件所传输到你的函数的数据并不是在同一个顺序.在ajax:success中是 evt,data,status,xhr ,而在ajax:failure中是,eve,xhr,status,error.This will get ya every time.\n别急,还有更重要的 如果你从文章开始就做足功课,并且看过了Rails.js文件,你应该会注意到一些额外的细节.\n.live Rails.js实际上用 .live() 而不是直接绑定click事件到远程表单、链接、输入.这样,在上面的javascript中,你可以替换 .bind() 为 .live() 显得更文艺一些.\nUpdate:  如果要在你的ajax处理中使用 .live() ,请确定你所使用的jQuery版本\u0026gt;v1.4.4,因为早些的版本会在IE中出现一些问题.\n :confirm =\u0026gt; \u0026ldquo;R U sure? \u0026ldquo; 你可能还会注意到 Rails.js文件处理 :confirm =\u0026gt; \u0026ldquo;R U sure? \u0026ldquo;(当你点击或者提交一些东西的时候嘣的弹出一个小窗口并显示\u0026rdquo;R U sure\u0026rdquo;(你也可以只写 :confirm =\u0026gt; true 使用默认的信息)),就像远程功能,Rails 3简单地添加有效的标签 data-confirm=true 到你的HTML元素\n:disable_with =\u0026gt; \u0026ldquo;Submitting…\u0026rdquo; Rails 3给了我们另外的一个选项,:disable_with,这样当一个远程表单被提交的时候, we can give to form input elements to disable and re-label them.添加data-disable-with标签到这些输入中,使Rails.js能选择和绑定这个功能.\n想知道如何让远程表单或者链接工作于 js.erb(或者其他任何与之相关联的形式),请看这篇文章的Part 2\n","permalink":"https://xguox.me/rails-3-remote-links-and-forms.html/","tags":["Translation","Ruby"],"title":"Rails 3 Remote Links and Forms: A Definitive Guide"},{"categories":["Jabber"],"contents":"这周的主题正如我的标题那样,安顿生活,融入上海.\n经过一周的各种转折,算是安定下了生活,包括在食宿方面等等.不过在住宿方面,我也因为一些心急付出了些代价,来到这边后的一个小教训啊.哎,要不是新换到跟同僚一起住,上班又杠杠的近,所以也就放开心态接受了这个教训牺牲.不然要是跟之前那般一个人憋着我估计用不了些时间会蛋碎…\n好吧,在此警醒自己以后关于合同的东西还是再三再再三的看清楚,表再像这次被做了些文章了.幸好这次还能补救…\n新同僚啊,话说我从家里带来再加上新买的书一共就6~7本,原本以为还蛮多的.谁知道一看俺同僚,目测都上10本了吧,其中我们还不少重复的,如《Ruby元编程》《松本行弘的程序世界》.不过他的更多是技术类的,我的更多是IT的人文.互相啃完这些书也都能收获不少Y,lol.\n至于工作,看起来这第一项任务真心考验到我了.javascript得上心学习了.我还记得大一那个暑假左右就开始翻过一本javascript的书,不过不明原因地没坚持下去.后来,尽管我知道javascript在web开发很是重要,但我却依旧只是挂心上而没真正去学.这会是必须下工夫了.\n另外,Larry不断地向我灌输一点就是把东西先做出来,把重构真正地放在心上.其实,这一点也是我一直以来记心的原则.最简单的一个,在写controller的时候经常会把一些重复的抽取出来些filter,不过我一般都是再把需求实现出来了再做这个.不过其实感觉还是逻辑上要先明确.不然,写出了filter随时可能会被抛弃.\nand 希望下周尽量不要被这租房的蛋事给影响到,刚来到还得好好表现的撒.\n这一周还去了Ruby Tuesday,据说这次人不怎么多,可能是因为没有明确的主题关系吧.不过我此行只是想感受下上海的Ruby/Rails氛围.周末原本Daniel组织了个培训,只是还是那租房的事又没去了.\n最后有个小无语的.俺的口音好像真的是杠杠的啊.走到哪只要开口说一句稍微长些的话别人不说我的口音我自己都觉得不太对劲.于是拗着口也改变自己的发声,结果说的还各种别扭.这不单我说话口音问题,我发觉我还经常听不懂别人说什么.哇了去咯,居然还有这语言障碍了.\n第一周,体验了各种酸甜苦辣,上海还是杠杠的.\n","permalink":"https://xguox.me/the-first-week-in-shanghai.html/","tags":["Jabber"],"title":"安顿生活,融入上海"},{"categories":["Jabber"],"contents":"29号早到达上海…\n原本这应该是在刚到时候写的…\n或许很多人听说我要去上海的第一感觉是我去旅行.很可惜,这次比厦门那一次还来狠的.我是去工作.我不想用实习这个词.尽管我就只是个实习生.\n尽管魔都是大城市,但我也不是去什么大公司.而只是个小创业公司.有多小?我是公司的第三人,够小了吧.很多人包括我爸妈问我去的是什么样的公司,是什么样的公司能吸引我大老远的从广东漂过来.我也不知道该怎么去跟他们解释说明我的工作环境.看过《rework》更让我坚定,小公司,没什么不好的.办公地点,不是必须在高大华丽的写字楼,在家才是最舒适的.工作时间,不一定是锁死的朝九晚五.人员的多,并不代表效率的高.还有个小兴奋的则是公司给配的27\u0026rsquo; iMac,终于可以使用Mac OS了.\n身边的同学几乎都很非常的向往像腾讯、网易那样的大公司,好吧,我真没那样的渴望.当然啦,人家大公司也看不上我这等菜鸟.或许真的看OP过度了,真心喜欢氛围好的强力小团队.去\u0026rdquo;四皇\u0026rdquo;旗下当然也不错啦!还记得红发手下那个送信的就值9000 0000贝里.\n当我跟别人说月入4K+的时候,都纷纷觉得这不可能生活在魔都这样的地方.我知道魔都不是那么好混,也有担忧过.我只知道,告诉我的大都是没真正在那生活过的,或许他们都当我去旅游来看,只有我真正尝试过才知道.\n不过其实我是真有担忧,并不是其他,而是我的小胃不好照顾.所以,尽快熟悉环境,稳定作息是杠杠的头号任务,我也不会自虐式的去节省.不管在哪,还是小胃第一.\n经过这两天的目测,环境也基本上能适应吧.But,试过才知道,真正的一个人在外边是什么样的感觉.如果我是自己做饭的话我想,我的周末两天就是在完全沉默中度过的.之前在厦门的那个月,还有小苏韩宙和其他人,尽管不是每天都很多话说,但是总算是与人说话了.现在的周末…于是,我迫切的想上班去.或许当初选择跟别人合租会好些吧,毕竟能认识些新朋友.\n","permalink":"https://xguox.me/rails-intern.html/","tags":["Jabber"],"title":"Life`上海"},{"categories":["Jabber"],"contents":"今个儿有发现问题,也有学到东西了.\n先说学到的吧,在做公司的论坛时候,要做一个浏览统计,因为一般做浏览统计都要加个时间限制的,刚接到任务时候第一反应就是在show这个action上使用session,后来自然很悲剧的粗大事了.因为每点击一篇topic都会keep住一个session,所以,如果做了时间限制后,点击了一篇再去点击其他的topic的话,则只会在第一篇topic那增加点击数,而其他的不会.其实,逻辑很Y的简单,那会临近下班时间,于是就没认真想,还以为很复杂.后来这么简单的问题还要公司的高手提示,真心觉得失败啊.\n其次就是,counter_cache的运用,刚开始还以为在关联关系中用了counter_cache后还要自己写increment_counter和decrement_counter 的相关方法,结果每次看日志都很奇怪的执行了两次update.搜索了之后才发现,原来定义好关联关系和counter_cache之后,xxxx_count字段值会自己增减.\n最后,不是关于技术知识的.而是我自己.我意识到好几次,萧然在指导我的时候,我的思维几乎不会转,至少说转的很慢,准确些说,这种状态只存在于我和他交流讨论的时候.因为好几次都是,他一转脚走开一下下,我就把大概的解决办法想出来.以至于都形成了习惯,经常跟他说,我先想想先.Maybe还是有些紧张吧,不是那种紧张,毕竟我跟他也算挺熟啦,肯定不会紧张什么,应该说是思维紧张吧.这个毛病有点悲剧啊.感觉自己的即兴思考能力还不及慢性思考能力的十分一.还是得多点跟人交流啊.\n","permalink":"https://xguox.me/some-findings.html/","tags":["Jabber"],"title":"一些学习,一些发现"},{"categories":["Ruby"],"contents":"最近在做个课程设计,好吧,俺老师是要求用JSP的,可惜我那弱爆了的水平是不可能完成的.于是顶着各种压力自己用Rails来做.\n今天才发现个问题,原来mysql默认存储的int貌似最多是10位就爆了,原本我的数据库中有学号那个字段的并且migration是存为integer,结果录入数据时候就各种甭了,因为学校学号都是10位数,不管我输入的是什么,进了数据库的学号字段都变成 2147483648, 刚开始看日志,看程序都木有发现什么问题,纠结不能,查看数据库发现,每条记录的学号西端都是统一的,搜索一通才发现问题所在.于是直接用mysql工具将学号字段改成了BIGINT,杠杠的就啥事都没了\n另外个是,用了will_paginate这个gem做分页,不过默认显示的是\u0026rdquo;previous\u0026rdquo;和\u0026rdquo;next\u0026rdquo;,改为中文的\u0026rdquo;前一页\u0026rdquo;和\u0026rdquo;后一页\u0026rdquo;需要先在config/initializers/ 下新建一个will_paginate.rb, and then在里头加入\n1 2 3  # coding: utf-8 WillPaginate::ViewHelpers.pagination_options[:previous_label ] = \u0026#34;前一页\u0026#34; WillPaginate::ViewHelpers.pagination_options[:next_label ] = \u0026#34;后一页\u0026#34;  ","permalink":"https://xguox.me/rails-note.html/","tags":["Ruby"],"title":"Rails 小记两则"},{"categories":["Jabber"],"contents":"最近各种没状态,我的Rails学习落下了不少,主要是胃病来袭外加一些校内的揪心事.哎,说到底,还是我还不会调整状态.一有什么较为棘手的外部因素就立马各种没状态,定不下来学习!\n前几天开始着手写公司的论坛,在本地跑的时候,日志文件显示,居然每个请求都执行两次,各种不能找到问题根源,偶然间换了firefox(原本是chromium),这一跑居然正常了!!!想来chromium为嘛会跑出这样的bug呢?尝试关闭插件,最终发现,罪魁祸首居然是之前安装的\u0026rdquo;Web Technology Notifier\u0026rdquo;插件.好吧,就这么个小玩意让我蛋疼了两天.\n小胃又折腾我了,每次看好了(可能只是治好标了)一个小小的放纵就估计又要悲剧了.这次貌似比往前两次要严重了些.Y,想起那天冒着大暴雨在威尼斯版妖都的大马路上艰难前进的求医\u0026hellip;或许,以后必须一切以自己的胃为优先考虑.好吧,我每次都这么对自己说,却貌似没坚持得了多久.\n目前我最需要的应该是静下心来,整理好思路\u0026hellip;\n还有就是必须学会的调整,事情无法改变的就别想太多.无论什么事情,什么时候,能轻易地控制自己的思维不跑调!\n今晚是这次胃病以来的一次考验,志愿队的送旧,小的们搞了好一箱啤酒回来,and then一个两个各种想灌,都被我坚决地以各种手段逃脱.可能在他们面前这样避酒不够厚道,不过我也不想的,如果我的胃好好的也用不着这么木有风度的避来避去.\n希望自己能坚持这样,不顾一切,以胃为最优先考虑\n好吧,借着病说休息,给自己借口打了两天大菠萝!!\n","permalink":"https://xguox.me/recently-status.html/","tags":["Jabber"],"title":"小记"},{"categories":["Ruby"],"contents":"昨晚,把ubuntu升级到了12.04,其实,对我目前的使用来说,系统是否升级几乎没影响.不过看着更新管理器冒出了新版本,反正也就按几个下的功夫,于是就升上去了.\n新版本多了首先感觉感觉性能上貌似快了些许,不多\u0026hellip;另外对我而言直观的体验是launcher,之前的launcher也太敏感了,不过现在这倒是太呆板了,有时想要却不出来.不过桌面状态下不显示这点我比较喜欢.(以前也可以设置?不知道,木有研究)\n另外alt键貌似上边会冒出个可输入命令行的,目前还木有发现其实际作用.\n好吧,其实,我对这些改进啊有的没的还不怎么敏感.所以与其要看我介绍新功能新特性还不如直接搜索.\n我敏感的只是,我的一些Rails项目都跑不动了.passenger报错\n1 2  Error message: libmysqlclient_r.so.16: cannot open shared object file: No such file or directory - /home/xguox/.rvm/gems/Ruby-1.9.2-p290@Rails/gems/mysql2-0.3.11/lib/mysql2/mysql2.so   我还以为是我缺少了libmysqlclient_r.so.16这个文件,自己下载了之后也还是不行.期间尝试重装mysql,也试过之前说的那个,安装各种组建的命令还是无果.最终在Ruby-china得到大神指点,居然只需卸了重装mysql2这个gem就可以了.貌似是因为/usr/lib 下的mysql2.so没有link到gem里边的mysql2.so 过后,还是跟往常没有多大差别的继续Rails\u0026hellip;\n","permalink":"https://xguox.me/ubuntu1204-and-rails.html/","tags":["Ruby"],"title":"ubuntu12.04 and Rails"},{"categories":["Ruby"],"contents":"居然今晚才发现Rails中try的用法,初一看,怎么那么像印象中某些语言的异常处理?不对啊,Ruby中的异常处理不是长这个样子的. 查了下Rails的API才了解了大致用法.\nAPI:\n不使用try的时候是\n1  @person \u0026amp;\u0026amp; @person.name   需确保@person不是nil,否则会报错\n1  NoMethodError: undefined method `name\u0026#39; for nil:NilClass   如果用try的话\n1  @person.try(:name)   如果@person为nil则返回nil而不是报错\n另外,try还可以接收block作为参数,\n1 2  Person.try(:find, 1) @people.try(:collect) {|p| p.name}   漏了一句,API开头就说了,像常规的Ruby的Object#send 那样工作 看了下源码,貌似是利用了send实现的.最近在研究Ruby的元编程.\n","permalink":"https://xguox.me/usage-of-try.html/","tags":["Ruby"],"title":"Rails中try的用法"},{"categories":[],"contents":"其实前不久就要写挺多东西的,只是忽然间事情就集中的涌了过来,状态没调整好,也就耽搁了些.之前是直接使用网上高人的vim配置以及插件,发现有很多自己也用不上,而且自己东添西改了之后就变得更臃肿了.于是,杠杠的重新整理.vim\u0026amp;.vimrc\n首先是配色方案,可能是我的屏幕分辨率比较低的缘故,感觉好几个自带的配色方案挺不错的,但是相对却比较模糊.最后发现,fruity的那个配色比较好一些(主要是没有那种模糊感),不过配色方面不咋滴,于是自己在fruity.vim里头东改西改,最终个人感觉还不错.and then就是一些插件了.目前主要是做Rails开发,以下几个插件是我较常使用到的: NERD tree、snipMate 、ctrlp、 ctags\nNERD tree主要提供的是树型目录结构(如上图左),它的功能不仅这一点,丰富的多了,不过我感觉最强大最常使用的还是书签功能.简单的一个:Bookmark 命令,以后就不用一直在目录下跳转啊跳转\nsnipMate则是一个代码补全的插件,比如在controller下输入rp然后按下tab会直接输出render :partial =\u0026gt; \u0026quot;item\u0026quot;在view里边输入rp然后按下tab的话则会直接输出\u0026lt;%= render :partial =\u0026gt; \u0026quot;file\u0026quot; %\u0026gt;在view下使用都会智能的为你添加上\u0026lt;% %\u0026gt;或者\u0026lt;%= %\u0026gt;\n补充一个,delimitMate,这个貌似也是一个补全代码的插件,不过我是作为snipMate的补充来用的,因为snipMate标点符号不会自动补充,比如输入 { 不会自动显示 }\n很多人都说这类插件会阻碍学习熟记代码什么的,不过我是觉得,既然能方便、快速编写代码,何乐而不为呢,再说并不是所有的代码都自动补全(那就不用开发了),这个阻碍并没传说那么大吧.\nctrlp 个人比较依赖的一个插件,快速打开你想要的文件,这对于Rails这种MVC框架来讲是灰常有帮助的.印象中Sublime Text同样的功能也是ctrl+p这个快捷键组合.\nctags 在根目录下使用命令ctags -R后会创建一个tags文件,之后,你可以轻易地通过ctrl+]跳转到某个方法定义的地方.不过要提的一点是,一般的项目都能正常运行,但是我发现在公司的大项目下这个功能貌似有点问题,经常跳转不成功,不知道是否因为太多子目录的缘故.\n另外说个Rails.vim,很多人都强烈推荐这个插件,不过装上后目前没发现什么功能是我特别需要的.\n最后就是配置文件.默认字体,大小等等一些基本设置.\n好吧,有兴趣的童邪直接往我的github看过来吧,我把.vim挂上去了.\n","permalink":"https://xguox.me/my-vim-config.html/","tags":[],"title":"My VIM Config"},{"categories":["Ruby"],"contents":"并不是我刚开始接触Rails两周,而是这两周我才能真正的算个入门吧.虽然学校有教C与Java,但就像我之前所说那般,其实Ruby才是我的first\u0026amp;major.\n为嘛说这两周才算真正的入门呢?自认为自己不是什么学习能力超常的人.但是我觉得,一般人能做到的,我也能做到,一般人难以做到的,maybe我加多几把油也能做到.\n从去年10月开始接触Ruby,看完《Ruby编程语言》就开始《Ruby on Rails Tutorail Learn Rails by Example》期间也交替翻翻AWDWR中文第三版和英文第四版.前不久的Github被爆事件,那会让我郁闷了好一会,看着社区上好多人在议论纷纷我却什么都不懂.仲使我再关注又如何.直到好久之后才通过各种奋力查资料知道Mass Assignment究竟是怎么回事.\n在这两周之前,我的Rails知识点很杂乱,大概是因为我的学习习惯吧.通常都是一般般懂的把整体过一遍,再回头逐个细细推敲.但是,这个整体过一遍的度把握的不太好,于是\u0026hellip;在此之前,我能不用对着官网那个guide搭出同样的博客,但是我之所以能搭出来只是因为我把那些步骤很纯粹的记住了,这一步做完大概下一步要做啥死记了.呃,有时候发觉自己的记忆能力还不错下.\n这两周,我想很大原因是有大牛指导着吧.前一周,我做好各种被拒绝的心理准备决定出去找一些用Rails的公司实习,但是很幸运的是,我的第一次面试就成功了.(PS:其实很疑惑为什么会招我,很明显的我并不是个牛X的Railser,当时的笔试题目貌似我答出来的也就一半那样吧；并且,在某网站上我发现我们的头头回复一名求职的实习生时说只招全职之类的.)\n相较之以往,如今,我的学习也比较系统一些.虽然在多表关联那一块运用还不太熟练,但Rails的MVC结构算是基本搞明白了.以及很多helper,route,也基本懂了.另外,我们的头头总喜欢问我们一句,有没更好的方法.每次我都只把这个更好的方法了解一半,彻底钻研的精神还不够啊.结果每次都不是最好的方法.泪奔啊\u0026hellip;让头儿失望了.\n不过有点小悲催的是,公司的VCS一直都是用SVN,而我只熟悉Git,呃,也不叫熟悉吧,要是我真的了如指掌了一定力劝公司改用Rails.\n这两周固然很多以前模模糊糊的都理解了.不过我想,要是没有以前的缓慢前进,也不会有这两周的成果呢?\n记得那次面试时候我发现我最大的缺点并不是基础不牢什么的,而是我的学习时间的分配以及计划没有安排好.我每天除了睡觉吃饭等等几乎就是学习Ruby/Rails相关,但其实我的效率可以说是比较低.\n哦,还有一个应该算是进步吧,编码的规范.以往只是自己练习写的代码各种凹凹凸凸\u0026hellip;现在都会注意那些空格换行等等格式了.\n虽说是进步了不少,但是,公司的代码看起来还是挺吃力的.能不吃力嘛,动不动一个model类或者view页面就5~600行.也不知道几个月的实习下来我能熟悉几成.\n","permalink":"https://xguox.me/two-weeks-withs-rails.html/","tags":["Ruby"],"title":"Two weeks withs Rails"},{"categories":["Ruby"],"contents":"从刚接触Rails开始就被灌输三种观念\u0026ndash;DRY、COC、REST.虽然这三种思想还没完全的领悟透彻,但是,已经感受到了它们的强大之处.\n目前印象最深刻的则是DRY.最近在跟着Ruby on Rails Tutorial学写一点Rspec测试.这种感觉又更强烈了.看着下面代码一步步的减少,感慨DRY的思想无处不在啊.这里只是一个Example可能感觉不到什么,但是当Example多了而且几乎都在测试同一样东西的时候,优点不言而喻.\n1 2 3 4 5 6 7 8 9 10 11 12  describe \u0026#34;Home page\u0026#34; do it \u0026#34;should have the h1 \u0026#39;Sample App\u0026#39;\u0026#34; do visit \u0026#39;/static_pages/home\u0026#39; page.should have_selector(\u0026#39;h1\u0026#39;, text: \u0026#39;Sample App\u0026#39;) end it \u0026#34;should have the title \u0026#39;Home\u0026#39;\u0026#34; do visit \u0026#39;/static_pages/home\u0026#39; page.should have_selector(\u0026#39;title\u0026#39;, text: \u0026#34;Ruby on Rails Tutorial Sample App | Home\u0026#34;) end end  1 2 3 4 5 6 7 8 9 10 11 12  describe \u0026#34;Home page\u0026#34; do it \u0026#34;should have the h1 \u0026#39;Sample App\u0026#39;\u0026#34; do visit root_path page.should have_selector(\u0026#39;h1\u0026#39;, text: \u0026#39;Sample App\u0026#39;) end it \u0026#34;should have the title \u0026#39;Home\u0026#39;\u0026#34; do visit root_path page.should have_selector(\u0026#39;title\u0026#39;, text: \u0026#34;Ruby on Rails Tutorial Sample App | Home\u0026#34;) end end  1 2 3 4 5 6 7 8 9 10 11 12  describe \u0026#34;Home page\u0026#34; do before { visit root_path } it \u0026#34;should have the h1 \u0026#39;Sample App\u0026#39;\u0026#34; do page.should have_selector(\u0026#39;h1\u0026#39;, text: \u0026#39;Sample App\u0026#39;) end it \u0026#34;should have the title \u0026#39;Home\u0026#39;\u0026#34; do page.should have_selector(\u0026#39;title\u0026#39;, text: \u0026#34;Ruby on Rails Tutorial Sample App | Home\u0026#34;) end end  1 2 3 4 5 6 7 8 9  subject { page } describe \u0026#34;Home page\u0026#34; do before { visit root_path } it { should have_selector(\u0026#39;h1\u0026#39;, text: \u0026#39;Sample App\u0026#39;) } it { should have_selector \u0026#39;title\u0026#39;, text: \u0026#34;Ruby on Rails Tutorial Sample App | Home\u0026#34; } end  1 2 3 4 5 6 7 8  subject { page } describe \u0026#34;Home page\u0026#34; do before { visit root_path } it { should have_selector(\u0026#39;h1\u0026#39;, text: \u0026#39;Sample App\u0026#39;) } it { should have_selector(\u0026#39;title\u0026#39;, text: full_title(\u0026#39;Home\u0026#39;)) } end  1 2 3 4 5 6 7 8 9 10 11 12  shared_examples_for \u0026#34;all static pages\u0026#34; do it { should have_selector(\u0026#39;h1\u0026#39;, text: heading) } it { should have_selector(\u0026#39;title\u0026#39;, text: full_title(page_title)) } end describe \u0026#34;Home page\u0026#34; do before { visit root_path } let(:heading) { \u0026#39;Sample App\u0026#39; } let(:page_title) { \u0026#39;Home\u0026#39; } it_should_behave_like \u0026#34;all static pages\u0026#34; end   其实,Rspec只是Ruby写的测试框架.但是,目前Rails程序上写测试用的最多的还是Rspec吧.而在Rails中,最直观的运用到这思想的应该是helper、partial了吧.把好几个view的重复代码.整合在一个partial下.代码量的减少不言而喻.减少了出错的几率.修改重构起来方便很多.\n当然DRY的思想绝不仅仅在于这一点点.不然国外也没那么名著专门描述它.\n昨天刚从一位网友也可以说是Rails学习道友那买了本打印版的《Agile Web Development with Rails,Fourth Edition》.在跟这位道友面交的时候他不断地鼓舞我,即使到最后用的人寥寥无几也要坚持 Rails下去,坚持它的思想.其实,不用他跟我说我也会坚持.不然,我不会在学校开展JAVA课程的时候自己那么寂寞的跑去学Ruby/Rails.只是他的一番话的确对我又有了不少的鼓舞.他不断地说我和年纪就接触这么新的语言框架什么的,其实,我更感慨敬佩他比我大了15~6岁还那么有power学习这些新事物.\nOK,扯远了,Over\n","permalink":"https://xguox.me/dry.html/","tags":["Ruby"],"title":"DRY"},{"categories":["Ruby"],"contents":"把平常中一些开发出错以及解决方法记录了下来,其实,基本上都是Google或者StackOverflow得到的答案.然后有些都不知道问题的根源,只知道个解决方法\n*\n1 2  Issue-- RVM is not a function, selecting rubies with \u0026#39;rvm use ...\u0026#39; will not work.  1 2 3  Solution-- 添加下面这句到 ~/.bashrc [[ -s \u0026#34;$HOME/.rvm/scripts/rvm\u0026#34; ]] \u0026amp;\u0026amp; . \u0026#34;$HOME/.rvm/scripts/rvm\u0026#34;重启终端   *\n1 2 3 4 5 6 7  Issue-- Gem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension. . . An error occured while installing mysql2 (0.3.11), and Bundler cannot continue. Make sure that `gem install mysql2 -v \u0026#39;0.3.11\u0026#39;` succeeds before bundling.  1 2  Solution-- sudo apt-get install libmysql-Ruby libmysqlclient-dev (Ubuntu)   *\n1 2  Issue-- Could not find a JavaScript runtime. See https://github.com/sstephenson/execjs for a list of available runtimes.  1 2 3 4  Solution-- Just install execjs and the Rubyracer in your gemfile and run bundle after. gem \u0026#39;execjs\u0026#39; gem \u0026#39;theRubyracer\u0026#39;   *\n1 2 3  Issue-- \u0026gt;Can\u0026#39;t connect to local MySQL server through socket \u0026#39;/tmp/mysql.sock\u0026#39; (2) Couldn\u0026#39;t create database for {\u0026#34;adapter\u0026#34;=\u0026gt;\u0026#34;mysql2\u0026#34;, \u0026#34;encoding\u0026#34;=\u0026gt;\u0026#34;utf8\u0026#34;, \u0026#34;database\u0026#34;=\u0026gt;\u0026#34;o_p_\u0026#34;, \u0026#34;pool\u0026#34;=\u0026gt;5, \u0026#34;username\u0026#34;=\u0026gt;\u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;=\u0026gt;nil, \u0026#34;socket\u0026#34;=\u0026gt;\u0026#34;/tmp/mysql.sock\u0026#34;}, charset: , collation:  1 2  Solution-- host:127.0.0.1(diff??localhost)   *\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Issue-- Gem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension. /usr/bin/Ruby1.9.1 extconf.rb checking for pg_config... no No pg_config... trying anyway. If building fails, please try again with --with-pg-config=/path/to/pg_config checking for libpq-fe.h... no Can\u0026#39;t find the \u0026#39;libpq-fe.h header * extconf.rb failed * Could not create Makefile due to some reason, probably lack of necessary libraries and/or headers. Check the mkmf.log file for more details. You may need configuration options. sudo apt-get install libpq-dev sudo gem install pg  1 2 3 4 5  Solution-- sudo apt-get install libpq-dev sudo gem install pg #貌似跟之前那个mysql的问题有点像  1 2  除了系统的Ruby文件可以用相对路径,自己编写的Ruby文件如果要require的话需要用绝对路径.否则会报错no such file to load . 而load则无论什么Ruby文件都可以用相对路径加载  ","permalink":"https://xguox.me/rails-dot-note.html/","tags":["Ruby"],"title":"Ruby/Rails.note"},{"categories":["Jabber"],"contents":"1 2 3 4 5 6  黑客行为必须包含三个特点:playful、cleverness、exploration ----Richard Stallman 黑客伦理的一个必然推论就是,黑客不服从管教,具有叛逆精神. 黑客价值观的核心原则可以概况成这样几点:分享(Sharing)、开放(Openness)、民主(Decentralization)、计算机的自由使用(Free access to computers)、进步(World Improvement).   我不是一个高端黑客,但是,看到里边的一些描述,我会有种很兴奋的感觉,甚至会自言自语地说\u0026rdquo;没错,真的就是这样的\u0026rdquo; 这本书,99%是写的很不错的,唯一的缺陷是,可能作者对Lisp极度热爱,关于Lisp,表达的似乎有些过了.几度贬低其他某些语言而用各种修饰把Lisp说成是最好的语言.要是纯JAVA程序员看了可能会更来气.其实也可能是英文与中文的问题吧.在英语\u0026rdquo;most\u0026rdquo;这个单词会很随意的用出来表达程度.翻译成中文就是\u0026rdquo;最\u0026rdquo;.或许,很多人看到这个\u0026rdquo;最\u0026rdquo;字就会有些不爽了.不过可能\u0026rdquo;most\u0026rdquo;的意思没有\u0026rdquo;最\u0026rdquo;来的那么强烈.对于这一部分,对编程语言的描述,其实我自己是认同大部分观点的(除了那个最字).\n 几乎所有的创作者在职业生涯的早期都有一份\u0026rdquo;白天工作\u0026rdquo;(day job).画家和作家尤为显著.如果幸运的化,你能找到一份与你的\u0026rdquo;真正工作\u0026rdquo;非常相关的\u0026rdquo;白天工作\u0026rdquo;.\n 就像我之前的一篇post写的,有些人把code纯粹当成赚钱生计的工具,他们并非对这份job真正的喜爱. 上次在家看电视,里头讲到公务员的东西,然后爸妈就不断说公务员怎么个好的\u0026hellip;我就说了句\u0026rdquo;扔给我都不去做\u0026rdquo;.可能有点极端,但却是发自内心的.现在,如果摆在我面前的选择有两个,月薪上W的公务员和月薪5K的Ruby程序员,我会选择Ruby程序员.但是,如果是月薪2K的公务员和月薪1K的Ruby程序员,我会选择当个公务员(day job).我不是要跟钱过不去,我还是一个俗人,我也需要金钱,我还要靠金钱过活、生存.不管是什么样的生活都是建立在有面包的基础上的.\n 如果你是一个20多岁的优秀黑客,每年的薪水大约是8万美元.这意味着,平均来看,你必须每年至少为公司带来8万美元的利润,这样才能保证公司没有亏钱.但是,你的真正工作时间其实可以是上班时间的2倍,如果你全神贯注,每小时的产出可提高3倍.如果再把大公司里令人讨厌的中间管理层除去(他们经常以主管的身份妨碍你的工作),你的效率可以再提高2倍.还有一个可以提高效率的地方:你不用再完成强行指派给你的工作,尽可以根据自己的愿望,做出最能发挥你聪明才智的成果.假定这会把工作效率再增加三倍.将这些因子放在一起做乘法,你的工作效率将是在公司时的36倍\u0026hellip;\n 上述的那些因子倍数不必太过在意,这些只是一个假设.重点是他里边说的种种条件,在我看来是确确实实的可以提高工作效率\n 如果你开发出来的技术是竞争对手难以复制的,那就够了,你不需要依靠其他防御手段了.一开始就选择较难的问题,此后的各种决策都选择较难的那个选项.\n事实上有两种程度的面向对象编程:某些语言允许你以这种风格编程,另一种语言则强迫你一定要这样编程.\n ","permalink":"https://xguox.me/hackers-and-painters.html/","tags":["Jabber"],"title":"《黑客与画家》语录---我的那些价值观"},{"categories":[],"contents":"为了不影响其他人的网速,我只好选择了通宵下载VMware和Ubuntu.今早上老早就起来安装,看网上好多人安装Ubuntu各种各样的出错,而我却很幸运的一路顺风.不过中途有个\u0026rdquo;看电影\u0026rdquo;环节,估计单是那就花了半个钟,刚好那时候到了吃饭时间,于是就出去吃了个饭,回来基本搞掂.\nUbuntu给我的第一感觉就是界面很Nice.第二感觉是字体很Nice.只用过windows的我顿时觉得自己弱爆了,爆了,了\u0026hellip;一直在windows都习惯了用chrome,Ubuntu自带的是Firefox,这俩都是网络上广受好评的浏览器.不过,我就是有那么一种惯性,用开了chrome就懒得再去试Firefox.不过既然是自带的话那我也不再去折腾装chrome了.我不是浏览器高手,基本快捷键都差不多就OK了,也不会不顺手什么的.PS:顺便吐槽下IE,连我的副标题都显示有问题.\n我用Ubuntu无非就是体验在Linux下开发Ruby/Rails.于是基本配置搞掂之后就开始安装Ruby开发环境.我是用RVM来安装的,一开始的步骤都没什么问题,到了安装Rails的时候就出事了.\n1  $ gem install Rails   当执行这行命令的时候,一直报错\n1 2  ERROR: While executing gem ... (Gem::DependencyError) Unable to resolve dependencies: Rails requires activesupport (= 3.1.3), actionpack (= 3.1.3), activerecord (= 3.1.3), activeresource (= 3.1.3), actionmailer (= 3.1.3), railties (= 3.1.3)   接着各种百度谷歌也没找着解决方法.到最后,我自己也忘记看哪里,然后突发奇想的就用了下面这行命令\n1  $ rvm gemset install Rails -v=3.1.3   结果就很神奇的装上了.据说是GEM被GFW了的原因.不过淘宝最近推出了Rubygems 镜像.于是果断把Gemfile和Gem source改了. 然后是Git的配置,在SSH Key上除了点问题,不过跟着gthub的官方说明走也OK了.最近学到比较多的是Git的一些知识,比如就是,通过git clone获取的远端git库,只包含了远端git库的当前工作分支.其他分支都不见鸟.如果想获取其它分支,则要使用命令\n1  git checkout -b 本地分支名 远程分支名   折腾了OS,VCS,最近我的Rails却停滞不前.还要受到最近期末考的一些干扰,心思难以投入Rails\u0026hellip;悲剧.\n昨天又划分了40G的空间给Ubuntu,原本硬盘就没剩几个G了.以后会更多的在Ubuntu下作业.比起windows,这里还多了一份宁静,可以让我更专注于开发与学习.\n","permalink":"https://xguox.me/one-day-with-linux.html/","tags":[],"title":"One Day With Linux"},{"categories":["Jabber"],"contents":"这是我的第一篇Octopress Post,曾经很浅显简单的玩过一下Wordpress,因为部署问题后面没有继续.因为Octopress跟github的天然关系,我又开始玩独立博客了.虽然我很想自己一边学习Rails一边搭建自己的博客框架,不过鉴于自己水平暂时还不够,再者秉承不重复造轮子的思想,于是用了octopress,并部署在github上.这是成功发表的第一篇,感谢来自Ruby on Rails 公园的@googya 的小Tips.\n","permalink":"https://xguox.me/my-first-post-of-octopress.html/","tags":["Jabber"],"title":"Hello Octopress!"},{"categories":["Ruby"],"contents":" Repost\nRuby 语言常以其灵活性为人所称道.正如 Dick Sites 所言,您可以 \u0026ldquo;为了编程而编程\u0026rdquo;.Ruby on Rails 扩展了核心 Ruby 语言,但正是 Ruby 本身使得这种扩展成为了可能.Ruby on Rails 使用了该语言的灵活性,这样一来,无需太多样板或额外的代码就可以轻松编写高度结构化的程序:无需额外工作,就可以获得大量标准的行为.虽然这种轻松自由的行为并不总是完美的,但毕竟您可以无需太多工作就可以获得很多好的架构.\n例如,Ruby on Rails 基于模型-视图-控制器(Model-View-Controller,MVC)模式,这意味着大多数 Rails 应用程序都可以清晰地分成三个部分.模型部分包含了管理应用程序数据所需的行为.通常,在一个 Ruby on Rails 应用程序中,模型和数据库表之间的关系是 1:1；Ruby on Rails 默认使用的对象关系映射(ORM)ActiveRecord 负责管理模型与数据库的交互,这意味着 Ruby on Rails 程序通常都具有(如果有的话)很少量的 SQL 代码.第二个部分是视图,它包含创建发送至用户的输出所需要的代码；它通常由 HTML、JavaScript 等组成.最后的一个部分是控制器,它将来自用户的输入转变为正确的模型,然后使用适当的视图呈现响应.\nRails 的倡导者通常都乐于将其易用性方面的提高归功于 MVC 范型 — 以及 Ruby 和 Rails 二者的其他一些特性,并称很少有程序员能够在较短的时间内创建更多的功能.当然,这意味着投入到软件开发的成本将能够产生更多的商业价值,因此 Ruby on Rails 开发愈发流行.\n不过,最初的开发成本并不是事情的全部,还有其他的后续成本需要考虑,比如应用程序运行的维护成本和硬件成本.Ruby on Rails 开发人员通常会使用测试和其他的敏捷开发技术来降低维护成本,但是这样一来,很容易忽视具有大量数据的 Rails 应用程序的有效运行.虽然 Rails 能够简化对数据库的访问,但它并不总是能够如此有效.\nRails 应用程序为何运行缓慢? Rails 应用程序之所以运行缓慢,其中有几个很基本的原因.第一个原因很简单:Rails 总是会做一些假设为您加速开发.通常,这种假设是正确而有帮助的.不过,它们并不总能有益于性能,并且还会导致资源使用的效率低下 — 尤其是数据库资源.\n例如,使用等同于 SELECT *的一个 SQL 语句,ActiveRecord 会默认选择查询上的所有字段.在具有为数众多的列的情况下 — 尤其是当有些字段是巨大的 VARCHAR 或 BLOB 字段时 — 就内存使用和性能而言这种行为很有问题.\n另一个显著的挑战是 N+1 问题,本文将对此进行详细的探讨.这会导致很多小查询的执行,而不是一个单一的大查询.例如,ActiveRecord 无从知道一组父记录中的哪一个会请求一个子记录,所以它会为每个父记录生成一个子记录查询.由于每查询的负荷,这种行为将导致明显的性能问题.\n其他的挑战则更多地与 Ruby on Rails 开发人员的开发习惯和态度相关.由于 ActiveRecord 能够让如此众多的任务变得轻而易举,Rails 开发人员常常会形成 \u0026ldquo;SQL 不怎样\u0026rdquo; 的一种态度,即便在更适合使用 SQL 的时候,也会避免 SQL.创建和处理数量巨大的 ActiveRecord 对象的速度会非常缓慢,所以在有些情况下,直接编写一个无需实例化任何对象的 SQL 查询会更快些.\n由于 Ruby on Rails 常被用来降低开发团队的规模,又由于 Ruby on Rails 开发人员通常都会执行部署和维护生产中的应用程序所需的一些系统管理任务,因此若对应用程序的环境知之甚少,就很可能出问题.操作系统和数据库有可能未被正确设置.比如,虽然并不最优,MySQL my.cnf 设置常常在 Ruby on Rails 部署内保留它们的默认设置.此外,可能还会缺少足够的监控和基准测试工具来提供性能的长期状况.当然,这并不是在责怪 Ruby on Rails 开发人员；这是非专业化导致的后果；在有些情况下,Rails 开发人员有可能是这两个领域的专家.\n最后一个问题是 Ruby on Rails 鼓励开发人员在本地环境中进行开发.这么做有几个好处 — 比如,开发延迟的减少和分布性的提高 — 但它并不意味着您可以因为工作站规模的减少而只处理有限的数据集.他们如何开发以及代码将被部署于何处之间的差异可能会是一个大问题.即便您在一个性能良好的轻载本地服务器上处理小规模的数据已经很长一段时间,也会发现对于拥塞的服务器上的大型数据此应用程序会有很明显的性能问题.\n当然,Rails 应用程序具有性能问题的原因可能有很多.查出 Rails 应用程序有何潜在性能问题的最佳方法是,利用能为您提供可重复、准确度量的诊断工具.\n检测性能问题 最好的工具之一是 Rails 开发日志,它通常位于每个开发机器上的 log/development.log 文件内.它具有各种综合指标:响应请求所花费的总时间、花费在数据库内的时间所占的百分比、生成视图所花时间的百分比等.此外,还有一些工具可用来分析此日志文件,比如 development-log-analyzer.\n在生产期间,通过查看 mysql_slow_log可以找到很多有价值的信息.更为全面的介绍超出了本文的讨论范围,更多信息可以在 参考资料 部分找到.\n其中一个最强大也是最为有用的工具是 query_reviewer 插件(参见 参考资料).这个插件可显示在页面上有多少查询在执行以及页面生成需要多长时间.并且它还会自动分析 ActiveRecord 生成的 SQL 代码以便发现潜在问题.例如,它能找到不使用 MySQL 索引的查询,所以如果您忘记了索引一个重要的列并由此造成了性能问题,那么您将能很容易地找到这个列(有关 MySQL 索引的更多信息,参见 参考资料).此插件在一个弹出的 \u0026lt;div\u0026gt;(只在开发模式下可见)中显示了所有这类信息.\n最后,不要忘记使用类似 Firebug、yslow、Ping 和 tracert 这样的工具来检测性能问题是来自于网络还是资源加载问题.\n接下来,让我们来看具体的一些 Rails 性能问题及其解决方案.\nN+1 查询问题 N+1 查询问题是 Rails 应用程序最大的问题之一.例如,清单 1 内的代码能生成多少查询?此代码是一个简单的循环,遍历了一个假想的 post 表内的所有 post,并显示 post 的类别和它的主体.\n清单 1. 未优化的 Post.all 代码 {% highlight ruby %} \u0026lt;%@posts = Post.all(@posts).each do |p|%\u0026gt; \u0026lt;%=p.category.name%\u0026gt; \u0026lt;%=p.body%\u0026gt;\n\u0026lt;%end%\u0026gt; {% endhighlight %}\n答案:上述代码生成了一个查询外加 @posts 内的每行一个查询.由于每查询的负荷,这可能会成为一个很大的挑战.罪魁祸首是对 p.category.name 的调用.这个调用只应用于该特定的 post 对象,而不是整个 @posts 数组.幸好,通过使用立即加载,我们可以修复这个问题.\n立即加载 意味着 Rails 将自动执行所需的查询来加载任何特定子对象的对象.Rails 将使用一个 JOIN SQL 语句或一个执行多个查询的策略.不过,假设指定了将要使用的所有子对象,那么将永远不会导致 N+1 的情形,在 N+1 情形下,一个循环的每个迭代都会生成额外的一个查询.清单 2 是对 清单 1 内代码的修订,它使用了立即加载来避免 N+1 问题.\n清单 2. 用立即加载优化后的 Post.all 代码 {% highlight ruby %} \u0026lt;%@posts = Post.find(:all, :include=\u0026gt;[:category] @posts.each do |p|%\u0026gt; \u0026lt;%=p.category.name%\u0026gt; \u0026lt;%=p.body%\u0026gt;\n\u0026lt;%end%\u0026gt; {% endhighlight %}\n该代码最多生成两个查询,而不管在此 posts 表内有多少行.\n当然,并不是所有情况都如此简单.处理复杂的 N+1 查询情况需要更多的工作.那么做这么多努力值得么?让我们来做一些快速的测试.\n测试 N+1 使用清单 3 内的脚本,可以发现查询可以达到 — 多慢 — 或多快. 清单 3 展示了如何在一个独立脚本中使用 ActiveRecord 来建立一个数据库连接、定义表并加载数据.然后,可以使用 Ruby 的内置基准测试库来查看哪种方式更快,快多少.\n清单 3. 立即加载基准测试脚本 {% highlight ruby %} require \u0026lsquo;Rubygems\u0026rsquo; require \u0026lsquo;faker\u0026rsquo; require \u0026lsquo;active_record\u0026rsquo; require \u0026lsquo;benchmark\u0026rsquo;\nThis call creates a connection to our database. ActiveRecord::Base.establish_connection( :adapter =\u0026gt; \u0026ldquo;mysql\u0026rdquo;, :host =\u0026gt; \u0026ldquo;127.0.0.1\u0026rdquo;, :username =\u0026gt; \u0026ldquo;root\u0026rdquo;, # Note that while this is the default setting for MySQL, :password =\u0026gt; \u0026ldquo;\u0026rdquo;, # a properly secured system will have a different MySQL # username and password, and if so, you\u0026rsquo;ll need to # change these settings. :database =\u0026gt; \u0026ldquo;test\u0026rdquo;)\nFirst, set up our database\u0026hellip; class Category \u0026lt; ActiveRecord::Base end\nunless Category.table_exists? ActiveRecord::Schema.define do create_table :categories do |t| t.column :name, :string end end end\nCategory.create(:name=\u0026gt;\u0026lsquo;Sara Campbell\\\u0026rsquo;s Stuff\u0026rsquo;) Category.create(:name=\u0026gt;\u0026lsquo;Jake Moran\\\u0026rsquo;s Possessions\u0026rsquo;) Category.create(:name=\u0026gt;\u0026lsquo;Josh\\\u0026rsquo;s Items\u0026rsquo;) number_of_categories = Category.count\nclass Item \u0026lt; ActiveRecord::Base belongs_to :category end\nIf the table doesn\u0026rsquo;t exist, we\u0026rsquo;ll create it. unless Item.table_exists? ActiveRecord::Schema.define do create_table :items do |t| t.column :name, :string t.column :category_id, :integer end end end\nputs \u0026ldquo;Loading data\u0026hellip;\u0026rdquo;\nitem_count = Item.count item_table_size = 10000\nif item_count \u0026lt; item_table_size (item_table_size - item_count).times do Item.create!(:name=\u0026gt;Faker.name, :category_id=\u0026gt;(1+rand(number_of_categories.to_i))) end end\nputs \u0026ldquo;Running tests\u0026hellip;\u0026rdquo;\nBenchmark.bm do |x| [100,1000,10000].each do |size| x.report \u0026ldquo;size:#{size}, with n+1 problem\u0026rdquo; do @items=Item.find(:all, :limit=\u0026gt;size) @items.each do |i| i.category end end x.report \u0026ldquo;size:#{size}, with :include\u0026rdquo; do @items=Item.find(:all, :include=\u0026gt;:category, :limit=\u0026gt;size) @items.each do |i| i.category end end end end {% endhighlight %}\n这个脚本使用 :include 子句测试在有和没有立即加载的情况下对 100、1,000 和 10,000 个对象进行循环操作的速度如何.为了运行此脚本,您可能需要用适合于您的本地环境的参数替换此脚本顶部的这些数据库连接参数.此外,需要创建一个名为 test 的 MySQL 数据库.最后,您还需要 ActiveRecord 和 faker 这两个 gem,二者可通过运行 gem install activerecord faker 获得.\n在我的机器上运行此脚本生成的结果如清单 4 所示.\n清单 4. 立即加载的基准测试脚本输出 {% highlight ruby %} \u0026ndash; create_table(:categories) -\u0026gt; 0.1327s \u0026ndash; create_table(:items) -\u0026gt; 0.1215s Loading data\u0026hellip; Running tests\u0026hellip; user system total real size:100, with n+1 problem 0.030000 0.000000 0.030000 ( 0.045996) size:100, with :include 0.010000 0.000000 0.010000 ( 0.009164) size:1000, with n+1 problem 0.260000 0.040000 0.300000 ( 0.346721) size:1000, with :include 0.060000 0.010000 0.070000 ( 0.076739) size:10000, with n+1 problem 3.110000 0.380000 3.490000 ( 3.935518) size:10000, with :include 0.470000 0.080000 0.550000 ( 0.573861) {% endhighlight %}\n在所有情况下,使用 :include 的测试总是更为迅速 — 分别快 5.02、4.52 和 6.86 倍.当然,具体的输出取决于您的特定情况,但立即加载可明显导致显著的性能改善.\n嵌套的立即加载 如果您想要引用一个嵌套的关系 — 关系的关系,又该如何呢? 清单 5 展示了这样一个常见的情形:循环遍历所有的 post 并显示作者的图像,其中 Author 与 Image 是 belongs_to 的关系.\n清单 5. 嵌套的立即加载用例 {% highlight ruby %} @posts = Post.all @posts.each do |p| \u0026lt;%=p.category.name%\u0026gt; \u0026lt;%=image_tag p.author.image.public_filename %\u0026gt; \u0026lt;%=p.body%\u0026gt; \u0026lt;%end%\u0026gt; {% endhighlight %}\n此代码与之前一样亦遭遇了相同的 N+1 问题,但修复的语法却没有那么明显,因为这里所使用的是关系的关系.那么如何才能立即加载嵌套关系呢?\n正确的答案是使用 :include 子句的哈希语法.清单 6 给出了使用哈希语法的一个嵌套的立即加载.\n清单 6. 嵌套的立即加载解决方案 {% highlight ruby %} @posts = Post.find(:all, :include=\u0026gt;{ :category=\u0026gt;[], :author=\u0026gt;{ :image=\u0026gt;[]}} ) @posts.each do |p| \u0026lt;%=p.category.name%\u0026gt; \u0026lt;%=image_tag p.author.image.public_filename %\u0026gt; \u0026lt;%=p.body%\u0026gt; \u0026lt;%end%\u0026gt; {% endhighlight %}\n正如您所见,您可以嵌套哈希和数组实量(literal).请注意在本例中哈希和数组之间的惟一区别是哈希可以含有嵌套的子条目,而数组则不能.否则,二者是等效的.\n间接的立即加载 并非所有的 N+1 问题都能很容易地察觉到.例如,清单 7 能生成多少查询?\n清单 7. 间接的立即加载示例用例 {% highlight ruby %} \u0026lt;%@user = User.find(5) @user.posts.each do |p|%\u0026gt; \u0026lt;%=render :partial=\u0026gt;\u0026lsquo;posts/summary\u0026rsquo;, :locals=\u0026gt;:post=\u0026gt;p %\u0026gt; \u0026lt;%end%\u0026gt; {% endhighlight %}\n当然,决定查询的数量需要对 posts/summary partial 有所了解.清单 8 中显示了这个 partial.\n清单 8. 间接立即加载 partial: posts/_summary.html.erb \u0026lt;h1\u0026gt;\u0026lt;%=post.user.name%\u0026gt;\u0026lt;/h1\u0026gt; 不幸的是,答案是 清单 7* 和 清单 8* 在 post 内每行生成一个额外查询,查找用户的名字 — 即便 post 对象由 ActiveRecord 从一个已在内存中的 User 对象自动生成.简言之,Rails 并不能关联子记录与其父记录. 修复方法是使用自引用的立即加载.基本上,由于 Rails 重载由父记录生成的子记录,所以需要立即加载这些父记录,就如同父与子记录之间是完全分开的关系一样.代码如清单 9 所示.\n清单 9. 间接的立即加载解决方案 {% highlight ruby %} \u0026lt;%@user = User.find(5, :include=\u0026gt;{:posts=\u0026gt;[:user]}) \u0026hellip;snip\u0026hellip; {% endhighlight %}\n虽然有悖于直觉,但这种技术与上述技术的工作原理大致相似.但是,很容易使用这种技术进行过多的嵌套,尤其是在体系结构复杂的情况下.简单的用例还好,比如 清单 9 内所示的,但繁复的嵌套也会出问题.在一些情况下,过多地加载 Ruby 对象有可能会比处理 N+1 问题还要缓慢 — 尤其是当每个对象并没有被整个树遍历时.在该种情况下,N+1 问题的其他解决方案可能更为适合.\n一种方式是使用缓存技术.Rails V2.1 内置了简单的缓存访问.使用 Rails.cache.read、 Rails.cache.write 及相关方法,可以轻松创建自己的简单缓存机制,并且后端可以是一个简单的内存后端、一个基于文件的后端或一个分布式缓存服务器.在 参考资料 部分可以找到有关 Rails 内置缓存支持的更多信息.但您无需创建自己的缓存解决方案；您可以使用一个预置的 Rails 插件,比如 Nick Kallen 的 cache money 插件.这个插件提供了 write-through 缓存并以 Twitter 上使用的代码为基础.更多信息参见 参考资料.\n当然,并不是所有的 Rails 问题都与查询的数量有关.\nRails 分组和聚合计算 您可能遇到的一个问题是在 Ruby 所做的工作本应由数据库完成.这考验了 Ruby 的强大程度.很难想象在没有任何重大激励的情况下人们会自愿在 C 中重新实现其数据库代码的各个部分,但很容易在 Rails 内对 ActiveRecord 对象组进行类似的计算.但是,Ruby 总是要比数据库代码慢.所以请不要使用纯 Ruby 的方式执行计算,如清单 10 所示.\n清单 10. 执行分组计算的不正确方式 {% highlight ruby %} all_ages = Person.find(:all).group_by(\u0026amp;:age).keys.uniq oldest_age = Person.find(:all).max {% endhighlight %}\n相反,Rails 提供了一系列的分组和聚合函数.可以像清单 11 所示的那样使用它们.\n清单 11. 执行分组计算的正确方式 {% highlight ruby %} all_ages = Person.find(:all, :group=\u0026gt;[:age]) oldest_age = Person.calcuate(:max, :age) {% endhighlight %} ActiveRecord::Base#find 有大量选项可用于模仿 SQL.更多信息可以在 Rails 文档内找到.注意,calculate 方法可适用于受数据库支持的任何有效的聚合函数,比如 :min、:sum 和 :avg.此外,calculate 能够接受若干实参,比如 :conditions.查阅 Rails 文档以获得更详细的信息.\n不过,并不是在 SQL 内能做的所有事情在 Rails 内也能做.如果插件不够,可以使用定制 SQL.\n用 Rails 定制 SQL 假设有这样一个表,内含人的职业、年龄以及在过去一年中涉及到他们的事故的数量.可以使用一个定制 SQL 语句来检索此信息,如清单 12 所示.\n清单 12. 用 ActiveRecord 定制 SQL 的例子 {% highlight ruby %} sql = \u0026ldquo;SELECT profession, AVG(age) as average_age, AVG(accident_count) FROM persons GROUP BY profession\u0026rdquo;\nPerson.find_by_sql(sql).each do |row| puts \u0026ldquo;#{row.profession}, \u0026rdquo; \u0026lt;\u0026lt; \u0026ldquo;avg. age: #{row.average_age}, \u0026ldquo; \u0026lt;\u0026lt; \u0026ldquo;avg. accidents: #{row.average_accident_count}\u0026rdquo; end {% endhighlight %}\n这个脚本应该能生成清单 13 所示的结果.\n清单 13. 用 ActiveRecord 定制 SQL 的输出 {% highlight ruby %} Programmer, avg. age: 18.010, avg. accidents: 9 System Administrator, avg. age: 22.720, avg. accidents: 8 {% endhighlight %} 当然,这是最简单的例子.您可以自己想象一下如何能将此例中的 SQL 扩展成一个有些复杂性的 SQL 语句.您还可以使用 ActiveRecord::Base.connection.execute 方法运行其他类型的 SQL 语句,比如 ALTER TABLE 语句,如清单 14 所示.\n清单 14. 用 ActiveRecord 定制非查找型 SQL {% highlight ruby %} ActiveRecord::Base.connection.execute \u0026ldquo;ALTER TABLE some_table CHANGE COLUMN\u0026hellip;\u0026rdquo; {% endhighlight %}\n大多数的模式操作,比如添加和删除列,都可以使用 Rails 的内置方法完成.但如果需要,也可以使用执行任意 SQL 代码的能力.\n##结束语\n与所有的框架一样,如果不多加小心和注意,Ruby on Rails 也会遭遇性能问题.所幸的是,监控和修复这些问题的技术相对简单且易学,而且即便是复杂的问题,只要有耐心并对性能问题的源头有所了解,也是可以解决的.\n","permalink":"https://xguox.me/ror-performance.html/","tags":["Ruby"],"title":"给 Ruby on Rails 提速"},{"categories":["Ruby"],"contents":" proc是代码块的对象形式,它的行为就像一个代码块.Lambda的的行为略有不同,它的行为更像方法而非代码块.调用一个proc则像对代码块进行yield,而调用一个lambda则像调用一个方法.在Ruby1.9中的,可以通过Proc对象的实例方法 lambda? 来判定该实例是一个proc还是lambda,如果返回值为真,那么它是一个lambda,否则为一个proc.\n代码块、proc和lambda中的return语句 在一个代码块中的return语句不仅仅会从调用代码块的迭代器返回,它还会从调用迭代器的方法返回.例如:\n1 2 3 4 5 6  def test puts \u0026#34;entering method\u0026#34; 1.times{puts \u0026#34;entering block\u0026#34;;return} puts \u0026#34;exiting method\u0026#34; end test   proc与代码块类似,因此如果调用的proc执行一个return语句,它会试图从代码块(这个代码块被转换为一个proc)所在的方法中返回.比如:\n1 2 3 4 5 6 7  def test puts \u0026#34;entering method\u0026#34; p =Proc.new {puts \u0026#34;entering proc\u0026#34;;return} p.call puts \u0026#34;exiting method\u0026#34; end test   不过,因为proc经常在不同方法间传递,在proc中使用return语句会十分诡异.在proc被调用时,在句法上包含该proc的方法已经返回:\n1 2 3 4 5 6 7 8 9 10 11  def procBuilder(message) Proc.new {puts message ;return} end def test puts \u0026#34;entering method\u0026#34; p = procBuilder(\u0026#34;entering proc\u0026#34;) p.call puts \u0026#34;exiting method\u0026#34; end test   在把代码块转换成对象后,可以四处传递该对象,并且在\u0026rdquo;上下文\u0026rdquo;之外使用.如果这样做,则要承担从一个已经返回的方法中返回的风险.就像本例所示那样.当这种情况发生时,Ruby会抛出一个LocalJumpError异常.\n当然,在这个臆造的例子中,可以通过去掉多余的return语句来修复这个问题.不过return语句并非总是多余的,这时可以通过使用lambda而非proc来修复这个问题.如前所述,lambda更像方法而非代码块.这样,在lambda中的return语句仅仅从lambda自身返回.而不会从产生lambda的方法中返回:\n1 2 3 4 5 6 7  def test puts \u0026#34;entering method\u0026#34; p = lambda{puts \u0026#34;entering lambda\u0026#34;;return} p.call puts \u0026#34;exiting method\u0026#34; end test   Lambda中的return仅仅从lambda自身返回,这个事实意味着我们根本无须考虑LocalJumpError;\n1 2 3 4 5 6 7 8 9 10 11  def lambdaBuilder(message) lambda {puts message;return} end def test puts \u0026#34;exiting method\u0026#34; l =lambdaBuilder(\u0026#34;entering lambda\u0026#34;) l.call puts \u0026#34;exiting method\u0026#34; end test   代码块、proc和lambda中的break语句 当我们用Proc.new创建一个proc对象时,这个Proc.new就是break语句应该返回的地方,当我们调用proc对象的时候,这个迭代器已经返回了,因此,用Proc.new创建一个顶级break语句是说不通的:\n1 2 3 4 5 6 7  def test puts \u0026#34;entering test method \u0026#34; proc =Proc.new{puts \u0026#34;entering proc\u0026#34;;break} proc.call puts \u0026#34;exiting test method\u0026#34; end test   如果通过迭代器方法的\u0026amp;参数方式创建一个proc,我们可以调用它让该迭代器方法返回；\n1 2 3 4 5 6 7 8 9 10  def iterator (\u0026amp;proc) puts \u0026#34;entering iterator\u0026#34; proc.call puts \u0026#34;exiting test method\u0026#34; end def test iterator {puts \u0026#34;entering proc\u0026#34;;break) end test   Lambda类似于方法,因此,如果把一个break语句单独地放在那里,而不是出现在循环或者迭代方法中是说不通的.下面的语句,没有任何东西可以被break,你可能认为它会失败. 不过,在这种情况下,break的行为与return一样;\n1 2 3 4 5 6 7  def test puts \u0026#34;entering test method\u0026#34; lambda = lambda{puts \u0026#34;entering lambda\u0026#34;;break;puts \u0026#34;exiting lambda\u0026#34;} lambda.call puts \u0026#34;exiting test method\u0026#34; end test  ","permalink":"https://xguox.me/proc-vs-lambda.html/","tags":["Ruby"],"title":"Lambda 和 Proc 的区别在哪儿"},{"categories":null,"contents":" This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ` [outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;] \\`\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ` ... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ... \\`\nEdit fuse.js options to Search static/js/search.js ` keys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ] \\`\n","permalink":"https://xguox.me/search/","tags":null,"title":"Search Results"}]